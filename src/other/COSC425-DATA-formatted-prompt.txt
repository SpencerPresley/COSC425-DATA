README.md:
```
# Academic Metrics

> [!IMPORTANT]  
> ðŸŽ‰ **Now Available on PyPI!**  
> Install with: `pip install academic-metrics`
>
> This is the recommended installation method for most users.
>
> See [**Installation and Setup Steps**](#installation-and-setup-steps)

**What is it?**

This repository (COSC425-DATA) hosts the source code for the **Academic Metrics** package.

**Academic Metrics** is an AI-powered toolkit for collecting and classifying academic research publications.

The system:

- Collects publication data from Crossref API based on institutional affiliation
- Uses LLMs to classify research into NSF PhD research focus areas
- Extracts and analyzes themes and methodologies from abstracts
- Generates comprehensive analytics at article, author, and category levels
- Stores results in MongoDB (local or live via atlas), local JSON files, and optionally Excel files

## Table of Contents

- [Academic Metrics](#academic-metrics)
  - [Table of Contents](#table-of-contents)
  - [Features](#features)
  - [Documentation](#documentation)
  - [Example Site and Demo](#example-site-and-demo)
  - [Installation and Setup Steps](#installation-and-setup-steps)
    - [0. External Setup](#0-external-setup)
    - [1. Installation](#1-installation)
    - [2. Creating the directory and necessary files](#2-creating-the-directory-and-necessary-files)
    - [3. Virtual Environment (Optional but Recommended)](#3-virtual-environment-optional-but-recommended)
    - [4. Environment Variables](#4-environment-variables)
    - [5. Setting required environment variables](#5-setting-required-environment-variables)
      - [1. Open the `.env` file you just created, and add the following variables](#1-open-the-env-file-you-just-created-and-add-the-following-variables)
      - [2. Retrieve and set your MongoDB URI](#2-retrieve-and-set-your-mongodb-uri)
      - [3. Set your database name](#3-set-your-database-name)
      - [4. Set your OpenAI API Key](#4-set-your-openai-api-key)
    - [6. Using the package](#6-using-the-package)
      - [Option 1 (Short Script)](#option-1-short-script)
        - [1. Create the python file](#1-create-the-python-file)
        - [2. Copy paste the following code into the file you just created](#2-copy-paste-the-following-code-into-the-file-you-just-created)
        - [3. Run the script](#3-run-the-script)
      - [Option 2 (Command Line Interface)](#option-2-command-line-interface)
        - [1. Create the python file](#1-create-the-python-file-1)
        - [2. Copy and paste the following code into the file you just created](#2-copy-and-paste-the-following-code-into-the-file-you-just-created)
        - [3. Run the script](#3-run-the-script-1)
        - [Examples](#examples)
  - [Wrapping Up](#wrapping-up)

## Features

- **Data Collection**: Automated fetching of publications via Crossref API
- **AI Classification**: LLM-powered analysis of research abstracts
- **Multi-level Analytics**:
  - Article-level metrics and classifications
  - Author/faculty publication statistics
  - Category-level aggregated data
- **Flexible Storage**: MongoDB integration, local JSON output, and optionally Excel files
- **Configurable Pipeline**: Customizable date ranges, models, and processing options
- **And more!**: There are many useful tools within the academic metrics package that can be used for much more than just classification of academic research data, and they're all quite intuitive to use. See [Other Uses](./additional_information/OtherUses.md) for more information.

## Documentation

To be able to see any and all implementation details regarding code logic, structure, prompts, and more you can check out our documentation. The documentation is built with [*Sphinx*](https://github.com/sphinx-doc/sphinx), allowing for easy use and a sense of famliarity.

[**Academic Metrics Documentation**](https://cosc425-data.readthedocs.io/en/latest/)

## Example Site and Demo

We also built an example site with the data we collected so that you can get a small idea of the potential uses for the data. This is by no means the only use case, but it does serve as a nice introduction to decide if this package would be useful for you.

> [!NOTE]
> The source code for the example site is available [here](https://github.com/cbarbes1/AITaxonomy-Front)

[**Example Site**](https://ai-taxonomy-front.vercel.app/)

> [!TIP]
> You can use our site source code for your own site!
> To easily launch your own website using the data you collect and classify via *Academic Metrics* see [**Site Creation Guide**](./additional_information/SiteCreationGuide.md)

To see a demo of the site, you can watch the below video:

[![Demo Video](https://img.youtube.com/vi/LojIwEvFgrk/maxresdefault.jpg)](https://youtu.be/LojIwEvFgrk)

---

## Installation and Setup Steps

Hey all, Spencer here, we are pleased to announce as of January 1st, 2025, you can now install the *Academic Metrics* package via *pip* and easily run the entire system via a short script or command line interface. Below are instructions outlining step by step how to do it. The steps walkthrough each piece of the process starting with installing python and setting up your environment, if you do not need help with those type of steps or want to jump straight to the code, first see [1. Installation](#1-installation), then you can skip to [6. Using the package](#6-using-the-package).

</br>

### 0. External Setup

1. **Installing and setting up Python 3.12:**

    While you should be able to use any version of Python >= 3.7, we recommend using Python 3.12 as that is the version we used to develop the system, and the one it's been tested on.

    For a detailed Python installation guide, see our [Python Installation Guide](./additional_information/_guides/_python_install.md).

2. **Installing and setting up MongoDB:**

    For a detailed MongoDB installation and setup guide, see our [MongoDB Installation Guide](./additional_information/_guides/_mongodb_install.md).

    Once you have MongoDB installed and running, you can create a database to store your data in, if you haven't already.

    To create a new database, you can run:

    ```bash
    use <db_name>
    ```

    If you need more help, the MongoDB Installation Guide goes into more detail on how to create a database and verify it exists.

    Collection creation is handled by the system, you do not need to create them.

    </br>

    ---

### 1. Installation

Install `academic_metrics>=1.0.98` via pip.

To install the latest version of the package, you can run the following command:

```bash
pip install academic-metrics
```

### 2. Creating the directory and necessary files

1. **Create the directory and navigate into it:**

   For this example we will be using `am_data_collection` as the name of the directory, but you can name it whatever you want.

    **All systems (seperate commands):**

    ```bash
    mkdir am_data_collection
    cd am_data_collection
    ```

    Or as a single line:

    **Linux / Mac / Windows Command Prompt**:

    ```bash
    mkdir am_data_collection && cd am_data_collection
    ```

    **Windows Powershell**:

    ```powershell
    mkdir am_data_collection; cd am_data_collection
    ```

</br>

### 3. Virtual Environment (Optional but Recommended)

Now that you've created and entered your project directory, you can set up a virtual environment.

For detailed instructions on setting up and using virtual environments, see our [Python Installation Guide - Virtual Environments Section](./additional_information/_guides/_python_install.md#setting-up-virtual-environments).

After setting up your virtual environment, return here to continue with the next steps.

</br>

### 4. Environment Variables

**Create a `.env` file inside the directory you just created.**

**Linux/Mac**:

```bash
touch .env
```

**Windows** (Command Prompt):

```cmd
type nul > .env
```

**Windows** (PowerShell):

```powershell
New-Item -Path .env -Type File
```

You should now have a `.env` file in your directory.

</br>

### 5. Setting required environment variables

</br>

#### 1. Open the `.env` file you just created, and add the following variables

- a variable to store your MongoDB URI, I recommend `MONGODB_URI`
- a variable to store your database name, I recommend `DB_NAME`
- a variable to store your OpenAI API Key, I recommend `OPENAI_API_KEY`

 After each variable you should add `=""` to the end of the variable.

 Once you've done this, your `.env` file should look something like this:

```python
MONGODB_URI=""
DB_NAME=""
OPENAI_API_KEY=""
```

</br>

#### 2. Retrieve and set your MongoDB URI

For local MongoDB it's typically:

```python
MONGODB_URI="mongodb://localhost:27017"
```

For live MongoDB:

For a live version you should use the MongoDB Atlas URI. It should look something like this:

```bash
mongodb+srv://<username>:<password>@<cluster-name>.<unique-id>.mongodb.net/?retryWrites=true&w=majority&appName=<YourAppNameOnAtlas>
```

So in the `.env` file you should have something that looks like this:

Local:

```python
MONGODB_URI="mongodb://localhost:27017"
```

Live:

```python
MONGODB_URI="mongodb+srv://<username>:<password>@<cluster-name>.<unique-id>.mongodb.net/?retryWrites=true&w=majority&appName=<YourAppNameOnAtlas>"
```

</br>

> [!WARNING]
> I recommend starting locally unless you need to use a live MongoDB instance.
> This will avoid the need to deal with setting up MongoDB Atlas, which while not difficult, it is an added step.

</br>

#### 3. Set your database name

You can pick any name you want for `DB_NAME`, but it needs to be a name of a valid database on your mongodb server. To make one on the command line you can run:

```bash
mongosh
use <db_name>
```

For this demonstration we will be using `academic_metrics_data` as the `DB_NAME`.

First we'll create the database on the command line:

```bash
mongosh
use academic_metrics_data
```

This is to ensure the database actually exists so that the system can access it.

Now that the database exists, we'll set the `DB_NAME` in the `.env` file.

```python
DB_NAME="academic_metrics_data"
```

</br>

#### 4. Set your OpenAI API Key

If you do not have an OpenAI API key you will need to create one, but do not worry, it's easy.

Go to the following link and click on "+ Create new secret key":

[https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)

Give the key a name, and then copy the key.

Then in the `.env` file paste the key in the `OPENAI_API_KEY` variable.

It should look similar to this, but with the full key instead of `sk-proj...`:

```python
OPENAI_API_KEY="sk-proj..."
```

</br>

> [!IMPORTANT]
> You will need to add funds to your OpenAI account to use the API.
>
> When using the default model for the system (gpt-4o-mini), it cost us about $3-4 dollars to process all of the data from Salisbury University from 2009-2024.
>
> For larger models such as gpt-4o, the cost will be much higher.
>
> We saw good results using gpt-4o-mini, and it's also the most cost effective. So I recommend starting with that.
>
> Additionally, whether you opt to use our command line interface or your own script, the data is processed one month at a time and saved to the database, so if you run out of funds on your OpenAI account you will not lose data for the entire run, only the current month being processed. Simply add funds to your account and continue.
>
> You do not have to change anything in the code once you run it again, the system checks for existing data and only processes data that has not yet been processed.

</br>

All together your `.env` file should look like this:

```python
MONGODB_URI="mongodb://localhost:27017"
DB_NAME="academic_metrics_data"
OPENAI_API_KEY="sk-proj..."
```

</br>

### 6. Using the package

To use the system, you have 2 options:

1. Writing a short script (code provided) to loop over a range of dates you'd like to collect.

2. Using a provided function to run a command line interface version.

For most users, I recommend the second option, it's only a few lines of code which you can copy and paste, the rest of the usage is handled by the command line interface and doesn't require any additional coding, you can find the second option in the [Option 2 (Command Line Interface)](#option-2-command-line-interface) section.

On the other hand, if you plan on using the main system, or other tools within the package within your own scripts, or just don't enjoy using command line interfaces, I recommend the first option.

While I recommend the second option unless you're planning on using the package's offerings in a more complex manner, the basic code to run the system for the first option is provided in full in [Option 1 (Short Script)](#option-1-short-script) section.

To see some examples of more complex use cases with examples, you can check out the [Other Uses](./additional_information/OtherUses.md) section.

</br>

#### Option 1 (Short Script)

For this option you need to do the following:
</br>
</br>

##### 1. Create the python file

Within your directory, create a new python file, for this example we will be using `run_am.py`, but you can name it whatever you want.

**Linux/Mac**:

```bash
touch run_am.py
```

**Windows (Command Prompt):**

```cmd
type nul > run_am.py
```

**Windows (PowerShell):**

```powershell
New-Item -Path run_am.py -Type File
```

You should now have a python file in your directory whose name matches the one you created.

</br>

##### 2. Copy paste the following code into the file you just created

```python

# dotenv is the python package responsible for handling env files
from dotenv import load_dotenv

# os is used to get the environment variables from the .env file
import os

# PipelineRunner is the main class used to run the pipeline
from academic_metrics.runners import PipelineRunner

# load_dotenv is used to load the environment variables from the .env file
load_dotenv()

# Get the environment variables from the .env file
ai_api_key = os.getenv("OPENAI_API_KEY")
mongodb_uri = os.getenv("MONGODB_URI")
db_name = os.getenv("DB_NAME")

# Set the date range you want to process
# Years is a list of years as strings you want to process
# Months is a list of strings representing the months you want processed for each year
# For example if you want to process data from 2009-2024 for all months out of the year, you would do:
# Note: the process runs left to right, so from beginning of list to the end of the list,
# so this will process 2024, then 2023, then 2022, etc.
# Data will be saved after each month is processed.
years = [
    "2024",
    "2023",
    "2022",
    "2021",
    "2020",
    "2019",
    "2018",
    "2017",
    "2016",
    "2015",
    "2014",
    "2013",
    "2012",
    "2011",
    "2010",
    "2009",
]
months = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"]

# Loop over the years and months and run the pipeline for each month
# New objects are created for each month to avoid memory issues as well as to avoid overwriting data
for year in years:
    for month in months:

        # Create a new PipelineRunner object for each month
        # parameters:
        # ai_api_key: the OpenAI API key
        # crossref_affiliation: the affiliation to use for the Crossref API
        # data_from_month: the month to start collecting data from
        # data_to_month: the month to end collecting data on
        # data_from_year: the year to start collecting data from
        # data_to_year: the year to end collecting data on
        # mongodb_uri: the URL of the MongoDB server
        # db_name: the name of the database to use
        pipeline_runner = PipelineRunner(
            ai_api_key=ai_api_key,
            crossref_affiliation="Salisbury University",
            data_from_month=int(month),
            data_to_month=int(month),
            data_from_year=int(year),
            data_to_year=int(year),
            mongodb_uri=mongodb_uri,
            db_name=db_name,
        ) 

        # Run the pipeline for the current month
        pipeline_runner.run_pipeline()
```

If you'd like to save the data to excel files in addition to the other data formats, you can do so via importing the function `get_excel_report` from `academic_metrics.runners` and calling it at the end of the script.

Full code for convenience:

```python

# dotenv is the python package responsible for handling env files
from dotenv import load_dotenv

# os is used to get the environment variables from the .env file
import os

# PipelineRunner is the main class used to run the pipeline
# get_excel_report is the function used to save the data to excel files
# it takes in a DatabaseWrapper object as a parameter, which connects to the database
# and retrives the data before writing it to 3 seperate excel files. One for each data type.
from academic_metrics.runners import PipelineRunner, get_excel_report

# DatabaseWrapper is the class used to connect to the database and retrieve the data
from academic_metrics.DB import DatabaseWrapper

# load_dotenv is used to load the environment variables from the .env file
load_dotenv()

# Get the environment variables from the .env file
# If you used the same names as the ones in the examples, you can just copy paste these
# if you used different names, you will need to change them to match the ones in your .env file
ai_api_key = os.getenv("OPENAI_API_KEY")
mongodb_uri = os.getenv("MONGODB_URI")
db_name = os.getenv("DB_NAME")

# Set the date range you want to process
# Years is a list of years as strings you want to process
# Months is a list of strings representing the months you want processed for each year
# For example if you want to process data from 2009-2024 for all months out of the year, you would do:
# Note: the process runs left to right, so from beginning of list to the end of the list,
# so this will process 2024, then 2023, then 2022, etc.
# Data will be saved after each month is processed.
years = [
    "2024",
    "2023",
    "2022",
    "2021",
    "2020",
    "2019",
    "2018",
    "2017",
    "2016",
    "2015",
    "2014",
    "2013",
    "2012",
    "2011",
    "2010",
    "2009",
]
months = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"]

# Loop over the years and months and run the pipeline for each month
#
# New objects are created for each month 
# to avoid memory issues as well as to avoid overwriting data
for year in years:
    for month in months:

        # Create a new PipelineRunner object for each month
        # parameters:
        # ai_api_key: the OpenAI API key
        # crossref_affiliation: the affiliation to use for the Crossref API
        # data_from_month: the month to start collecting data from
        # data_to_month: the month to end collecting data on
        # data_from_year: the year to start collecting data from
        # data_to_year: the year to end collecting data on
        # mongodb_uri: the URL of the MongoDB server
        # db_name: the name of the database to use
        pipeline_runner = PipelineRunner(
            ai_api_key=ai_api_key,
            crossref_affiliation="Salisbury University",
            data_from_month=int(month),
            data_to_month=int(month),
            data_from_year=int(year),
            data_to_year=int(year),
            mongodb_uri=mongodb_uri,
            db_name=db_name,
        ) 

        # Run the pipeline for the current month
        pipeline_runner.run_pipeline()

# Create a new DatabaseWrapper object so it can be given to get_excel_report
db = DatabaseWrapper(db_name=db_name, mongo_uri=mongodb_uri)

# Call the get_excel_report function, passing in the db object, to save the data to excel files
#
# Once this finishes running, you should have 3 excel files in your directory:
# article_data.xlsx, faculty_data.xlsx, and category_data.xlsx
get_excel_report(db)
```

</br>

##### 3. Run the script

```bash
python run_am.py
```

</br>

#### Option 2 (Command Line Interface)

For this options you will still need to create a python file, but the code will only be a couple lines long as you'll be passing in your arguments via the command line.

</br>

##### 1. Create the python file

Within your directory, create a new python file, for this example we will be using `run_am.py`, but you can name it whatever you want.

**Linux/Mac**:

```bash
touch run_am.py
```

**Windows (Command Prompt):**

```cmd
type nul > run_am.py
```

**Windows (PowerShell):**

```powershell
New-Item -Path run_am.py -Type File
```

You should now have a python file in your directory whose name matches the one you created.

</br>

##### 2. Copy and paste the following code into the file you just created

```python
from dotenv import load_dotenv
from academic_metrics.runners import command_line_runner

load_dotenv()

command_line_runner()
```

> [!WARNING]
> If you did not use `MONGODB_URI` and `OPENAI_API_KEY` as the variable names in the .env file, you will need to make a couple changes to the above code.

**How to use with different variable names:**

The `command_line_runner` function takes in 2 optional arguments:

- `openai_api_key_env_var_name`
- `mongodb_uri_env_var_name`

Which correspond to the names of the environment variables you used in your .env file.

To use the different names, do the following:

```python
from dotenv import load_dotenv
from academic_metrics.runners import command_line_runner

load_dotenv()

# The strings should be changes to match the names you used in your .env file
command_line_runner(
    openai_api_key_env_var_name="YOUR_OPENAI_API_KEY_ENV_VAR_NAME",
    mongodb_uri_env_var_name="YOUR_MONGODB_URI_ENV_VAR_NAME",
)
```

</br>

##### 3. Run the script

For this option you will still run the script from command line, but you will also be passing in arguments, details laid out below.

There are various command line arguments you can pass in, almost all are detailed here, but to see a complete list you can run:

```bash
python run_am.py --help
```

When running the script, you can configure the pipeline by passing in the following arguments:

- `--from-month` - The month to start collecting data from, defaults to 1
- `--to-month` - The month to end collecting data on, defaults to 12
- `--from-year` - The year to start collecting data from, defaults to 2024
- `--to-year` - The year to end collecting data on, defaults to 2024
- `--db-name` - The name of the database to use (required)
- `--crossref-affiliation` - The affiliation to use for the Crossref API, defaults to Salisbury University (required)

If you want to save the data to excel files you can pass in the `--as-excel` argument.

>[!NOTE]
> The `--as-excel` argument is an additional action, it doesn't remove the the saving to other data formats, but merely adds the excel saving functionality.

</br>

##### Examples

Say you want to collect data for every month from 2019 to 2024 for Salisbury University and save it to excel files. You would run the following command:

```bash
python run_am.py --from-month=1 \
--to-month=12 \
--from-year=2019 \
--to-year=2024 \
--crossref-affiliation="Salisbury University" \
--as-excel \
--db-name="Your_Database_Name"
```

To make this simpler, we can actually take advantage of the default values for some of the arguments.

Recall from before:

- `--from-month` defaults to `1`
- `--to-month` defaults to `12`
- `--from-year` defaults to `2024`
- `--to-year` defaults to `2024`
- `--crossref-affiliation` defaults to `Salisbury University`

Using the defaults, we can make that command much more concise:

```bash
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name"
```

</br>

**On AI Models**:

The default AI (LLM) model used for all phases is `gpt-4o-mini`. You can specify a different model for each phase independently by passing in the following arguments:

- `--pre-classification-model` - The model to use for the pre-classification step
- `--classification-model` - The model to use for the classification step
- `--theme-model` - The model to use for the theme extraction step

Here's how you would run the pipeline using the larger `gpt-4o` model:

```bash
python run_am.py --from-month=1 \
--to-month=12 \
--from-year=2019 \
--to-year=2024 \
--crossref-affiliation="Salisbury University" \
--as-excel \
--db-name="Your_Database_Name" \
--pre-classification-model="gpt-4o" \
--classification-model="gpt-4o" \
--theme-model="gpt-4o"
```

and taking advantage of the defaults:

```bash
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name" \
--pre-classification-model="gpt-4o" \
--classification-model="gpt-4o" \
--theme-model="gpt-4o"
```

>[!WARNING]
> This process consumes a lot of tokens, and OpenAI API service usage is based off the number of input/output tokens used, with each model having different cost per input/output token.
>
> You can check the cost of each model at [https://openai.com/api/pricing/](https://openai.com/api/pricing/).
>
> During testing we found that using `gpt-4o-mini` was the most cost effective.
>
> In addition we spent a lot of time testing prompts and models, our prompts have been tuned to a point where they elicit good results from `gpt-4o-mini`, thus a larger model may not be necessary to get the results you want.
>
> If you want to use a larger model like `gpt-4o` you can do so, but be warned it could end up running through your funds very quickly, depending on the size of the date range you're processing, and how many articles Crossref covers for the institution being processed.
>
> If you are interested in using a larger model, I recommend you first start with a smaller model on a limited date range to see if you're satisfied with the results.
>
> If you then decide to use a larger model such as `gpt-4o`, whether it be out of curiosity or you want to see if it provides better results, I still recommend you start with a smaller date range to get an idea of what it will cost. If you find the cost to be acceptable, then you can start expanding the date range.

</br>

**Other institutions**:

Our system uses the Crossref API to collect available data, then it scrapes the DOI link to get any missing data as well as any additional data that may be available.

We found that the Crossref API sometimes misses some Abstracts for example, our scraping process will fill in nearly all, if not all, of the missing abstracts.

Due to this, and the wealth of institutions Crossref covers, you can use the system for any institution that has a DOI link.

Here's how you'd run the same query on the system but for **University of Maryland** data:

```bash
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

You can even go back as far as you want, for example say you want to collect all data from the beginning of the 21st century:

```bash
python run_am.py \
--from-year=2000 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

Or maybe you want to collect all data as far back as possible, so you can see longterm trends and history of the institution:

```bash
python run_am.py \
--from-year=1900 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

The from year does not require that there be data that far back, it simply means that is the cutoff point for the data you want to collect.

So say you're not entirely sure what year your University started, or aren't sure how far back Crossref covers, you can simply enter a very far back year, like 1900, and the system will collect all data from that year and onwards.

---

## Wrapping Up

That's it! You've now successfully installed and run the system.

If you have any questions, need help, or have interest in collaborating on this project or others, feel free to reach out to me, contact information is provided below.

If you are a potential employer, please reach out to me by email or linkedin, contact information is provided below.

Contact information:

- Email: [spencerpresley96@gmail.com](mailto:spencerpresley96@gmail.com)
- LinkedIn: [https://www.linkedin.com/in/spencerpresley96/](https://www.linkedin.com/in/spencerpresley96/)

Happy coding!

```

Directory Structure:
.dockerignore
.gitignore
.readthedocs.yaml
CrossRefClassificationOrder.md
README.md
__init__.py
[additional_information/]
    OtherUses.md
    SiteCreationGuide.md
    [_guides/]
        _mongodb_install.md
        _python_install.md
am_deps.txt
check_deps.py
compare_deps.py
deps.txt
dockerfile
[docs/]
    .nojekyll
    Makefile
    make.bat
    requirements.txt
    [source/]
        AI.rst
        ChainBuilder.rst
        DB.rst
        [_build/]
            [linkcheck/]
                [.doctrees/]
                    AI.doctree
                    ChainBuilder.doctree
                    DB.doctree
                    ai_prompts.doctree
                    config.doctree
                    core.doctree
                    data_collection.doctree
                    dataclass_models.doctree
                    enums.doctree
                    environment.pickle
                    factories.doctree
                    index.doctree
                    orchestrators.doctree
                    other.doctree
                    runners.doctree
                    strategies.doctree
                    utils.doctree
                output.json
                output.txt
        ai_prompts.rst
        conf.py
        config.rst
        constants.rst
        core.rst
        data_collection.rst
        dataclass_models.rst
        enums.rst
        factories.rst
        index.rst
        orchestrators.rst
        other.rst
        postprocessing.rst
        runners.rst
        strategies.rst
        utils.rst
    [temp_source/]
        abstract_classifier.rst
        category_processor.rst
        configs.rst
        constants.rst
        faculty_set_postprocessor.rst
        modules.rst
pyproject.toml
requirements.txt
setup_environment.py
[src/]
    __init__.py
    [academic_metrics/]
        [AI/]
            __init__.py
            abstract_classifier.py
        [ChainBuilder/]
            ChainBuilder.py
            __init__.py
        [DB/]
            DatabaseSetup.py
            __init__.py
            article_data.json
            article_data.xlsx
            article_to_excel.py
            category_data.json
            category_data.xlsx
            clear_db.py
            faculty_data.json
            faculty_data.xlsx
            get_all_data.py
        __init__.py
        [ai_data_models/]
            __init__.py
            ai_pydantic_models.py
        [ai_prompts/]
            __init__.py
            abstract_summary_prompts.py
            classification_prompts.py
            human_prompt.py
            method_prompts.py
            sentence_analysis_prompts.py
            theme_prompts.py
        [configs/]
            __init__.py
            global_config.py
        [constants/]
            __init__.py
            dir_paths.py
        [core/]
            __init__.py
            category_processor.py
        [data_collection/]
            CrossrefWrapper.py
            __init__.py
            scraper.py
        [dataclass_models/]
            __init__.py
            abstract_base_dataclass.py
            concrete_dataclasses.py
            string_variation.py
        [enums/]
            __init__.py
            dataclass_enums.py
            enums.py
        [factories/]
            __init__.py
            abstract_classifier_factory.py
            dataclass_factory.py
            strategy_factory.py
        [logs/]
            empty-file-for-first-push
        [mapping/]
            _AbstractCategoryMap.py
            __init__.py
        [orchestrators/]
            __init__.py
            category_data_orchestrator.py
            classification_orchestrator.py
            test_processed_category_data.json
        [other/]
            __init__.py
            in_memory_taxonomy.py
        [postprocessing/]
            BasePostprocessor.py
            DepartmentPostprocessor.py
            FacultyPostprocessor.py
            __init__.py
        [runners/]
            __init__.py
            classified_data.json
            crossref_unknown_authors.json
            pipeline.py
            processed_data.json
            raw_results.json
        [strategies/]
            AttributeExtractionStrategies.py
            __init__.py
        [utils/]
            __init__.py
            api_key_validator.py
            minhash_util.py
            taxonomy_util.py
            unicode_chars_dict.py
            utilities.py
            warning_manager.py
    [assets/]
        citations.rtf
        citations.txt
        examplecategory.json
        [json_data/]
            Accounting-and-Legal-Studies.json
            Economics.json
            Finance.json
            Information-and-Decision-Sciences.json
            Management.json
            Marketing.json
            [output_verification_using_dm/]
                existingTitles_missingCitations_missingArticlesTitles.json
                needs_second_pass.json
                titles_comparison.json
                titles_from_processed_category_data_and_existing_titles_missing_citations_missing_articles_titles.json
                titles_only_in_existing.json
                wos_titles_comparison.json
        professor_department_dump.csv
        [verification_scripts/]
            [reformatJsonScripts/]
                economics_reformat.py
                [reformattedFiles/]
                    Economics_reformatted.json
                    accounting_output.json
                    accounting_output.txt
                    economics_output.json
                    economics_output.txt
                    finance_output.json
                    ids_output.json
                    management_output.json
                    marketing_output.json
    [data/]
        [core/]
            2017-2024-paper-doi-list.json
            [__crossref_split_files/]
                0_crossref_item.json
                1_crossref_item.json
                2_crossref_item.json
                3_crossref_item.json
            [_crossref_split_files/]
                0_crossref_item.json
                100_crossref_item.json
                101_crossref_item.json
                102_crossref_item.json
                103_crossref_item.json
                104_crossref_item.json
                105_crossref_item.json
                106_crossref_item.json
                107_crossref_item.json
                108_crossref_item.json
                109_crossref_item.json
                10_crossref_item.json
                110_crossref_item.json
                111_crossref_item.json
                112_crossref_item.json
                113_crossref_item.json
                114_crossref_item.json
                115_crossref_item.json
                116_crossref_item.json
                117_crossref_item.json
                118_crossref_item.json
                119_crossref_item.json
                11_crossref_item.json
                120_crossref_item.json
                121_crossref_item.json
                122_crossref_item.json
                123_crossref_item.json
                124_crossref_item.json
                125_crossref_item.json
                126_crossref_item.json
                127_crossref_item.json
                128_crossref_item.json
                129_crossref_item.json
                12_crossref_item.json
                130_crossref_item.json
                131_crossref_item.json
                132_crossref_item.json
                133_crossref_item.json
                134_crossref_item.json
                135_crossref_item.json
                136_crossref_item.json
                137_crossref_item.json
                138_crossref_item.json
                139_crossref_item.json
                13_crossref_item.json
                140_crossref_item.json
                141_crossref_item.json
                142_crossref_item.json
                143_crossref_item.json
                144_crossref_item.json
                145_crossref_item.json
                146_crossref_item.json
                147_crossref_item.json
                148_crossref_item.json
                149_crossref_item.json
                14_crossref_item.json
                150_crossref_item.json
                151_crossref_item.json
                152_crossref_item.json
                153_crossref_item.json
                154_crossref_item.json
                155_crossref_item.json
                156_crossref_item.json
                157_crossref_item.json
                158_crossref_item.json
                159_crossref_item.json
                15_crossref_item.json
                160_crossref_item.json
                161_crossref_item.json
                162_crossref_item.json
                163_crossref_item.json
                164_crossref_item.json
                165_crossref_item.json
                166_crossref_item.json
                167_crossref_item.json
                168_crossref_item.json
                169_crossref_item.json
                16_crossref_item.json
                170_crossref_item.json
                171_crossref_item.json
                172_crossref_item.json
                173_crossref_item.json
                174_crossref_item.json
                175_crossref_item.json
                176_crossref_item.json
                177_crossref_item.json
                178_crossref_item.json
                179_crossref_item.json
                17_crossref_item.json
                180_crossref_item.json
                181_crossref_item.json
                182_crossref_item.json
                183_crossref_item.json
                184_crossref_item.json
                185_crossref_item.json
                186_crossref_item.json
                187_crossref_item.json
                188_crossref_item.json
                189_crossref_item.json
                18_crossref_item.json
                190_crossref_item.json
                191_crossref_item.json
                192_crossref_item.json
                193_crossref_item.json
                194_crossref_item.json
                195_crossref_item.json
                196_crossref_item.json
                197_crossref_item.json
                198_crossref_item.json
                199_crossref_item.json
                19_crossref_item.json
                1_crossref_item.json
                200_crossref_item.json
                201_crossref_item.json
                202_crossref_item.json
                203_crossref_item.json
                204_crossref_item.json
                205_crossref_item.json
                206_crossref_item.json
                207_crossref_item.json
                208_crossref_item.json
                209_crossref_item.json
                20_crossref_item.json
                210_crossref_item.json
                211_crossref_item.json
                212_crossref_item.json
                213_crossref_item.json
                214_crossref_item.json
                215_crossref_item.json
                216_crossref_item.json
                217_crossref_item.json
                218_crossref_item.json
                219_crossref_item.json
                21_crossref_item.json
                220_crossref_item.json
                221_crossref_item.json
                222_crossref_item.json
                223_crossref_item.json
                224_crossref_item.json
                225_crossref_item.json
                226_crossref_item.json
                227_crossref_item.json
                228_crossref_item.json
                229_crossref_item.json
                22_crossref_item.json
                230_crossref_item.json
                231_crossref_item.json
                232_crossref_item.json
                233_crossref_item.json
                234_crossref_item.json
                235_crossref_item.json
                236_crossref_item.json
                237_crossref_item.json
                238_crossref_item.json
                239_crossref_item.json
                23_crossref_item.json
                240_crossref_item.json
                241_crossref_item.json
                242_crossref_item.json
                243_crossref_item.json
                244_crossref_item.json
                245_crossref_item.json
                246_crossref_item.json
                247_crossref_item.json
                248_crossref_item.json
                249_crossref_item.json
                24_crossref_item.json
                250_crossref_item.json
                251_crossref_item.json
                252_crossref_item.json
                253_crossref_item.json
                254_crossref_item.json
                255_crossref_item.json
                256_crossref_item.json
                257_crossref_item.json
                258_crossref_item.json
                259_crossref_item.json
                25_crossref_item.json
                260_crossref_item.json
                261_crossref_item.json
                262_crossref_item.json
                263_crossref_item.json
                264_crossref_item.json
                265_crossref_item.json
                266_crossref_item.json
                267_crossref_item.json
                268_crossref_item.json
                269_crossref_item.json
                26_crossref_item.json
                270_crossref_item.json
                271_crossref_item.json
                272_crossref_item.json
                273_crossref_item.json
                274_crossref_item.json
                275_crossref_item.json
                276_crossref_item.json
                277_crossref_item.json
                278_crossref_item.json
                279_crossref_item.json
                27_crossref_item.json
                280_crossref_item.json
                281_crossref_item.json
                282_crossref_item.json
                283_crossref_item.json
                284_crossref_item.json
                285_crossref_item.json
                286_crossref_item.json
                287_crossref_item.json
                288_crossref_item.json
                289_crossref_item.json
                28_crossref_item.json
                290_crossref_item.json
                291_crossref_item.json
                292_crossref_item.json
                293_crossref_item.json
                294_crossref_item.json
                295_crossref_item.json
                296_crossref_item.json
                297_crossref_item.json
                298_crossref_item.json
                299_crossref_item.json
                29_crossref_item.json
                2_crossref_item.json
                300_crossref_item.json
                301_crossref_item.json
                302_crossref_item.json
                303_crossref_item.json
                304_crossref_item.json
                305_crossref_item.json
                306_crossref_item.json
                307_crossref_item.json
                308_crossref_item.json
                309_crossref_item.json
                30_crossref_item.json
                310_crossref_item.json
                311_crossref_item.json
                312_crossref_item.json
                313_crossref_item.json
                314_crossref_item.json
                315_crossref_item.json
                316_crossref_item.json
                317_crossref_item.json
                318_crossref_item.json
                319_crossref_item.json
                31_crossref_item.json
                320_crossref_item.json
                321_crossref_item.json
                322_crossref_item.json
                323_crossref_item.json
                324_crossref_item.json
                325_crossref_item.json
                326_crossref_item.json
                327_crossref_item.json
                328_crossref_item.json
                329_crossref_item.json
                32_crossref_item.json
                330_crossref_item.json
                331_crossref_item.json
                332_crossref_item.json
                333_crossref_item.json
                334_crossref_item.json
                335_crossref_item.json
                336_crossref_item.json
                337_crossref_item.json
                338_crossref_item.json
                339_crossref_item.json
                33_crossref_item.json
                340_crossref_item.json
                341_crossref_item.json
                342_crossref_item.json
                343_crossref_item.json
                344_crossref_item.json
                345_crossref_item.json
                346_crossref_item.json
                347_crossref_item.json
                348_crossref_item.json
                349_crossref_item.json
                34_crossref_item.json
                350_crossref_item.json
                351_crossref_item.json
                352_crossref_item.json
                353_crossref_item.json
                354_crossref_item.json
                355_crossref_item.json
                356_crossref_item.json
                357_crossref_item.json
                358_crossref_item.json
                359_crossref_item.json
                35_crossref_item.json
                360_crossref_item.json
                361_crossref_item.json
                362_crossref_item.json
                363_crossref_item.json
                364_crossref_item.json
                365_crossref_item.json
                366_crossref_item.json
                367_crossref_item.json
                368_crossref_item.json
                369_crossref_item.json
                36_crossref_item.json
                370_crossref_item.json
                371_crossref_item.json
                372_crossref_item.json
                373_crossref_item.json
                374_crossref_item.json
                375_crossref_item.json
                376_crossref_item.json
                377_crossref_item.json
                378_crossref_item.json
                379_crossref_item.json
                37_crossref_item.json
                380_crossref_item.json
                381_crossref_item.json
                382_crossref_item.json
                383_crossref_item.json
                384_crossref_item.json
                385_crossref_item.json
                386_crossref_item.json
                387_crossref_item.json
                388_crossref_item.json
                389_crossref_item.json
                38_crossref_item.json
                390_crossref_item.json
                391_crossref_item.json
                392_crossref_item.json
                393_crossref_item.json
                394_crossref_item.json
                395_crossref_item.json
                396_crossref_item.json
                397_crossref_item.json
                398_crossref_item.json
                399_crossref_item.json
                39_crossref_item.json
                3_crossref_item.json
                400_crossref_item.json
                401_crossref_item.json
                402_crossref_item.json
                403_crossref_item.json
                404_crossref_item.json
                405_crossref_item.json
                406_crossref_item.json
                407_crossref_item.json
                408_crossref_item.json
                409_crossref_item.json
                40_crossref_item.json
                410_crossref_item.json
                411_crossref_item.json
                412_crossref_item.json
                413_crossref_item.json
                414_crossref_item.json
                415_crossref_item.json
                416_crossref_item.json
                417_crossref_item.json
                418_crossref_item.json
                419_crossref_item.json
                41_crossref_item.json
                420_crossref_item.json
                421_crossref_item.json
                422_crossref_item.json
                423_crossref_item.json
                424_crossref_item.json
                425_crossref_item.json
                426_crossref_item.json
                427_crossref_item.json
                428_crossref_item.json
                429_crossref_item.json
                42_crossref_item.json
                430_crossref_item.json
                431_crossref_item.json
                432_crossref_item.json
                433_crossref_item.json
                434_crossref_item.json
                435_crossref_item.json
                436_crossref_item.json
                437_crossref_item.json
                438_crossref_item.json
                439_crossref_item.json
                43_crossref_item.json
                440_crossref_item.json
                441_crossref_item.json
                442_crossref_item.json
                443_crossref_item.json
                444_crossref_item.json
                445_crossref_item.json
                446_crossref_item.json
                447_crossref_item.json
                448_crossref_item.json
                449_crossref_item.json
                44_crossref_item.json
                450_crossref_item.json
                451_crossref_item.json
                452_crossref_item.json
                453_crossref_item.json
                454_crossref_item.json
                455_crossref_item.json
                456_crossref_item.json
                457_crossref_item.json
                458_crossref_item.json
                459_crossref_item.json
                45_crossref_item.json
                460_crossref_item.json
                461_crossref_item.json
                462_crossref_item.json
                463_crossref_item.json
                464_crossref_item.json
                465_crossref_item.json
                466_crossref_item.json
                467_crossref_item.json
                468_crossref_item.json
                469_crossref_item.json
                46_crossref_item.json
                470_crossref_item.json
                471_crossref_item.json
                472_crossref_item.json
                473_crossref_item.json
                474_crossref_item.json
                475_crossref_item.json
                476_crossref_item.json
                477_crossref_item.json
                478_crossref_item.json
                479_crossref_item.json
                47_crossref_item.json
                480_crossref_item.json
                481_crossref_item.json
                482_crossref_item.json
                483_crossref_item.json
                484_crossref_item.json
                485_crossref_item.json
                486_crossref_item.json
                487_crossref_item.json
                488_crossref_item.json
                489_crossref_item.json
                48_crossref_item.json
                490_crossref_item.json
                491_crossref_item.json
                492_crossref_item.json
                493_crossref_item.json
                49_crossref_item.json
                4_crossref_item.json
                50_crossref_item.json
                51_crossref_item.json
                52_crossref_item.json
                53_crossref_item.json
                54_crossref_item.json
                55_crossref_item.json
                56_crossref_item.json
                57_crossref_item.json
                58_crossref_item.json
                59_crossref_item.json
                5_crossref_item.json
                60_crossref_item.json
                61_crossref_item.json
                62_crossref_item.json
                63_crossref_item.json
                64_crossref_item.json
                65_crossref_item.json
                66_crossref_item.json
                67_crossref_item.json
                68_crossref_item.json
                69_crossref_item.json
                6_crossref_item.json
                70_crossref_item.json
                71_crossref_item.json
                72_crossref_item.json
                73_crossref_item.json
                74_crossref_item.json
                75_crossref_item.json
                76_crossref_item.json
                77_crossref_item.json
                78_crossref_item.json
                79_crossref_item.json
                7_crossref_item.json
                80_crossref_item.json
                81_crossref_item.json
                82_crossref_item.json
                83_crossref_item.json
                84_crossref_item.json
                85_crossref_item.json
                86_crossref_item.json
                87_crossref_item.json
                88_crossref_item.json
                89_crossref_item.json
                8_crossref_item.json
                90_crossref_item.json
                91_crossref_item.json
                92_crossref_item.json
                93_crossref_item.json
                94_crossref_item.json
                95_crossref_item.json
                96_crossref_item.json
                97_crossref_item.json
                98_crossref_item.json
                99_crossref_item.json
                9_crossref_item.json
            [ai_outputs/]
                [classifications/]
                    classification_results.json
                [static_references/]
                    abstracts.py
                    classification_output.xlsx
                    classification_results.json
                    logs.txt
                    [method_extraction/]
                        method_json_output_0.json
                        method_json_output_1.json
                        method_json_output_2.json
                        method_json_output_3.json
                    output_to_excel.py
                    raw_classification_outputs.json
                    [sentence_analysis/]
                        abstract_chain_output_0.json
                        abstract_chain_output_1.json
                        abstract_chain_output_2.json
                        abstract_chain_output_3.json
                    [summary/]
                        summary_chain_output_0.json
                        summary_chain_output_1.json
                        summary_chain_output_2.json
                        summary_chain_output_3.json
                    theme_results.json
            [crossref_split_files/]
                0_crossref_item.json
            fullData.json
            [input_files/]
                test_run_data.json
            [output_files/]
                article_url_list.json
                article_url_list.py
                article_urls.json
                heatmap.py
                matrix_vis.py
                test.py
                test_json.json
                test_processed_article_stats_data.json
                test_processed_article_stats_obj_data.json
                test_processed_category_data.json
                test_processed_faculty_stats_data.json
                test_processed_global_faculty_stats_data.json
                test_vis_w_lux.ipynb
                vis_2.py
                vis_cat_stats.py
            [taxonomy/]
                taxonomy.json
            test_processed_article_stats_data.json
            test_processed_category_data.json
            test_processed_faculty_stats_data.json
        [other/]
            [ai_tests/]
                [outputs/]
                    classification_results.json
                    [method_extraction/]
                        method_json_output_0.json
                        method_json_output_1.json
                        method_json_output_2.json
                        method_json_output_3.json
                    raw_classification_outputs.json
                    [sentence_analysis/]
                        abstract_chain_output_0.json
                        abstract_chain_output_1.json
                        abstract_chain_output_2.json
                        abstract_chain_output_3.json
                    [summary/]
                        summary_chain_output_0.json
                        summary_chain_output_1.json
                        summary_chain_output_2.json
                        summary_chain_output_3.json
                    theme_results.json
            [reformattedFiles/]
                Economics_reformatted.json
                accounting_output.json
                economics_output.json
                finance_output.json
                ids_output.json
                management_output.json
                marketing_output.json
            [split_files/]
                Author:Adams, Stephen B._Title:From orchards to chips_ Silicon Valley's evolving entrepreneurial_   ecosystem.txt
                Author:Agarwal, Vinita_Title:Ayurvedic protocols of chronic pain management_ spatiotemporality as_   present moment awareness and embodied time.txt
                Author:Agarwal, Vinita_Title:Mimetic Self-Reflexivity and Intersubjectivity in Complementary and_   Alternative Medicine Practices_ The Mirror Neuron System in Breast_   Cancer Survivorship.txt
                Author:Agarwal, Vinita_Title:Patient Assessment and Chronic Pain Self-Management in Ethnomedicine_   Seasonal and Ecosystemic Embodiment in Ayurvedic Patient-Centered Care.txt
                Author:Agarwal, Vinita_Title:Patient Communication of Chronic Pain in the Complementary and_   Alternative Medicine Therapeutic Relationship.txt
                Author:Agarwal, Vinita_Title:The Provider's Body in the Therapeutic Relationship_ How Complementary_   and Alternative Medicine Providers Describe Their Work as Healers.txt
                Author:Allen, Kimberly_Title:Examining the Impact of a Nursing Course Redesign on Student Outcomes,_   Faculty Workload, and Costs.txt
                Author:Anthony, Becky_Title:Social Work Education Anti-Racism (SWEAR) Scale.txt
                Author:Anthony, Becky_Title:The impact of nontraditional and sustainable textbook options on student_   learning in a social work skills course.txt
                Author:Arban, Kathleen_Title:Evidence-Based Strategies to Reduce Anxiety in Students With Autism_   Spectrum Disorder.txt
                Author:Auerbach, Anna Jo J._Title:Exploring the Relationship between Teacher Knowledge and Active-Learning_   Implementation in Large College Biology Courses.txt
                Author:Auerbach, Anna Jo J._Title:Fourteen Recommendations to Create a More Inclusive Environment for_   LGBTQ plus Individuals in Academic Biology.txt
                Author:Austin, Jathan_Title:A note on generating primitive Pythagorean triples using matrices.txt
                Author:Austin, Jathan_Title:GENERALIZED FIBONACCI SEQUENCES IN PYTHAGOREAN TRIPLE PRESERVING_   MATRICES.txt
                Author:Austin, Jathan_Title:GENERATING PYTHAGOREAN TRIPLES OF A GIVEN HEIGHT.txt
                Author:Austin, Jathan_Title:ON PYTHAGOREAN TRIPLE PRESERVING MATRICES THAT CONTAIN FIBONACCI NUMBERS.txt
                Author:Bachran, Karsin_Title:PUPATION SITES AND CONSERVATION OF FROSTED ELFINS (LYCAENIDAE) IN_   MARYLAND, USA.txt
                Author:Barnes, Annette_Title:Cardiovascular Disease Risk Screening for Commercial Drivers Examined in_   Occupational Practice_ Implementing Evidence-Based Practice to Champion_   the Health of Essential Workers.txt
                Author:Barnes, Samuel_Title:Assessing storm surge impacts on coastal inundation due to climate_   change_ case studies of Baltimore and Dorchester County in Maryland.txt
                Author:Barrett, G. Douglas_Title:Contemporary Art and the Problem of Music_ Towards a Musical_   Contemporary Art.txt
                Author:Barrett, G. Douglas_Title:Deep (Space) Listening_ Posthuman Moonbounce in Pauline Oliveros's_   Echoes from the Moon.txt
                Author:Bemis, Rhyannon H._Title:That was last time! The effect of a delay on children's episodic_   memories of learning new facts.txt
                Author:Berns, Chelsea M._Title:The postembryonic transformation of the shell in emydine box turtles.txt
                Author:Billups, M. J._Title:How to Improve Written Case Analysis and Reduce Grading Time_ The_   One-Page, Two-Case Method.txt
                Author:Billups, M. Judith_Title:Pedagogical strategy to improve qualification alignment of students to_   the demands of potential employers.txt
                Author:Bolton, Joshua P._Title:_I'm in_ presidential campaign announcement speeches among well known_   and unknown candidates.txt
                Author:Bones, Lela_Title:Conjecture O holds for some horospherical varieties of Picard_   rank 1.txt
                Author:Bowler, Richard_Title:Demonstrating the Natural Order_ The Physiocratic Trials in Baden,_   1770-1802.txt
                Author:Boyd, Marshall_Title:Ground-nesting warblers on the eastern shore of Maryland_ declining_   population trends and the effects of forest composition and structure.txt
                Author:Bradley, Christina J._Title:Ecophysiology of mesophotic reef-building corals in Hawai'i is_   influenced by symbiont-host associations, photoacclimatization, trophic_   plasticity, and adaptation.txt
                Author:Bradley, Christina J._Title:Isotopic and genetic methods reveal the role of the gut microbiome in_   mammalian host essential amino acid metabolism.txt
                Author:Bradley, Christina J._Title:Multiple trophic pathways support fish on floodplains of California's_   Central Valley.txt
                Author:Brady, Alyssa_Title:Cerebrospinal fluid replacement solutions promote neuroglia migratory_   behaviors and spinal explant outgrowth in microfluidic culture.txt
                Author:Brannock-Cox, Jennifer_Title:Deep Participation in Underserved Communities_ A Quantitative Analysis_   of Hearken's Model for Engagement Journalism.txt
                Author:Briand, Christopher H._Title:Integrating Multiple Sources to Reconstruct the Pre- and Early_   Postcolonial Forests of the Chesapeake_ 1588-1838.txt
                Author:Cai, Jiacheng_Title:A finite volume-alternating direction implicit method for the valuation_   of American options under the Heston model.txt
                Author:Cai, Jiacheng_Title:Eugenol derivatives_ strong and long-lasting repellents against both_   undisturbed and disturbed red imported fire ants.txt
                Author:Cai, Jiacheng_Title:Larval Aggregation of Heortia vitessoides Moore (Lepidoptera_ Crambidae)_   and Evidence of Horizontal Transfer of Avermectin.txt
                Author:Cai, Jiacheng_Title:The Effects of Trichoderma Fungi on the Tunneling, Aggregation,_   and Colony-Initiation Preferences of Black-Winged Subterranean Termites,_   Odontotermes formosanus (Blattodea_ Termitidae).txt
                Author:Cai, Jiacheng_Title:Toxicity, horizontal transfer, and physiological and behavioral effects_   of cycloxaprid against Solenopsis invicta (Hymenoptera_   Formicidae).txt
                Author:Cammarano, Cristina_Title:Seeing Through Serpent and Eagle Eyes_ Teachers as Handlers of Memories.txt
                Author:Caviglia-Harris, Jill L._Title:Community is key_ estimating the impact of living learning communities_   on college retention and GPA.txt
                Author:Caviglia-Harris, Jill L._Title:Increasing Participation and Access to Economic Associations and Their_   Services.txt
                Author:Caviglia-Harris, Jill L._Title:Potential conservation gains from improved protected area management in_   the Brazilian Amazon.txt
                Author:Caviglia-Harris, Jill L._Title:Sustainability of agricultural production following deforestation in the_   tropics_ Evidence on the value of newly-deforested, long-deforested and_   forested land in the Brazilian Amazon.txt
                Author:Caviglia-Harris, Jill_Title:Do forests provide watershed services for farmers in the humid tropics_   Evidence from the Brazilian Amazon.txt
                Author:Caviglia-Harris, Jill_Title:It's not all in their heads_ the differing role of cognitive factors and_   non-cognitive traits in undergraduate success.txt
                Author:Caviglia-Harris, Jill_Title:Opening the gates_ The increasing impact of papers beyond the top five_   and other changes in economic publishing.txt
                Author:Caviglia-Harris, Jill_Title:The Brazilian Forest Code and riparian preservation areas_   spatiotemporal analysis and implications for hydrological ecosystem_   services.txt
                Author:Caviglia-Harris, Jill_Title:The color of water_ The contributions of green and blue water to_   agricultural productivity in the Western Brazilian Amazon.txt
                Author:Caviglia-Harris, Jill_Title:The six dimensions of collective leadership that advance sustainability_   objectives_ rethinking what it means to be an academic leader.txt
                Author:Caviglia-Harris, Jill_Title:Using the process approach to teach writing in economics.txt
                Author:Cha, Hoon S._Title:Sustainability Calculus in Adopting Smart Speakers-Personalized Services_   and Privacy Risks.txt
                Author:Chambers, Dustin_Title:Barriers to prosperity_ the harmful impact of entry regulations on_   income inequality.txt
                Author:Chambers, Dustin_Title:Employment and output effects of federal regulations on small business.txt
                Author:Chambers, Dustin_Title:Federal Regulation and Mortality in the 50 States.txt
                Author:Chambers, Dustin_Title:How do federal regulations affect consumer prices_ An analysis of the_   regressive effects of regulation.txt
                Author:Chambers, Dustin_Title:How many regulations does it take to get a beer_ The geography of beer_   regulations.txt
                Author:Chambers, Dustin_Title:Natural Resource Dependency and Entrepreneurship_ Are Nations with High_   Resource Rents Cursed_.txt
                Author:Chambers, Dustin_Title:Regulation and income inequality in the United States.txt
                Author:Chambers, Dustin_Title:Regulation and poverty_ an empirical examination of the relationship_   between the incidence of federal regulation and the occurrence of_   poverty across the US states.txt
                Author:Chambers, Dustin_Title:Regulation, entrepreneurship, and dynamism.txt
                Author:Chambers, Dustin_Title:Regulation, entrepreneurship, and firm size.txt
                Author:Chambers, Dustin_Title:Regulations, institutional quality and entrepreneurship.txt
                Author:Chambers, Dustin_Title:Regulatory Restrictions Across U.S. Protein Supply Chains.txt
                Author:Chambers, Dustin_Title:The economic theory of regulation and inequality.txt
                Author:Chappell, Charisse_Title:Psychology of Religion Courses in the Undergraduate Curriculum.txt
                Author:Chaudhry, Eaqan A._Title:Habitat Usage, Dietary Niche Overlap, and Potential Partitioning between_   the Endangered Spotted Turtle (Clemmys guttata) and Other Turtle_   Species.txt
                Author:Chen, Mara_Title:Enhancing the U.S. TBI data infrastructure_ geospatial perspective.txt
                Author:Chen, Xingzhi Mara_Title:The Impact of Climate Change on Environmental Sustainability and Human_   Mortality.txt
                Author:Chen, Xuan_Title:Effects of a tropical cyclone on salt marsh insect communities and_   post-cyclone reassembly processes.txt
                Author:Chen, Xuan_Title:Effects of soil-treatment with fungal biopesticides on pupation_   behaviors, emergence success and fitness of tea geometrid, Ectropis_   grisescens (Lepidoptera_ Geometridae).txt
                Author:Chen, Xuan_Title:Extensive regional variation in the phenology of insects and their_   response to temperature across North America.txt
                Author:Chen, Xuan_Title:Food Transport of Red Imported Fire Ants (Hymenoptera_ Formicidae) on_   Vertical Surfaces.txt
                Author:Chen, Xuan_Title:Food-burying behavior in red imported fire ants (Hymenoptera_   Formicidae).txt
                Author:Chen, Xuan_Title:Microbial Community Succession Along a Chronosequence in Constructed_   Salt Marsh Soils.txt
                Author:Chen, Xuan_Title:Paving Behavior in Ants and Its Potential Application in Monitoring Two_   Urban Pest Ants, Solenopsis invicta and Tapinoma_   melanocephalum.txt
                Author:Chen, Xuan_Title:Red imported fire ants (Hymenoptera_ Formicidae) cover inaccessible_   surfaces with particles to facilitate food search and transportation.txt
                Author:Chen, Xuan_Title:Red imported fire ants cover the insecticide-treated surfaces with_   particles to reduce contact toxicity.txt
                Author:Choi, Yoojin_Title:Relationship between sleep and obesity among US and South Korean college_   students.txt
                Author:Cimiluca, Mark_Title:District-wide school reform and student performance_ Evidence from_   Montgomery County, Maryland.txt
                Author:Clements, Paul_Title:Cross-modal facilitation of auditory discrimination in a frog.txt
                Author:Conrath, Ryan_Title:Disarming Montage.txt
                Author:Conrath, Ryan_Title:Space Race Cauleen Smith's Cinematic Errantry.txt
                Author:Conrath, Ryan_Title:The Ecological Cut.txt
                Author:Corfield, Jeremy R._Title:Tempo and Pattern of Avian Brain Size Evolution.txt
                Author:Coss, Derek A._Title:Can you hear_see me_ Multisensory integration of signals does not always_   facilitate mate choice.txt
                Author:Coss, Derek A._Title:Female song in eastern bluebirds varies in acoustic structure according_   to social context.txt
                Author:Coss, Derek A._Title:Migratory return rates and breeding fidelity in Eastern Bluebirds_   (Sialia sialis).txt
                Author:Coss, Derek A._Title:Silence is sexy_ soundscape complexity alters mate choice in tungara_   frogs.txt
                Author:Coss, Derek A._Title:Why do females sing_-pair communication and other song functions in_   eastern bluebirds.txt
                Author:Cronin, Andrew D._Title:Environmental heterogeneity alters mate choice behavior for multimodal_   signals.txt
                Author:Cronin, Andrew_Title:Dueling frogs_ do male green tree frogs (Hyla cinerea) eavesdrop on and_   assess nearby calling competitors_.txt
                Author:Daly, E. Susanne_Title:Zygomatic arch root position in relation to dietary type in haplorhine_   primates.txt
                Author:Davis, Jeni_Title:Prospective Teachers' Instructional Decisions and Pedagogical Moves when_   Responding to Student Thinking in Elementary Mathematics and Science_   Lessons.txt
                Author:DiBartolo, Mary C._Title:Implementing a Standardized Workflow Process to Increase the Palliative_   Care to Hospice Admission Rate.txt
                Author:DiBartolo, Mary C._Title:Rising From the Floor in Persons With Parkinson's Disease.txt
                Author:Duong, Hong_Title:Types of nonaudit service fees and earnings response coefficients in the_   post-sarbanes-oxley era.txt
                Author:Egan, Chrys_Title:The Capacious Model and Leader Identity_ An Integrative Framework.txt
                Author:Eksi, Asli_Title:Hedged Mutual Funds and Competition for Sources of Alpha.txt
                Author:Emerson, David J._Title:Investors' Responses to Social Conflict between CSR and Corporate Tax_   Avoidance.txt
                Author:Emerson, David J._Title:Psychological Distress, Burnout, and Business Student Turnover_ The Role_   of Resilience as a Coping Mechanism.txt
                Author:Emmert, Elizabeth A. B._Title:Effect of land use changes on soil microbial enzymatic activity and soil_   microbial community composition on Maryland ' s Eastern Shore.txt
                Author:Emmert, Elizabeth_Title:Guidelines for Biosafety in Teaching Laboratories Version 2.0_ A Revised_   and Updated Manual for 2019.txt
                Author:Ennerfelt, Hannah_Title:Disruption of peripheral nerve development in a zebrafish model of_   hyperglycemia.txt
                Author:Erickson, Patti T._Title:Transcriptomic analysis of hookworm Ancylostoma ceylanicum life_   cycle stages reveals changes in G-protein coupled receptor diversity_   associated with the onset of parasitism.txt
                Author:Eun, Jihyun_Title:Green Product Portfolio and Environmental Lobbying.txt
                Author:Eun, Jihyun_Title:Market performance and the loss aversion behind green management.txt
                Author:Eun, Jihyun_Title:Value of corporate political contributions from the investors'_   perspective.txt
                Author:Evans, La'Tier_Title:Galkin's lower bound conjecture holds for the Grassmannian.txt
                Author:Fennell, Patrick B. B._Title:Not My Circus, Not my Monkeys_ Frontline Employee Perceptions of_   Customer Deviant Behaviors and Service Firms' Guardianship Policies.txt
                Author:Fennell, Patrick B._Title:The effects of transaction methods on perceived contamination and_   attitude toward retailers.txt
                Author:Fennell, Patrick B._Title:The moderating role of donation quantifiers on price fairness judgments.txt
                Author:Fennell, Patrick_Title:I guess that is fair_ How the efforts of other customers influence buyer_   price fairness perceptions.txt
                Author:Finch, Maida A._Title:Language, Gender, and School-Leadership Labor Markets.txt
                Author:Finch, Maida A._Title:Reforming School Discipline_ Responses by School District Leadership to_   Revised State Guidelines for Student Codes of Conduct.txt
                Author:Folkoff, Michael E._Title:Soil drainage-class influences on the distribution of witness trees_   (1664-1700) in Wicomico County, Maryland, USA.txt
                Author:Follmer, D. Jake_Title:A latent variable analysis of the contribution of executive function to_   adult readers' comprehension of science text_ the roles of vocabulary_   ability and level of comprehension.txt
                Author:Follmer, D. Jake_Title:Examining the Role of Self-Regulated Learning Microanalysis in the_   Assessment of Learners' Regulation.txt
                Author:Follmer, Kayla B._Title:To Lead Is to Err_ The Mediating Role of Attribution in the Relationship_   Between Leader Error and Leader Ratings.txt
                Author:Fountain, William A._Title:Order of concentric and eccentric muscle actions affects metabolic_   responses.txt
                Author:Fox, James T._Title:A Culture of Organizational Grit From the Perspective of US Military_   Officers_ A Qualitative Inquiry.txt
                Author:Fox, James_Title:K-2 principal knowledge (not leadership) matters for dyslexia_   intervention.txt
                Author:Franchi, Giulia_Title:High-Resolution Monitoring of Tidal Systems Using UAV_ A Case Study on_   Poplar Island, MD (USA).txt
                Author:Franchi, Giulia_Title:Seasonality and Characterization Mapping of Restored Tidal Marsh by NDVI_   Imageries Coupling UAVs and Multispectral Camera.txt
                Author:Franzak, Judith K._Title:_We're Rural Not Dumb_ An Examination of Literacy Sponsorship.txt
                Author:Freeman, Angela R._Title:Effects of a Gonadotropin-Releasing Hormone Agonist on Sex Behavior in_   Females of the Southern Giant Pouched Rat.txt
                Author:Freeman, Angela R._Title:Sex differences in social odor discrimination by southern giant pouched_   rats (Cricetomys ansorgei).txt
                Author:French, Kara M._Title:Prejudice for Profit Escaped Nun Stories and American Catholic Print_   Culture.txt
                Author:Fritz, Heidi L._Title:Caregiving in quarantine_ Humor styles, reframing, and psychological_   well-being among parents of children with disabilities.txt
                Author:Fritz, Heidi L._Title:Coping with caregiving_ Humor styles and health outcomes among parents_   of children with disabilities.txt
                Author:Fritz, Heidi L._Title:Three dimensions of desirability of control_ divergent relations with_   psychological and physical well-being.txt
                Author:Fritz, Heidi L._Title:Why are humor styles associated with well-being, and does social_   competence matter_ Examining relations to psychological and physical_   well-being, reappraisal, and social support.txt
                Author:Gang, Kwang Wook_Title:Impact of Korean pro-market reforms on firm innovation strategies.txt
                Author:Gang, Kwang Wook_Title:U-shaped relationship between market liberalisation and technology_   exploration_ evidence from South Korean firms.txt
                Author:Gang, KwangWook_Title:Changes in foreign ownership and innovation investment_ the case of_   Korean corporate governance reforms.txt
                Author:Gang, KwangWook_Title:Patterns of alliances and acquisitions_ An exploratory study.txt
                Author:Gang, KwangWook_Title:Shareholder reactions to corporate label change_ evidence from South_   Korean firms.txt
                Author:Gang, Kwangwook_Title:The Effects of Expertise and Social Status on Team Member Influence and_   the Moderating Roles of Intragroup Conflicts.txt
                Author:Garcia, Mark J._Title:Epigenomic changes in the tungara frog (Physalaemus pustulosus)_   possible effects of introduced fungal pathogen and urbanization.txt
                Author:Genareo, Vincent R._Title:Technical Adequacy of Procedural and Conceptual Algebra Screening_   Measures in High School Algebra.txt
                Author:Gonzalez, Aston_Title:Stolen looks, people unbound_ picturing contraband people during the_   civil war.txt
                Author:Gonzalez, Aston_Title:William Dorsey and the construction of an African American history_   archive.txt
                Author:Goyens, Tom_Title:Anarchists and national identity_ The case of German radicals in the_   United States and Brazil, 1880-1945.txt
                Author:Grecay, Paul A._Title:Foraging by estuarine juveniles of two paralichthyid flounders_   experimental analyses of the effects of light level, turbidity, and prey_   type.txt
                Author:Grecay, Paul A._Title:Growth of the estuarine fish Fundulus heteroclitus in_   response to diel-cycling hypoxia and acidification_ interaction with_   temperature.txt
                Author:Green, Daniel C._Title:Access to health services among sexual minority people in the United_   States.txt
                Author:Green, Daniel C._Title:Identifying Healthcare Stereotype Threat in Older Gay Men Living with_   HIV.txt
                Author:Green, Daniel_Title:A Proposed Taxonomy for Categorizing Sexual Identities in Adolescence.txt
                Author:Green, Daniel_Title:Experiences of minority stress and access to primary care services among_   sexual minority adults in the United States.txt
                Author:Groth, Randall E. E._Title:A method for assessing students' interpretations of contextualized data.txt
                Author:Groth, Randall E._Title:Applying Design-Based Research Findings to Improve the Common Core State_   Standards for Data and Statistics in Grades 4-6.txt
                Author:Groth, Randall E._Title:Dimensions of Learning Probability Vocabulary.txt
                Author:Groth, Randall E._Title:Probability puppets.txt
                Author:Groth, Randall E._Title:Reflections on the Current and Potential K-12 Impact of the Journal_   of Statistics and Data Science Education.txt
                Author:Groth, Randall E._Title:The Relevance of Statistical Knowledge for Teaching to Health Care_   Professionals_ Reflections on a COVID-19 Press Briefing.txt
                Author:Groth, Randall E._Title:Toward a theoretical structure to characterize early probabilistic_   thinking.txt
                Author:Groth, Randall_Title:Theory-based Evaluation of Lesson Study Professional Development_   Challenges, Opportunities, and Lessons Learned.txt
                Author:Habermeyer, Ryan_Title:Skin Walking.txt
                Author:Hagadorn, Mallory A._Title:RELATIONSHIP OF DUNG BEETLE (COLEOPTFRA_ SCARABAF1DAE AND GEOTRUPDME)_   ABUNDANCE AND PARASITE CONTROL IN CATTLE ON PASTURES THROUGHOUT MARYLAND.txt
                Author:Hahn, Eugene D._Title:Regression modelling with the tilted beta distribution_ A Bayesian_   approach.txt
                Author:Hahn, Eugene D._Title:The Tilted Beta-Binomial Distribution in Overdispersed Data_ Maximum_   Likelihood and Bayesian Estimation.txt
                Author:Hall, Nicole_Title:Using Toolkits to Improve Students' Skills in Advocacy.txt
                Author:Hamilton, Stuart E._Title:Mangroves and coastal topography create economic _safe havens_ from_   tropical storms.txt
                Author:Hamilton, Stuart E._Title:The use of unmanned aircraft systems and high-resolution satellite_   imagery to monitor tilapia fish-cage aquaculture expansion in Lake_   Victoria, Kenya.txt
                Author:Hamilton, Stuart_Title:Mangroves shelter coastal economic activity from cyclones.txt
                Author:Hammond, Courtney Nicole_Title:Phytoplankton carbon and nitrogen biomass estimates are robust to volume_   measurement method and growth environment.txt
                Author:Han, Eun-Jeong_Title:Stereotype formation An examination of three contesting models in_   Vietnam.txt
                Author:Hanley, Yvonne Downie_Title:Broken engagement_ The role of grit and LMX in enhancing faculty_   engagement.txt
                Author:Harrington, Gary_Title:Word_Play_ Death of a Salesman.txt
                Author:Harrington, Gary_Title:_Mystery and Magic_ Additional Biblical Allusions in Welty's _A Worn_   Path_.txt
                Author:Hart, Jennifer_Title:Improving Inpatient Education and Follow-Up in Patients with Heart_   Failure_ A Hospital-Based Quality Improvement Project.txt
                Author:Hatley, James_Title:There is Buffalo Ecocide_ A Meditation upon Homecoming in Buffalo_   Country.txt
                Author:Hensiek, Sarah_Title:Survey of Undergraduate Students' Goals and Achievement Strategies for_   Laboratory Coursework.txt
                Author:Hill, Amanda_Title:Exploring the relationship between national identity and attitudes_   towards immigrants in the United States.txt
                Author:Hill, Brian_Title:EARNED INCOME TAX CREDITS AND INFANT HEALTH_ A LOCAL EITC INVESTIGATION.txt
                Author:Hill, Brian_Title:The effects of increasing income for vulnerable families_ better birth_   outcomes, continued disparities.txt
                Author:Hill, Brian_Title:Tournament incentives and performance_ Evidence from the WNBA.txt
                Author:Hill, Brian_Title:What Factors Entice States to Manipulate Corporate Income Tax_   Apportionment Formulas_.txt
                Author:Hinderer, Katherine A._Title:Chinese Americans' attitudes toward advance directives_ An assessment of_   outcomes based on a nursing-led intervention.txt
                Author:Hoffman, Richard C._Title:Entrepreneurial Processes and industry Development_The Case of_   Baltimore's Calming Entrepreneurs.txt
                Author:Hoffman, Richard C._Title:Producer co-operatives of the Knights of Labor_ seeking worker_   independence.txt
                Author:Hoffman, Richard_Title:Modes of governance for market entry by international franchisors_   factors affecting the choice.txt
                Author:Hogue, Aaron S._Title:The greatest threats to species.txt
                Author:Hong Duong_Title:The effects of ownership structure on dividend policy_ Evidence from_   seasoned equity offerings (SEOs).txt
                Author:Hunter, Kimberly L._Title:'Crazy love'_ nonlinearity and irrationality in mate choice.txt
                Author:Hunter, Kimberly L._Title:Covariation among multimodal components in the courtship display of the_   tungara frog.txt
                Author:Hunter, Kimberly L._Title:Creosote bush (Larrea tridentata) ploidy history along its_   diploid-tetraploid boundary in southeastern Arizona-southwestern New_   Mexico, USA.txt
                Author:Hylton, Mary E._Title:The Voter Engagement Model_ Preparing the Next Generation of Social_   Workers for Political Practice.txt
                Author:Hylton, Mary_Title:A CRT Analysis of Policy Making in Nevada_ A Case Study for Social Work_   Education.txt
                Author:Irons, Jonathan_Title:Usurpation and Brooding of Least Tern (Sternula antillarum)_   Chicks by Common Terns (Sterna hirundo).txt
                Author:Jarosinski, Judith M._Title:Nurse Faculty Shortage Voices of Nursing Program Administrators.txt
                Author:Jarosinski, Judith M._Title:_Learning How to Teach_ in Nursing_ Perspectives of Clinicians After a_   Formal Academy.txt
                Author:Jauregui, Jean E._Title:Frequency of Opioid Prescribing for Acute Low Back Pain in a Rural_   Emergency Department.txt
                Author:Jeon, Kwonchan_Title:Two-megahertz impedance index prediction equation for appendicular lean_   mass in Korean older people.txt
                Author:Jewell, Jennifer R._Title:Utilizing Technology in Social Work Education_ Development of the_   Technology Effectiveness and Social Connectedness Scale.txt
                Author:Johnson, Aaron_Title:Overcoming the challenge of low familiarity_ Can a weakly familiar brand_   signal quality with exceptionally strong warranty_.txt
                Author:Johnson, David T._Title:From Silence to Story, through Listening and Healing_ Following the_   Cinematic Arc of the Returning Combat Veteran in The Yellow Birds.txt
                Author:Jung, Kyoung-Rae_Title:Korean fathers' immigration experience.txt
                Author:Karimzad, Farzad_Title:Chronotopeography_ Nostalgia and modernity in South Delhi's linguistic_   landscape.txt
                Author:Karimzad, Farzad_Title:Chronotopic resolution, embodied subjectivity, and collective learning_   A sociolinguistic theory of survival.txt
                Author:Karimzad, Farzad_Title:Metapragmatics of normalcy_ Mobility, context, and language choice.txt
                Author:Karimzad, Farzad_Title:Multilingualism, Chronotopes, and Resolutions_ Toward an Analysis of the_   Total Sociolinguistic Fact.txt
                Author:Keethaponcalan, S. I._Title:Sole Representatives in War and Peace_ The Case of Sri Lanka's_   Liberation Tigers of Tamil Eelam.txt
                Author:Keifer, David_Title:Enthalpy and the Second Law of Thermodynamics.txt
                Author:Kim, Sook Hyun_Title:An Exploration of Human Rights and Social Work Education in the United_   States.txt
                Author:Kim, Sook Hyun_Title:Borders_ An International Comparative Analysis of Social Work's Response.txt
                Author:Kim, Yun Kyoung_Title:Inefficiencies and bias in first job placement_ the case of professional_   Asian nationals in the United States.txt
                Author:Kim, Yun-Kyoung_Title:Job insecurity and subjective sleep quality_ The role of spillover and_   gender.txt
                Author:Kim, Yun-Kyoung_Title:Predictors of employees' strike attitudes in multinational corporations_   in China_ a multilevel relational model.txt
                Author:King, Carolyne M._Title:Guided Reading_ The Influence of Visual Design on Writing with Sources.txt
                Author:Koh, Bibiana D._Title:Epistemic ethics justice_ a _radical imaginary_.txt
                Author:Koh, Bibiana D._Title:Intersectional ethics_ a pedagogical heuristic for ethical deliberation.txt
                Author:Kolstoe, Sonja_Title:A tale of two samples_ Understanding WTP differences in the age of_   social media.txt
                Author:Kolstoe, Sonja_Title:Estimating habit-forming and variety-seeking behavior_ Valuation of_   recreational birdwatching.txt
                Author:Kolstoe, Sonja_Title:Leveraging the NEON Airborne Observation Platform for_   socio-environmental systems research.txt
                Author:Kotlowski, Dean J._Title:Australia's Presidents_ Herbert Hoover and Lyndon B. Johnson Remembered.txt
                Author:Kotlowski, Dean J._Title:The presidents club revisited_ Herbert Hoover, Lyndon Johnson, and the_   politics of legacy and bipartisanship.txt
                Author:Kotlowski, Dean_Title:Ratifying Greatness_ Franklin D. Roosevelt in Film and Television.txt
                Author:Koval, Michael R._Title:Encouraging Collaboration in a Business Law Classroom_ Two Activities_   That Challenge and Engage.txt
                Author:Koval, Michael R._Title:LEGAL ENVIRONMENT DE-DENSIFIED_ MAKING IT WORK BY LETTING THEM GO.txt
                Author:Krach, Noah_Title:High-resolution bathymetries and shorelines for the Great Lakes of the_   White Nile basin.txt
                Author:Kramer, Michael E._Title:Chronic migraine with aura as a neurologic manifestation of an atrial_   myxoma-A case report.txt
                Author:Laaouad-dodoo, Soraya_Title:Traditional Games and Sports of the Women in the Kabylie.txt
                Author:Labb, Samantha A._Title:Synthesis of a Water-Soluble, Soft N-Donor BTzBP Ligand Containing Only_   CHON.txt
                Author:Leaver, Echo_Title:Stress Reduction From a Musical Intervention.txt
                Author:Lei, Shan_Title:Familiarity bias in direct stock investment by individual investors.txt
                Author:Lei, Shan_Title:Financial well-being, family financial support and depression of older_   adults in China.txt
                Author:Lei, Shan_Title:Investment in financial literacy and financial advice-seeking_   Substitutes or complements_.txt
                Author:Lei, Shan_Title:Never married individuals and homeownership in urban China.txt
                Author:Lei, Shan_Title:Use of social networks in stock investment.txt
                Author:Leonel, Ronei_Title:When CEO compensation plan based on risk changes firm strategic_   variation and strategic deviation_ The moderating role of shareholder_   return.txt
                Author:Li, Ning_Title:Endocrine therapy initiation and overall survival outcomes with omission_   of radiation therapy in older Medicare patients with early-stage_   hormone-receptor-positive breast cancer.txt
                Author:Li, Ning_Title:Health and household labor supply_ instantaneous and adaptive behavior_   of an aging workforce.txt
                Author:Li, Ning_Title:Opioid and Non-Opioid Pharmacotherapy Use for Pain Management Among_   Privately Insured Pediatric Patients With Cancer in the United States.txt
                Author:Liebgold, Eric B._Title:(Not) far from home_ No sex bias in dispersal, but limited genetic patch_   size, in an endangered species, the Spotted Turtle (Clemmys_   guttata).txt
                Author:Liebgold, Eric B._Title:Density-dependent fitness, not dispersal movements, drives temporal_   variation in spatial genetic structure in dark-eyed juncos (Junco_   hyemalis).txt
                Author:Liebgold, Eric B._Title:Effects of landscape structure and land use on turtle communities across_   the eastern United States.txt
                Author:Liebgold, Eric B._Title:Is the future female for turtles_ Climate change and wetland_   configuration predict sex ratios of a freshwater species.txt
                Author:Liebgold, Eric B._Title:The Right Light_ Tiger Salamander Capture Rates and Spectral Sensitivity.txt
                Author:Liebgold, Eric B._Title:The spread of the parthenogenetic mourning gecko, Lepidodactylus_   lugubris (Dumeril and Bibron, 1836) to Paradise Island, The Bahamas,_   with comments on citizen science observations of non-native herpetofauna.txt
                Author:MacDougall, Madison_Title:SARS-CoV-2 and Multiple Sclerosis_ Potential for Disease Exacerbation.txt
                Author:Maier, Karl J._Title:Climate to COVID, global to local, policies to people_ a biopsychosocial_   ecological framework for syndemic prevention and response in behavioral_   medicine.txt
                Author:Maier, Karl J._Title:The _modest majority and big minority_ of climate change_ Believers and_   nonbelievers are inaccurate about the extent that others agree.txt
                Author:Manole, Denise_Title:Ants of the Forest and Dune Habitats of an Atlantic Coastal Barrier_   Island.txt
                Author:Marinaro, Laura_Title:How traditional undergraduate college students define and perceive_   wellness_ A qualitative phenomenological study.txt
                Author:Marquette, Lisa_Title:Autoregulated and Non-Autoregulated Blood Flow Restriction on Acute_   Arterial Stiffness.txt
                Author:Martin, Jennifer M._Title:Records, Responsibility, and Power_ An Overview of Cataloging Ethics.txt
                Author:Mathers, Ani Manakyan_Title:Shareholder coordination and corporate innovation.txt
                Author:Mathers, Ani Manakyan_Title:The impact of stakeholder orientation on tax avoidance_ Evidence from a_   natural experiment.txt
                Author:Maykrantz, Sherry A._Title:Coping with the crisis_ the effects of psychological capital and coping_   behaviors on perceived stress.txt
                Author:Maykrantz, Sherry A._Title:How Trust in Information Sources Influences Preventative Measures_   Compliance during the COVID-19 Pandemic.txt
                Author:Maykrantz, Sherry A._Title:Self-Leadership and Psychological Capital as Key Cognitive Resources for_   Shaping Health-Protective Behaviors during the COVID-19 Pandemic.txt
                Author:Maykrantz, Sherry A._Title:Self-leadership and stress among college students_ Examining the_   moderating role of coping skillsâ€ â€ .txt
                Author:McCartney, Jason_Title:Exposing the hazards of teaching 19th century genetic science.txt
                Author:McCarty, Michael_Title:A Monk for All Seasons_ Visions of Jien (1155-1225) in Medieval Japan.txt
                Author:Mcelroy, Honor B._Title:Rural Women, Creative Writing, and Resistance.txt
                Author:Miao, Chao_Title:Emotional intelligence and service quality_ a meta-analysis with initial_   evidence on cross-cultural factors and future research directions.txt
                Author:Miao, Chao_Title:Institutional factors, religiosity, and entrepreneurial activity_ A_   quantitative examination across 85 countries.txt
                Author:Miao, Chao_Title:Internationalization and family firm performance A cross-cultural_   meta-analysis of the main effect and moderating factors.txt
                Author:Miao, Chao_Title:Relative Importance of Major Job Performance Dimensions in Determining_   Supervisors' Overall Job Performance Ratings.txt
                Author:Miao, Chao_Title:Substantial Differences in Turnover Intention Between Direct Care_   Workers in Chinese Hospitals and Long-Term Care Facilities.txt
                Author:Miao, Chao_Title:The cross-cultural moderators of the influence of emotional intelligence_   on organizational citizenship behavior and counterproductive work_   behavior.txt
                Author:Miao, Chao_Title:Watching you descend, I help others rise_ the influence of leader_   humility on prosocial motivation.txt
                Author:Miller, Jerome A._Title:On the Way to Divine Providence_ From the Abyss of Time to the Throe of_   Eternity.txt
                Author:Miller, Jerome A._Title:Robust Evolution in Historical Time.txt
                Author:Miller, Stephanie_Title:Functional Degeneracy in Paracoccus denitrificans Pd1222 Is Coordinated_   via RamB, Which Links Expression of the Glyoxylate Cycle to Activity of_   the Ethylmalonyl-CoA Pathway.txt
                Author:Montgomery, Chandini B._Title:The First Confirmed Occurrence of Myotis septentrionalis_   (Northern Long-eared Bat) on the Delmarva Peninsula.txt
                Author:Morgan, Brian_Title:Effect of an 11-Week Resistance Training Program on Arterial Stiffness_   in Young Women.txt
                Author:Morningred, Connor_Title:SURVEY AND HABITAT ASSESSMENT OF KING'S HAIRSTREAK (SATYRIUM_   KINGI) IN MARYLAND COASTAL PLAIN FORESTS.txt
                Author:Munemo, Jonathan_Title:Do African resource rents promote rent-seeking at the expense of_   entrepreneurship_.txt
                Author:Munemo, Jonathan_Title:Export entrepreneurship promotion_ The role of regulation-induced time_   delays and institutions.txt
                Author:Munemo, Jonathan_Title:The effect of regulation-driven trade barriers and governance quality on_   export entrepreneurship.txt
                Author:Nan, Wenxiu (Vince)_Title:Improving the resilience of SMEs in times of crisis_ The impact of_   mobile money amid Covid-19 in Zambia.txt
                Author:Nan, Wenxiu (Vince)_Title:Internal Relevance between Analysts' Forecasts and Target_   Prices-Informativeness and Investment Value.txt
                Author:Nan, Wenxiu (Vince)_Title:To gamble or not to gamble_ The effect of mobile money on gambling in_   Kenya.txt
                Author:Nobiling, Brandye D._Title:Reported Self-Efficacy of Health Educators During COVID-19.txt
                Author:Norman, Brandon_Title:Functional Abnormalities of Cerebellum and Motor Cortex in Spinal_   Muscular Atrophy Mice.txt
                Author:Nyland, Jennifer_Title:Increased adiposity, inflammation, metabolic disruption and dyslipidemia_   in adult male offspring of DOSS treated C57BL_6 dams.txt
                Author:Okubo, Yuki_Title:Social Justice Challenges_ Students of Color and Critical Incidents in_   the Graduate Classroom.txt
                Author:Osman, Suzanne L._Title:Addressing the Overlap_ Sexual Victimization and_or Perpetration_   Experience, and Participant Gender Predicting Rape Empathy.txt
                Author:Osman, Suzanne L._Title:Knowing a Rape Victim, Personal Sexual Victimization Experience, and_   Gender Predicting Rape Victim Empathy.txt
                Author:Osman, Suzanne L._Title:Predicting Body-Esteem Based on Type of Sexual Victimization Experience.txt
                Author:Osman, Suzanne L._Title:Predicting College Women's Body-Esteem and Self-Esteem Based on Rape_   Experience, Recency, and Labeling.txt
                Author:Osman, Suzanne L._Title:Predicting College Women's Self-esteem Based on Verbal Coercion_   Experience and Verbal Tactic Items on the Revised Sexual Experiences_   Survey.txt
                Author:Osman, Suzanne L._Title:Sexual victimization by current partner is negatively associated with_   women's sexual satisfaction.txt
                Author:Osman, Suzanne L._Title:Sexual victimization experience, acknowledgment labeling and rape_   empathy among college men and women.txt
                Author:Owens-King, Allessia P._Title:Measuring Undergraduate Social Work Students' Knowledge and_   Understanding of Privilege and Oppression.txt
                Author:Owens-King, Allessia P._Title:Secondary traumatic stress and self-care inextricably linked.txt
                Author:Padgett, Stephen M._Title:_He just teaches whatever he thinks is important_ Analysis of comments_   in student evaluations of teaching.txt
                Author:Pandey, Anjali_Title:Re-Englishing 'flat-world' fiction.txt
                Author:Pandey, Anjali_Title:_Authorized to work in the US_ Examining the myth of porous borders in_   the era of populism for practicing linguists.txt
                Author:Park, Minseok_Title:Does Forecast-Accuracy-Based Allocation Induce Customers to Share_   Truthful Order Forecasts_.txt
                Author:Park, Minseok_Title:Predicting supply chain risks through big data analytics_ role of risk_   alert tool in mitigating business disruption.txt
                Author:Park, Minseok_Title:Responding to epidemic-driven demand_ the role of supply channels.txt
                Author:Pasirayi, Simbarashe_Title:#Activism_ Investor Reactions to Corporate Sociopolitical Activism.txt
                Author:Pasirayi, Simbarashe_Title:Assessing the impact of manufacturer power on private label market share_   in an equilibrium framework.txt
                Author:Pasirayi, Simbarashe_Title:Stock market reactions to store-in-store agreements.txt
                Author:Pasirayi, Simbarashe_Title:The Effect of Mobile Payments on Retailer Firm Value_ The Moderating_   Role of Promotions, Customer Segment, and Rollout Strategy.txt
                Author:Pasirayi, Simbarashe_Title:The effect of subscription-based direct-to-consumer channel additions on_   firm value.txt
                Author:Pasirayi, Simbarashe_Title:The effect of third-party delivery partnerships on firm value.txt
                Author:Patel, Shruti_Title:Networks of Power in the Nineteenth Century_ The Sampradaya, Princely_   States and Company Rule.txt
                Author:Pellinger, Thomas K._Title:Acute Lower Leg Heating Increases Exercise Capacity in Patients With_   Peripheral Artery Disease.txt
                Author:Peng, Yuqi_Title:Airline revenue management around sporting mega-events_ an application_   using data from the Super Bowl XLIX.txt
                Author:Phillips, David S._Title:Effect of Vigorous Physical Activity on Executive Control in_   Middle-School Students.txt
                Author:Phillips, Robert A._Title:Identification of Genes Required for Enzalutamide Resistance in_   Castration-Resistant Prostate Cancer Cells in Vitro.txt
                Author:Poddar, Amit_Title:False advertising or slander_ Using location based tweets to assess_   online rating-reliability.txt
                Author:Poddar, Amit_Title:Run-of-the-Mill or Avant Garde_ Identifying restaurant category_   positioning and tastemakers from digital geo-location history.txt
                Author:Polkinghorn, Brian D._Title:Collaborative Partnering for Airport Construction Projects_   State-of-Practice.txt
                Author:Pope, Alexander_Title:Civic Engagement Among Youth Exposed to Community Violence_ Directions_   for Research and Practice.txt
                Author:Pope, Alexander_Title:Making civic engagement go viral_ Applying social epidemiology_   principles to civic education.txt
                Author:Porter, Heather D._Title:Reframing and Repositioning College Readers' Assumptions About Reading_   Through Eye Movement Miscue Analysis.txt
                Author:Presotto, Andrea_Title:Establishing the relationship between non-human primates and mangrove_   forests at the global, national, and local scales.txt
                Author:Presotto, Andrea_Title:Free-Living Aquatic Turtles as Sentinels of Salmonella spp. for_   Water Bodies.txt
                Author:Presotto, Andrea_Title:Navigating in a challenging semiarid environment_ the use of a_   route-based mental map by a small-bodied neotropical primate.txt
                Author:Presotto, Andrea_Title:Rare Bearded Capuchin (Sapajus libidinosus) Tool-Use Culture is_   Threatened by Land use Changes in Northeastern Brazil.txt
                Author:Presotto, Andrea_Title:Spatial cognition in western gorillas (Gorilla gorilla)_ an_   analysis of distance, linearity, and speed of travel routes.txt
                Author:Presotto, Andrea_Title:Spatial mapping shows that some African elephants use cognitive maps to_   navigate the core but not the periphery of their home ranges.txt
                Author:Presotto, Andrea_Title:Stone tools improve diet quality in wild monkeys.txt
                Author:Presotto, Andrea_Title:The Coexistence of People and Bearded Capuchins (Sapajus libidinosus) in_   a Nonindustrial Ecosystem_ An Assessment of Tourist and Local_   Perceptions in the Coastal Area of MaranhÃ£o, Brazil.txt
                Author:Presotto, Andrea_Title:The role of hunting on Sapajus xanthosternos' landscape of fear_   in the Atlantic Forest, Brazil.txt
                Author:Quan, Jing_Title:IT Application Maturity, Management Institutional Capability and Process_   Management Capability.txt
                Author:Quan, Jing_Title:R&D investment, intellectual capital, organizational learning, and firm_   performance_ a study of Chinese software companies.txt
                Author:Quan, Jing_Title:Risk and Revenue Management in the Chinese Auto Loan Industry.txt
                Author:Quan, Jing_Title:Software Vulnerability and Application Security Risk.txt
                Author:Quan, Jing_Title:What Do Agile, Lean, and ITIL Mean to DevOps_.txt
                Author:Ramanathan, Gurupriya_Title:A Framework for Scientific Inquiry in Preschool.txt
                Author:Ramanathan, Gurupriya_Title:Engineering in Preschool_ What Little Minds Can Teach Us About Big_   Skills.txt
                Author:Ramseyer, Craig A._Title:An Empirical Study of the Relationship between Seasonal Precipitation_   and Thermodynamic Environment in Puerto Rico.txt
                Author:Ramseyer, Craig A._Title:Future precipitation variability during the early rainfall season in the_   El Yunque National Forest.txt
                Author:Ratti, Manav_Title:Intersectionality, Sikhism, and Black feminist legal theory_   Reconceptualizing Sikh precarity and minoritization in the US and India.txt
                Author:Ratti, Manav_Title:Justice, subalternism, and literary justice_ Aravind Adiga'sThe White_   Tiger.txt
                Author:Ratti, Manav_Title:The Icon and the Text_ American Book History and the Construction of the_   World's Largest-Grossing Illustrated Book, Madonna's Sex (1992).txt
                Author:Ratti, Manav_Title:The intersections of postcolonialism, postsecularism, and literary_   studies_ Potentials, limitations, bibliographies.txt
                Author:Ravizza, Dean M._Title:Effectiveness of an oral hygiene promoting program for elementary school_   students using a smartphone endomicroscope in Thailand.txt
                Author:Rexroth, Kayla S._Title:Effects of life stage on eDNA detection of the invasive European green_   crab ( Carcinus maenas ) in estuarine systems.txt
                Author:Richerson, Rob_Title:Dynamic Right-Slanted Fonts Increase the Effectiveness of Promotional_   Retail Advertising.txt
                Author:Richerson, Rob_Title:Evolutionary motives and food behavior modeling in romantic_   relationships.txt
                Author:Rittinger, Eric R._Title:Inspiring Students to Think Theoretically About International Relations_   Through the Game of Diplomacy.txt
                Author:Rocker, Amanda_Title:Acute effects of hyperglycemia on the peripheral nervous system in_   zebrafish (Danio rerio) following nitroreductase-mediated Î²-cell_   ablation.txt
                Author:Roose, Jordan J._Title:The Development of Denitrification and of the Denitrifying Community in_   a Newly-Created Freshwater Wetland.txt
                Author:Sargent, Sheridan_Title:Regenerative potential and limitations in a zebrafish model of_   hyperglycemia-induced nerve degeneration.txt
                Author:Schlehofer, Michele M. M._Title:_Things Will Get Worse Before They Get Better_ LGBTQ plus People's_   Reactions to the 2020 US Presidential Election.txt
                Author:Schlehofer, Michele M._Title:Early Reactions of Parents to Their Trans and Gender Non-Conforming_   Children.txt
                Author:Schlehofer, Michele M._Title:Experiences of Parent-Advocates of Trans and Gender Non-Conforming Youth.txt
                Author:Schlehofer, Michele M._Title:Investing in Black LGBTQ plus liberation as white people_ A call to_   action for community psychology.txt
                Author:Schlehofer, Michele M._Title:LGBTQ.txt
                Author:Schlehofer, Michele M._Title:Public Psychology_ Introduction to the Special Issue.txt
                Author:Schlehofer, Michele M._Title:Then and now_ A 50-year retrospective thematic analysis of Society for_   Community Research and Action presidential addresses.txt
                Author:Schlehofer, Michele M._Title:_If Extended Family Can't Deal horizontal ellipsis _ Disclosing Trans_   and Gender Non-Conforming Children's Identity.txt
                Author:Schneider, Gustavo_Title:The influence of visually dynamic imagery on purchase intentions_ The_   roles of arousal and lay rationalism.txt
                Author:Schneider, Lisa_Title:On the asymptotic behavior of the q-analog of Kostant's partition_   function.txt
                Author:Schneider, Lisa_Title:When is the q-multiplicity of a weight a power of q_.txt
                Author:Schuldt, Michael A._Title:Do industry specialist auditors enhance accounting quality in the EU_   Evidence from the pre-IFRS and mandatory post-IFRS periods.txt
                Author:Scott, Michael S._Title:Developing a State University System Model to Diversify Faculty in the_   Biomedical Sciences.txt
                Author:Scott, Michael_Title:The Role of Education in Increasing Awareness and Reducing Impact of_   Natural Hazards.txt
                Author:Seldomridge, Lisa A._Title:Increasing the Number of Faculty With CNE(R) Certification_ A Statewide_   Initiative.txt
                Author:Seldomridge, Lisa A._Title:Preparing New Clinical Educators_ 10-Year Outcomes of a Hybrid Program.txt
                Author:Seldomridge, Lisa A._Title:The Maryland Graduate Nurse Faculty Scholarship_ Program Evaluation of a_   Nurse Faculty Workforce Initiative.txt
                Author:Seldomridge, Lisa A._Title:The Maryland Nurse Support Program II_ A Program Evaluation of Faculty_   Workforce Initiative.txt
                Author:Sen, Argha_Title:Managing two-sided B2B electronic markets_ Governance mechanisms,_   performance implications, and boundary conditions.txt
                Author:Sen, Argha_Title:The effect of in-store electronic word of mouth on local competitor_   spillovers in the quick service restaurant industry.txt
                Author:Shakur, Asif_Title:Backward-Design Model for an Astronomy Course Using RSpec.txt
                Author:Shakur, Asif_Title:Damped Oscillations with a Smart Cart.txt
                Author:Shakur, Asif_Title:Diffraction Experiments with a Smart Cart.txt
                Author:Shakur, Asif_Title:Flyby Measurement of the Magnetic Field of a Helmholtz Coil with a Smart_   Cart.txt
                Author:Shakur, Asif_Title:Solution to the September, 2022 Challenge Second to one.txt
                Author:Shifler, Ryan M._Title:Conjecture O holds for the odd symplectic Grassmannian.txt
                Author:Shifler, Ryan M._Title:Curve Neighborhoods of Schubert Varieties in the Odd Symplectic_   Grassmannian.txt
                Author:Shifler, Ryan M._Title:Equivariant quantum cohomology of the odd symplectic Grassmannian.txt
                Author:Shifler, Ryan M._Title:Minimum Quantum Degrees for Isotropic Grassmannians in Types B and C.txt
                Author:Shifler, Ryan M._Title:ON FROBENIUS-PERRON DIMENSION.txt
                Author:Shifler, Ryan M._Title:On the spectral properties of the quantum cohomology of odd quadrics.txt
                Author:Silaphone, K._Title:An evaluation of the Chesapeake Bay management strategy to improve water_   quality in small agricultural watersheds.txt
                Author:Silaphone, Keota_Title:Accounting for misclassification of subspecies provides insights about_   habitat use and dynamics of the Florida Grasshopper Sparrow in response_   to fire.txt
                Author:Singh, Nitya P._Title:Impact of strategic and operational risk management practices on firm_   performance_ An empirical investigation.txt
                Author:Singh, Nitya P._Title:Managing environmental uncertainty for improved firm financial_   performance_ the moderating role of supply chain risk management_   practices on managerial decision making.txt
                Author:Singh, Nitya P._Title:Managing the adverse effect of supply chain risk on corporate_   reputation_ The mediating role of corporate social responsibility_   practices.txt
                Author:Singh, Nitya P._Title:Responding to pandemic challenges_ leadership lessons from multinational_   enterprises (MNEs) in India.txt
                Author:Singh, Nitya P._Title:The rise of emerging Indian multinationals_ strategic learning for EMNC_   foreign market entry and internationalization.txt
                Author:Singh, Nitya Prasad_Title:Building supply chain risk resilience Role of big data analytics in_   supply chain disruption mitigation.txt
                Author:Singh, Nitya_Title:Developing Business Risk Resilience through Risk Management_   Infrastructure_ The Moderating Role of Big Data Analytics.txt
                Author:Small, Hannah G._Title:There's No Place Like Home_ Influential Habitat Characteristics of_   Eastern Tiger Salamander (Ambystoma tigrinum tigrinum) Breeding_   Ponds in Maryland and Delaware.txt
                Author:Smith, Kenneth J._Title:Resilience as a coping strategy for reducing auditor turnover intentions.txt
                Author:Smith, Kenneth J._Title:Resilience as a coping strategy for reducing departure intentions of_   accounting students.txt
                Author:Smith, Kenneth J._Title:Resilience, Psychological Distress, and Academic Burnout among_   Accounting Students.txt
                Author:Smith, Kenneth_Title:An examination of online cheating among business students through the_   lens of the Dark Triad and Fraud Diamond.txt
                Author:Sokoloski, Joshua E._Title:_Helicase_ activity promoted through dynamic interactions between a_   ssDNA translocase and a diffusing SSB protein.txt
                Author:Stanfield, Kellie_Title:MOBILE JOURNALISM AS LIFESTYLE JOURNALISM_ Field Theory in the_   integration of mobile in the newsroom and mobile journalist role_   conception.txt
                Author:Steele, Rachel R._Title:Moral Typecasting Explains Evaluations of Undocumented Immigrants.txt
                Author:Steele, Rachel R._Title:Muslim women negotiating their identity in the era of the Muslim ban.txt
                Author:Stoner, Alexander M._Title:MARX, CRITICAL THEORY, AND THE TREADMILL OF PRODUCTION OF VALUE_ WHY_   ENVIRONMENTAL SOCIOLOGY NEEDS A CRITIQUE OF CAPITAL.txt
                Author:Stutelberg, Erin B._Title:Teaching as invasion_ emotions, boundaries and entanglements.txt
                Author:Surak, Sarah_Title:The administrative making of the recycler.txt
                Author:Taylor, Ryan C._Title:Behavioral and neural auditory thresholds in a frog.txt
                Author:Taylor, Ryan C._Title:Complex sensory environments alter mate choice outcomes.txt
                Author:Taylor, Ryan C._Title:Evolutionary and Allometric Insights into Anuran Auditory Sensitivity_   and Morphology.txt
                Author:Teller, Kyle G._Title:Determining the most recent common ancestor in a finite linear habitat_   with asymmetric dispersal.txt
                Author:Townsend, Zachary_Title:Understanding Libraries as Part of the Rural Active Living Environment_   Evidence From a Content Analysis of Library Facebook Posts Made in_   Summer 2022.txt
                Author:Treuth, Margarita S._Title:Physical Activity Levels in Six Native American Communities Using the_   FITT-VP Approach.txt
                Author:Troup, Nicholas W._Title:Close Binary Companions to APOGEE DR16 Stars_ 20,000 Binary-star Systems_   Across the Color-Magnitude Diagram.txt
                Author:Troup, Nicholas W._Title:Geometry of the Draco C1 Symbiotic Binary.txt
                Author:Troup, Nicholas W._Title:Multiplicity Statistics of Stars in the Sagittarius Dwarf Spheroidal_   Galaxy_ Comparison to the Milky Way.txt
                Author:Troup, Nicholas W._Title:Stellar multiplicity and stellar rotation_ insights from APOGEE.txt
                Author:Troup, Nicholas W._Title:The Seventeenth Data Release of the Sloan Digital Sky Surveys_ Complete_   Release of MaNGA, MaStar, and APOGEE-2 Data.txt
                Author:Troup, Nicholas W._Title:The close binary fraction as a function of stellar parameters in APOGEE_   a strong anticorrelation with Î± abundances.txt
                Author:Troup, Nicholas_Title:Final Targeting Strategy for the Sloan Digital Sky Survey IV Apache_   Point Observatory Galactic Evolution Experiment 2 North Survey.txt
                Author:Troup, Nicholas_Title:First results from the Dark Skies, Bright Kids astronomy club_   draw-a-scientist test.txt
                Author:Troup, Nicholas_Title:The Eighteenth Data Release of the Sloan Digital Sky Surveys_ Targeting_   and First Spectra from SDSS-V.txt
                Author:Tu, Junyi_Title:Synchronization of memristive FitzHugh-Nagumo neural networks.txt
                Author:Vennos, A._Title:Dedekind sums arising from newform Eisenstein series.txt
                Author:Villalobos, Laura_Title:Local Effects of Payments for Ecosystem Services on Rural Poverty.txt
                Author:Villalobos, Laura_Title:Quantifying COVID-19_s silver lining_ Avoided deaths from air quality_   improvements in Bogota.txt
                Author:Wang, Shuangquan_Title:LAX-Score_ Quantifying Team Performance in Lacrosse and Exploring IMU_   Features towards Performance Enhancement.txt
                Author:Wathen, Bailee_Title:Buchwald-Hartwig Amination, High-Throughput Experimentation, and Process_   Chemistry_ An Introduction via Undergraduate Laboratory Experimentation.txt
                Author:Webster, Debra_Title:Advocacy, Collaboration, and Conflict Management Teaching Core Skill_   Sets in Mental Health Nursing.txt
                Author:Webster, Debra_Title:Lights, Camera, Action_ Lessons Learned From a Nursing and Theater_   Collaboration.txt
                Author:Weer, Christy H._Title:Managers' Assessments of Employees' Organizational Career Growth_   Opportunities_ The Role of Extra-Role Performance, Work Engagement, and_   Perceived Organizational Commitment.txt
                Author:Wenke, John_Title:Imposture and Subversion Charles Brockden Brown's Memoirs of Carwin_   the Biloquist.txt
                Author:Werner, Timothy J._Title:Effects of a 12-Week Resistance Training Program on Arterial Stiffness_   A Randomized Controlled Trial.txt
                Author:Wesolowski, S._Title:Exploring Bayesian parameter estimation for chiral effective field_   theory using nucleon-nucleon phase shifts.txt
                Author:Wesolowski, S._Title:Quantifying correlated truncation errors in effective field theory.txt
                Author:Wesolowski, S._Title:Rigorous constraints on three-nucleon forces in chiral effective field_   theory from fast and accurate calculations of few-body observables.txt
                Author:Wesolowski, Sarah C._Title:Extreme ultraviolet quasar colours from GALEX observations of the_   SDSS DR14Q catalogue.txt
                Author:Wille, Brendan_Title:Characterization of an L-Ascorbate Catabolic Pathway with Unprecedented_   Enzymatic Transformations.txt
                Author:Willey, Amanda_Title:Exploring Factors That Contribute to Nursing Students' Willingness to_   Report Peer Academic Integrity Violations.txt
                Author:Williamson, Thea_Title:Experiences of Alienation and Intimacy_ The Work of Secondary Writing_   Instruction.txt
                Author:Williamson, Thea_Title:Illustrating linguistic dexterity in _English mostly_ spaces_ how_   translanguaging can support academic writing in secondary ELA classrooms.txt
                Author:Williamson, Thea_Title:More, Faster, Neater_ Middle School Students' Self-Assessed Literacy_   Concerns.txt
                Author:Winter, Dorothea M._Title:A Faith-Based Intervention to Improve HYPERTENSION MANAGEMENT Among_   African Americans.txt
                Author:Wu, Ying_Title:An Analysis of State Capital Share and Its Implications to the_   Efficiency-Equality Nexus.txt
                Author:Wu, Ying_Title:Macroeconomic Impacts of the US External Imbalances with Two Large_   Emerging Asian Economies_ Japan (1970-1990) versus China (2000-2018).txt
                Author:Wu, Ying_Title:Monetary sterilization response to the movements in exchange rates and_   official net foreign assets_ a case of China.txt
                Author:Wu, Ying_Title:The ownership effect on corporate investment distortion in the_   transitional economies_ Mitigating or exacerbating_.txt
                Author:Wu, Yun_Title:Enhancing Hospital Performance_ the role of interfirm dynamic_   capabilities from the information processing view.txt
                Author:Wu, Yun_Title:Organizational mindfulness towards digital transformation as a_   prerequisite of information processing capability to achieve market_   agility.txt
                Author:Wu, Yun_Title:Value co-creation in online healthcare communities.txt
                Author:Wu, Yun_Title:eWOM, what are we suspecting_ Motivation, truthfulness or identity.txt
                Author:Yoon, David J. J._Title:Relationship conflict and counterproductive work behavior_ the roles of_   affective well-being and emotional intelligence.txt
                Author:Yoon, David J._Title:Customer courtesy and service performance_ The roles of self-efficacy_   and social context.txt
                Author:Yoon, David J._Title:Leader Behavioral Integrity and Employee In-Role Performance_ The Roles_   of Coworker Support and Job Autonomy.txt
                Author:Yoon, David J._Title:Rude customers and service performance_ roles of motivation and_   personality.txt
                Author:Yoon, David J._Title:The balance between positive and negative affect in employee well-being.txt
                Author:_Title:A Pilot Study Comparing Postmortem and Antemortem CT for the_   Identification of Unknowns_ Could a Forensic Pathologist Do It_.txt
                Author:_Title:Mattress Coil Spring Fatigue and Weight-Bearing Support_ Comparison of_   Weight-Bearing and Non-Weight-Bearing Springs.txt
            [split_files_2/]
                Author:Agarwal, Vinita_Title:Ayurvedic protocols of chronic pain management_ spatiotemporality as present moment awareness and embodied time.txt
                Author:Agarwal, Vinita_Title:Whole person healthcare as social and environmental justice_ new research directions toward a paradigmatic integration of an ecological whole system medicine framework.txt
                Author:Austin, Jathan_Title:ON PYTHAGOREAN TRIPLE PRESERVING MATRICES THAT CONTAIN FIBONACCI NUMBERS.txt
                Author:Berns, Chelsea M._Title:Bill shape reflects divergent prey consumption for island compared to mainland American kestrels (Falco sparverius).txt
                Author:Chen, Xuan_Title:Food search and transport in red imported fire ants (Hymenoptera_ Formicidae) under wet conditions.txt
                Author:Cox, Jennifer Brannock_Title:The News Sourcing Practices of Solutions Journalists in Africa, Europe, and the U.S..txt
                Author:Eksi, Asli_Title:Nonstandard Errors.txt
                Author:Eksi, Asli_Title:The Importance of Risk Preference Parameters in Prospect Theory_ Evidence from Mutual Fund Flows.txt
                Author:Emerson, Jamie_Title:Interim rank and risk-taking_ Evidence from long jump competitions.txt
                Author:Flores, Brian_Title:Re-Centering Students and Teachers_ Voices from Literacy Clinics.txt
                Author:Freeman, Angela R._Title:Characterization of oxytocin and vasopressin receptors in the Southern giant pouched rat and comparison to other rodents.txt
                Author:Gang, Kwangwook_Title:Incomplete Decisions on Reward-Based Crowdfunding Platforms_ Exploring Motivations from Temporal and Social Perspectives.txt
                Author:Green, Daniel_Title:A Proposed Taxonomy for Categorizing Sexual Identities in Adolescence.txt
                Author:Hahn, Eugene D._Title:Cross-border and domestic early-stage financial investment in 3D printing_ An empirical perspective on drivers and locations.txt
                Author:Holdai, Veera_Title:Bioabsorbable, subcutaneous naltrexone implants mitigate fentanyl-induced respiratory depression at 3 months-A pilot study in male canines.txt
                Author:Irons, Jonathan_Title:Usurpation and Brooding of Least Tern (Sternula antillarum) Chicks by Common Terns (Sterna hirundo).txt
                Author:Jarosinski, Judith M._Title:Lessons learned_ Why study-abroad remains a critical component of nursing curriculums.txt
                Author:Johnson, Aaron_Title:The effect of evidence in nonprofit donation requests_ how does mindset play a role_.txt
                Author:Johnson, David T._Title:Middle of Nowhere (2012)_ Waiting Studies, Cinema, and Temporal Experience.txt
                Author:Jung, Kyoung-Rae_Title:Korean fathers' immigration experience.txt
                Author:Keifer, David Z._Title:A Mass Spectrometry Experiment on the Degrees of Freedom Effect.txt
                Author:Kim, Yun Kyoung_Title:Inefficiencies and bias in first job placement_ the case of professional Asian nationals in the United States.txt
                Author:Koh, Bibiana D._Title:Epistemic ethics justice_ a _radical imaginary_.txt
                Author:Koh, Bibiana D._Title:Insights from the lived experience of Buddhist ethics_ implications for social work ethics education.txt
                Author:Li, Ning_Title:Opioid Overdose Hospitalizations During COVID-19_ The Experience of Pennsylvania.txt
                Author:Marinaro, Laura_Title:How traditional undergraduate college students define and perceive wellness_ A qualitative phenomenological study.txt
                Author:Miao, Chao_Title:Entrepreneurship_ an extension to anti-work perspectives.txt
                Author:Miao, Chao_Title:Fear and work performance_ A meta-analysis and future research directions.txt
                Author:Miller, Katherine R._Title:Short-term and long-term exposure to combined elevated temperature and CO2 leads to differential growth, toxicity, and fatty acid profiles in the harmful dinoflagellate Karlodinium veneficum.txt
                Author:Muller, Keaghan A._Title:Continuous Wavelet Transform Analysis of Climate Variability, Resiliency, and Restoration Strategies in Mesohaline Tidal Creeks.txt
                Author:Nan, Wenxiu (Vince)_Title:To gamble or not to gamble_ The effect of mobile money on gambling in Kenya.txt
                Author:Okubo, Yuki_Title:Making Waves in Academia_ Asian_Asian American Feminist Mentoring as Activism.txt
                Author:Osman, Suzanne L._Title:Incapacitated and_or Forcible Rape Experience Predicting College Women's Rape Victim Empathy.txt
                Author:Perret, Arnaud_Title:Beyond Analogy French and Francophone Studies and the Problem of Denomination.txt
                Author:Presotto, Andrea_Title:The Impact of Land Conversion on Primate Habitats_ Refining the Extent of Occurrence Data for Four Capuchin Species in North and Northeastern Brazil.txt
                Author:Seldomridge, Lisa A._Title:Preparing New Clinical Educators_ 10-Year Outcomes of a Hybrid Program.txt
                Author:Shifler, Ryan M._Title:Positivity determines the quantum cohomology of the odd symplectic Grassmannian of lines.txt
                Author:Spillson, Christine_Title:Teaching Creative Nonfiction in the Literature Classroom_ A Proposed Framework.txt
                Author:Sutton-Ryan, Alison_Title:Soul of the Nation_ Freedom to Exist, 2023.txt
                Author:Taylor, Susannah_Title:The Impact of Physical Activity Enjoyment, Exercise Self-Efficacy, Recording Physical Activity, and Exercise Goal Setting on Physical Activity Levels of College Students.txt
                Author:Townsend, Zachary_Title:Factors Associated With Leisure-Time Bicycling Among Adults in the United States_ An Urban-Rural Comparison.txt
                Author:Tu, Junyi_Title:NEW EFFECTIVE TRANSFORMATIONAL COMPUTATIONAL METHODS.txt
                Author:Villalobos, Laura_Title:Points, cells, or polygons_ On the choice of spatial units in forest conservation policy impact evaluation.txt
                Author:Weber, Erin M._Title:An interdisciplinary assessment of information literacy instruction.txt
                Author:Willey, Jeffrey_Title:Online Repository of Genomics Educational Resources to Enhance Nursing Knowledge.txt
                Author:Wu, Ying_Title:Environmental and social disclosure, managerial entrenchment, and investment efficiency.txt
                Author:Wu, Ying_Title:Risk-averse corporate investment behavior and the effectiveness of quantitative easing.txt
                Author:Wulf, Isabel Quintana_Title:Seeing the unseen_ abjection, social death, and neoliberal implication in Hector Tobar's The Tattooed Soldier.txt
                Author:_Title:Campus Sexual Violence and the Cost of Protecting Institutions_ Carceral Systems and Trans Student Experience.txt
                Author:_Title:Unkown.txt
    [other/]
        10-paper-doi-list.json
        5-paper-doi-list
        AbstractCategoryMap.py
        REAL_verification.py
        [RuntimeFiles/]
            professor_department_dump.csv
        __init__.py
        _faculty_set_postprocessor.py
        abstracts_to_categories.json
        article_citation_distribution.png
        article_stats_dict.pkl
        category_dict.pkl
        collaboration_network.png
        [custom_logging/]
            logger.py
            logger_Documentation.md
        [digital_measures_verification/]
            __init__.py
            verification.py
        docs.md
        faculty_citation_distribution.png
        faculty_department_manager.py
        faculty_performance_clustering.png
        faculty_stats_dict.pkl
        file_convert.py
        file_creation_log.txt
        firecrawl_test.py
        firecrawl_test_results.json
        firecrawl_test_results.md
        firecrawl_test_results_only_cs.json
        firecrawl_test_results_only_cs.md
        fullData.json
        fullRecs_to_splitRecs.py
        get_absracts.py
        get_crossref_ab.py
        get_crossref_ab_title.py
        get_crossref_authors.py
        get_crossref_titles.py
        get_n_items_from_crossref_static_data.py
        get_titles.py
        guide.md
        items_test.json
        json_transformer.py
        missing_abstracts.txt
        [outputs/]
            classification_results.json
            raw_classification_outputs.json
            theme_results.json
        paper-abstracts-list.json
        paper-authors-list.json
        paper-titles-list.json
        postProcess.json
        professor_dump_to_pandas.py
        raw_firecrawl_test_results_only_cs.md
        [reformattedFiles/]
            accounting_output.json
            economics_output.json
            finance_output.json
            ids_output.json
            management_output.json
            marketing_output.json
        savedrecs-5.txt
        savedrecs.txt
        some_stat_visualizations.py
        [split_files/]
            Author:Adams, Stephen B._Title:From orchards to chips_ Silicon Valley's evolving entrepreneurial_   ecosystem.txt
            Author:Agarwal, Vinita_Title:Ayurvedic protocols of chronic pain management_ spatiotemporality as_   present moment awareness and embodied time.txt
            Author:Agarwal, Vinita_Title:Mimetic Self-Reflexivity and Intersubjectivity in Complementary and_   Alternative Medicine Practices_ The Mirror Neuron System in Breast_   Cancer Survivorship.txt
            Author:Agarwal, Vinita_Title:Patient Assessment and Chronic Pain Self-Management in Ethnomedicine_   Seasonal and Ecosystemic Embodiment in Ayurvedic Patient-Centered Care.txt
            Author:Agarwal, Vinita_Title:Patient Communication of Chronic Pain in the Complementary and_   Alternative Medicine Therapeutic Relationship.txt
            Author:Agarwal, Vinita_Title:The Provider's Body in the Therapeutic Relationship_ How Complementary_   and Alternative Medicine Providers Describe Their Work as Healers.txt
            Author:Allen, Kimberly_Title:Examining the Impact of a Nursing Course Redesign on Student Outcomes,_   Faculty Workload, and Costs.txt
            Author:Anthony, Becky_Title:Social Work Education Anti-Racism (SWEAR) Scale.txt
            Author:Anthony, Becky_Title:The impact of nontraditional and sustainable textbook options on student_   learning in a social work skills course.txt
            Author:Arban, Kathleen_Title:Evidence-Based Strategies to Reduce Anxiety in Students With Autism_   Spectrum Disorder.txt
            Author:Auerbach, Anna Jo J._Title:Exploring the Relationship between Teacher Knowledge and Active-Learning_   Implementation in Large College Biology Courses.txt
            Author:Auerbach, Anna Jo J._Title:Fourteen Recommendations to Create a More Inclusive Environment for_   LGBTQ plus Individuals in Academic Biology.txt
            Author:Austin, Jathan_Title:A note on generating primitive Pythagorean triples using matrices.txt
            Author:Austin, Jathan_Title:GENERALIZED FIBONACCI SEQUENCES IN PYTHAGOREAN TRIPLE PRESERVING_   MATRICES.txt
            Author:Austin, Jathan_Title:GENERATING PYTHAGOREAN TRIPLES OF A GIVEN HEIGHT.txt
            Author:Austin, Jathan_Title:ON PYTHAGOREAN TRIPLE PRESERVING MATRICES THAT CONTAIN FIBONACCI NUMBERS.txt
            Author:Bachran, Karsin_Title:PUPATION SITES AND CONSERVATION OF FROSTED ELFINS (LYCAENIDAE) IN_   MARYLAND, USA.txt
            Author:Barnes, Annette_Title:Cardiovascular Disease Risk Screening for Commercial Drivers Examined in_   Occupational Practice_ Implementing Evidence-Based Practice to Champion_   the Health of Essential Workers.txt
            Author:Barnes, Samuel_Title:Assessing storm surge impacts on coastal inundation due to climate_   change_ case studies of Baltimore and Dorchester County in Maryland.txt
            Author:Barrett, G. Douglas_Title:Contemporary Art and the Problem of Music_ Towards a Musical_   Contemporary Art.txt
            Author:Barrett, G. Douglas_Title:Deep (Space) Listening_ Posthuman Moonbounce in Pauline Oliveros's_   Echoes from the Moon.txt
            Author:Bemis, Rhyannon H._Title:That was last time! The effect of a delay on children's episodic_   memories of learning new facts.txt
            Author:Berns, Chelsea M._Title:The postembryonic transformation of the shell in emydine box turtles.txt
            Author:Billups, M. J._Title:How to Improve Written Case Analysis and Reduce Grading Time_ The_   One-Page, Two-Case Method.txt
            Author:Billups, M. Judith_Title:Pedagogical strategy to improve qualification alignment of students to_   the demands of potential employers.txt
            Author:Bolton, Joshua P._Title:_I'm in_ presidential campaign announcement speeches among well known_   and unknown candidates.txt
            Author:Bones, Lela_Title:Conjecture O holds for some horospherical varieties of Picard_   rank 1.txt
            Author:Bowler, Richard_Title:Demonstrating the Natural Order_ The Physiocratic Trials in Baden,_   1770-1802.txt
            Author:Boyd, Marshall_Title:Ground-nesting warblers on the eastern shore of Maryland_ declining_   population trends and the effects of forest composition and structure.txt
            Author:Bradley, Christina J._Title:Ecophysiology of mesophotic reef-building corals in Hawai'i is_   influenced by symbiont-host associations, photoacclimatization, trophic_   plasticity, and adaptation.txt
            Author:Bradley, Christina J._Title:Isotopic and genetic methods reveal the role of the gut microbiome in_   mammalian host essential amino acid metabolism.txt
            Author:Bradley, Christina J._Title:Multiple trophic pathways support fish on floodplains of California's_   Central Valley.txt
            Author:Brady, Alyssa_Title:Cerebrospinal fluid replacement solutions promote neuroglia migratory_   behaviors and spinal explant outgrowth in microfluidic culture.txt
            Author:Brannock-Cox, Jennifer_Title:Deep Participation in Underserved Communities_ A Quantitative Analysis_   of Hearken's Model for Engagement Journalism.txt
            Author:Briand, Christopher H._Title:Integrating Multiple Sources to Reconstruct the Pre- and Early_   Postcolonial Forests of the Chesapeake_ 1588-1838.txt
            Author:Cai, Jiacheng_Title:A finite volume-alternating direction implicit method for the valuation_   of American options under the Heston model.txt
            Author:Cai, Jiacheng_Title:Eugenol derivatives_ strong and long-lasting repellents against both_   undisturbed and disturbed red imported fire ants.txt
            Author:Cai, Jiacheng_Title:Larval Aggregation of Heortia vitessoides Moore (Lepidoptera_ Crambidae)_   and Evidence of Horizontal Transfer of Avermectin.txt
            Author:Cai, Jiacheng_Title:The Effects of Trichoderma Fungi on the Tunneling, Aggregation,_   and Colony-Initiation Preferences of Black-Winged Subterranean Termites,_   Odontotermes formosanus (Blattodea_ Termitidae).txt
            Author:Cai, Jiacheng_Title:Toxicity, horizontal transfer, and physiological and behavioral effects_   of cycloxaprid against Solenopsis invicta (Hymenoptera_   Formicidae).txt
            Author:Cammarano, Cristina_Title:Seeing Through Serpent and Eagle Eyes_ Teachers as Handlers of Memories.txt
            Author:Caviglia-Harris, Jill L._Title:Community is key_ estimating the impact of living learning communities_   on college retention and GPA.txt
            Author:Caviglia-Harris, Jill L._Title:Increasing Participation and Access to Economic Associations and Their_   Services.txt
            Author:Caviglia-Harris, Jill L._Title:Potential conservation gains from improved protected area management in_   the Brazilian Amazon.txt
            Author:Caviglia-Harris, Jill L._Title:Sustainability of agricultural production following deforestation in the_   tropics_ Evidence on the value of newly-deforested, long-deforested and_   forested land in the Brazilian Amazon.txt
            Author:Caviglia-Harris, Jill_Title:Do forests provide watershed services for farmers in the humid tropics_   Evidence from the Brazilian Amazon.txt
            Author:Caviglia-Harris, Jill_Title:It's not all in their heads_ the differing role of cognitive factors and_   non-cognitive traits in undergraduate success.txt
            Author:Caviglia-Harris, Jill_Title:Opening the gates_ The increasing impact of papers beyond the top five_   and other changes in economic publishing.txt
            Author:Caviglia-Harris, Jill_Title:The Brazilian Forest Code and riparian preservation areas_   spatiotemporal analysis and implications for hydrological ecosystem_   services.txt
            Author:Caviglia-Harris, Jill_Title:The color of water_ The contributions of green and blue water to_   agricultural productivity in the Western Brazilian Amazon.txt
            Author:Caviglia-Harris, Jill_Title:The six dimensions of collective leadership that advance sustainability_   objectives_ rethinking what it means to be an academic leader.txt
            Author:Caviglia-Harris, Jill_Title:Using the process approach to teach writing in economics.txt
            Author:Cha, Hoon S._Title:Sustainability Calculus in Adopting Smart Speakers-Personalized Services_   and Privacy Risks.txt
            Author:Chambers, Dustin_Title:Barriers to prosperity_ the harmful impact of entry regulations on_   income inequality.txt
            Author:Chambers, Dustin_Title:Employment and output effects of federal regulations on small business.txt
            Author:Chambers, Dustin_Title:Federal Regulation and Mortality in the 50 States.txt
            Author:Chambers, Dustin_Title:How do federal regulations affect consumer prices_ An analysis of the_   regressive effects of regulation.txt
            Author:Chambers, Dustin_Title:How many regulations does it take to get a beer_ The geography of beer_   regulations.txt
            Author:Chambers, Dustin_Title:Natural Resource Dependency and Entrepreneurship_ Are Nations with High_   Resource Rents Cursed_.txt
            Author:Chambers, Dustin_Title:Regulation and income inequality in the United States.txt
            Author:Chambers, Dustin_Title:Regulation and poverty_ an empirical examination of the relationship_   between the incidence of federal regulation and the occurrence of_   poverty across the US states.txt
            Author:Chambers, Dustin_Title:Regulation, entrepreneurship, and dynamism.txt
            Author:Chambers, Dustin_Title:Regulation, entrepreneurship, and firm size.txt
            Author:Chambers, Dustin_Title:Regulations, institutional quality and entrepreneurship.txt
            Author:Chambers, Dustin_Title:Regulatory Restrictions Across U.S. Protein Supply Chains.txt
            Author:Chambers, Dustin_Title:The economic theory of regulation and inequality.txt
            Author:Chappell, Charisse_Title:Psychology of Religion Courses in the Undergraduate Curriculum.txt
            Author:Chaudhry, Eaqan A._Title:Habitat Usage, Dietary Niche Overlap, and Potential Partitioning between_   the Endangered Spotted Turtle (Clemmys guttata) and Other Turtle_   Species.txt
            Author:Chen, Mara_Title:Enhancing the U.S. TBI data infrastructure_ geospatial perspective.txt
            Author:Chen, Xingzhi Mara_Title:The Impact of Climate Change on Environmental Sustainability and Human_   Mortality.txt
            Author:Chen, Xuan_Title:Effects of a tropical cyclone on salt marsh insect communities and_   post-cyclone reassembly processes.txt
            Author:Chen, Xuan_Title:Effects of soil-treatment with fungal biopesticides on pupation_   behaviors, emergence success and fitness of tea geometrid, Ectropis_   grisescens (Lepidoptera_ Geometridae).txt
            Author:Chen, Xuan_Title:Extensive regional variation in the phenology of insects and their_   response to temperature across North America.txt
            Author:Chen, Xuan_Title:Food Transport of Red Imported Fire Ants (Hymenoptera_ Formicidae) on_   Vertical Surfaces.txt
            Author:Chen, Xuan_Title:Food-burying behavior in red imported fire ants (Hymenoptera_   Formicidae).txt
            Author:Chen, Xuan_Title:Microbial Community Succession Along a Chronosequence in Constructed_   Salt Marsh Soils.txt
            Author:Chen, Xuan_Title:Paving Behavior in Ants and Its Potential Application in Monitoring Two_   Urban Pest Ants, Solenopsis invicta and Tapinoma_   melanocephalum.txt
            Author:Chen, Xuan_Title:Red imported fire ants (Hymenoptera_ Formicidae) cover inaccessible_   surfaces with particles to facilitate food search and transportation.txt
            Author:Chen, Xuan_Title:Red imported fire ants cover the insecticide-treated surfaces with_   particles to reduce contact toxicity.txt
            Author:Choi, Yoojin_Title:Relationship between sleep and obesity among US and South Korean college_   students.txt
            Author:Cimiluca, Mark_Title:District-wide school reform and student performance_ Evidence from_   Montgomery County, Maryland.txt
            Author:Clements, Paul_Title:Cross-modal facilitation of auditory discrimination in a frog.txt
            Author:Conrath, Ryan_Title:Disarming Montage.txt
            Author:Conrath, Ryan_Title:Space Race Cauleen Smith's Cinematic Errantry.txt
            Author:Conrath, Ryan_Title:The Ecological Cut.txt
            Author:Corfield, Jeremy R._Title:Tempo and Pattern of Avian Brain Size Evolution.txt
            Author:Coss, Derek A._Title:Can you hear_see me_ Multisensory integration of signals does not always_   facilitate mate choice.txt
            Author:Coss, Derek A._Title:Female song in eastern bluebirds varies in acoustic structure according_   to social context.txt
            Author:Coss, Derek A._Title:Migratory return rates and breeding fidelity in Eastern Bluebirds_   (Sialia sialis).txt
            Author:Coss, Derek A._Title:Silence is sexy_ soundscape complexity alters mate choice in tungara_   frogs.txt
            Author:Coss, Derek A._Title:Why do females sing_-pair communication and other song functions in_   eastern bluebirds.txt
            Author:Cronin, Andrew D._Title:Environmental heterogeneity alters mate choice behavior for multimodal_   signals.txt
            Author:Cronin, Andrew_Title:Dueling frogs_ do male green tree frogs (Hyla cinerea) eavesdrop on and_   assess nearby calling competitors_.txt
            Author:Daly, E. Susanne_Title:Zygomatic arch root position in relation to dietary type in haplorhine_   primates.txt
            Author:Davis, Jeni_Title:Prospective Teachers' Instructional Decisions and Pedagogical Moves when_   Responding to Student Thinking in Elementary Mathematics and Science_   Lessons.txt
            Author:DiBartolo, Mary C._Title:Implementing a Standardized Workflow Process to Increase the Palliative_   Care to Hospice Admission Rate.txt
            Author:DiBartolo, Mary C._Title:Rising From the Floor in Persons With Parkinson's Disease.txt
            Author:Duong, Hong_Title:Types of nonaudit service fees and earnings response coefficients in the_   post-sarbanes-oxley era.txt
            Author:Egan, Chrys_Title:The Capacious Model and Leader Identity_ An Integrative Framework.txt
            Author:Eksi, Asli_Title:Hedged Mutual Funds and Competition for Sources of Alpha.txt
            Author:Emerson, David J._Title:Investors' Responses to Social Conflict between CSR and Corporate Tax_   Avoidance.txt
            Author:Emerson, David J._Title:Psychological Distress, Burnout, and Business Student Turnover_ The Role_   of Resilience as a Coping Mechanism.txt
            Author:Emmert, Elizabeth A. B._Title:Effect of land use changes on soil microbial enzymatic activity and soil_   microbial community composition on Maryland ' s Eastern Shore.txt
            Author:Emmert, Elizabeth_Title:Guidelines for Biosafety in Teaching Laboratories Version 2.0_ A Revised_   and Updated Manual for 2019.txt
            Author:Ennerfelt, Hannah_Title:Disruption of peripheral nerve development in a zebrafish model of_   hyperglycemia.txt
            Author:Erickson, Patti T._Title:Transcriptomic analysis of hookworm Ancylostoma ceylanicum life_   cycle stages reveals changes in G-protein coupled receptor diversity_   associated with the onset of parasitism.txt
            Author:Eun, Jihyun_Title:Green Product Portfolio and Environmental Lobbying.txt
            Author:Eun, Jihyun_Title:Market performance and the loss aversion behind green management.txt
            Author:Eun, Jihyun_Title:Value of corporate political contributions from the investors'_   perspective.txt
            Author:Evans, La'Tier_Title:Galkin's lower bound conjecture holds for the Grassmannian.txt
            Author:Fennell, Patrick B. B._Title:Not My Circus, Not my Monkeys_ Frontline Employee Perceptions of_   Customer Deviant Behaviors and Service Firms' Guardianship Policies.txt
            Author:Fennell, Patrick B._Title:The effects of transaction methods on perceived contamination and_   attitude toward retailers.txt
            Author:Fennell, Patrick B._Title:The moderating role of donation quantifiers on price fairness judgments.txt
            Author:Fennell, Patrick_Title:I guess that is fair_ How the efforts of other customers influence buyer_   price fairness perceptions.txt
            Author:Finch, Maida A._Title:Language, Gender, and School-Leadership Labor Markets.txt
            Author:Finch, Maida A._Title:Reforming School Discipline_ Responses by School District Leadership to_   Revised State Guidelines for Student Codes of Conduct.txt
            Author:Folkoff, Michael E._Title:Soil drainage-class influences on the distribution of witness trees_   (1664-1700) in Wicomico County, Maryland, USA.txt
            Author:Follmer, D. Jake_Title:A latent variable analysis of the contribution of executive function to_   adult readers' comprehension of science text_ the roles of vocabulary_   ability and level of comprehension.txt
            Author:Follmer, D. Jake_Title:Examining the Role of Self-Regulated Learning Microanalysis in the_   Assessment of Learners' Regulation.txt
            Author:Follmer, Kayla B._Title:To Lead Is to Err_ The Mediating Role of Attribution in the Relationship_   Between Leader Error and Leader Ratings.txt
            Author:Fountain, William A._Title:Order of concentric and eccentric muscle actions affects metabolic_   responses.txt
            Author:Fox, James T._Title:A Culture of Organizational Grit From the Perspective of US Military_   Officers_ A Qualitative Inquiry.txt
            Author:Fox, James_Title:K-2 principal knowledge (not leadership) matters for dyslexia_   intervention.txt
            Author:Franchi, Giulia_Title:High-Resolution Monitoring of Tidal Systems Using UAV_ A Case Study on_   Poplar Island, MD (USA).txt
            Author:Franchi, Giulia_Title:Seasonality and Characterization Mapping of Restored Tidal Marsh by NDVI_   Imageries Coupling UAVs and Multispectral Camera.txt
            Author:Franzak, Judith K._Title:_We're Rural Not Dumb_ An Examination of Literacy Sponsorship.txt
            Author:Freeman, Angela R._Title:Effects of a Gonadotropin-Releasing Hormone Agonist on Sex Behavior in_   Females of the Southern Giant Pouched Rat.txt
            Author:Freeman, Angela R._Title:Sex differences in social odor discrimination by southern giant pouched_   rats (Cricetomys ansorgei).txt
            Author:French, Kara M._Title:Prejudice for Profit Escaped Nun Stories and American Catholic Print_   Culture.txt
            Author:Fritz, Heidi L._Title:Caregiving in quarantine_ Humor styles, reframing, and psychological_   well-being among parents of children with disabilities.txt
            Author:Fritz, Heidi L._Title:Coping with caregiving_ Humor styles and health outcomes among parents_   of children with disabilities.txt
            Author:Fritz, Heidi L._Title:Three dimensions of desirability of control_ divergent relations with_   psychological and physical well-being.txt
            Author:Fritz, Heidi L._Title:Why are humor styles associated with well-being, and does social_   competence matter_ Examining relations to psychological and physical_   well-being, reappraisal, and social support.txt
            Author:Gang, Kwang Wook_Title:Impact of Korean pro-market reforms on firm innovation strategies.txt
            Author:Gang, Kwang Wook_Title:U-shaped relationship between market liberalisation and technology_   exploration_ evidence from South Korean firms.txt
            Author:Gang, KwangWook_Title:Changes in foreign ownership and innovation investment_ the case of_   Korean corporate governance reforms.txt
            Author:Gang, KwangWook_Title:Patterns of alliances and acquisitions_ An exploratory study.txt
            Author:Gang, KwangWook_Title:Shareholder reactions to corporate label change_ evidence from South_   Korean firms.txt
            Author:Gang, Kwangwook_Title:The Effects of Expertise and Social Status on Team Member Influence and_   the Moderating Roles of Intragroup Conflicts.txt
            Author:Garcia, Mark J._Title:Epigenomic changes in the tungara frog (Physalaemus pustulosus)_   possible effects of introduced fungal pathogen and urbanization.txt
            Author:Genareo, Vincent R._Title:Technical Adequacy of Procedural and Conceptual Algebra Screening_   Measures in High School Algebra.txt
            Author:Gonzalez, Aston_Title:Stolen looks, people unbound_ picturing contraband people during the_   civil war.txt
            Author:Gonzalez, Aston_Title:William Dorsey and the construction of an African American history_   archive.txt
            Author:Goyens, Tom_Title:Anarchists and national identity_ The case of German radicals in the_   United States and Brazil, 1880-1945.txt
            Author:Grecay, Paul A._Title:Foraging by estuarine juveniles of two paralichthyid flounders_   experimental analyses of the effects of light level, turbidity, and prey_   type.txt
            Author:Grecay, Paul A._Title:Growth of the estuarine fish Fundulus heteroclitus in_   response to diel-cycling hypoxia and acidification_ interaction with_   temperature.txt
            Author:Green, Daniel C._Title:Access to health services among sexual minority people in the United_   States.txt
            Author:Green, Daniel C._Title:Identifying Healthcare Stereotype Threat in Older Gay Men Living with_   HIV.txt
            Author:Green, Daniel_Title:A Proposed Taxonomy for Categorizing Sexual Identities in Adolescence.txt
            Author:Green, Daniel_Title:Experiences of minority stress and access to primary care services among_   sexual minority adults in the United States.txt
            Author:Groth, Randall E. E._Title:A method for assessing students' interpretations of contextualized data.txt
            Author:Groth, Randall E._Title:Applying Design-Based Research Findings to Improve the Common Core State_   Standards for Data and Statistics in Grades 4-6.txt
            Author:Groth, Randall E._Title:Dimensions of Learning Probability Vocabulary.txt
            Author:Groth, Randall E._Title:Probability puppets.txt
            Author:Groth, Randall E._Title:Reflections on the Current and Potential K-12 Impact of the Journal_   of Statistics and Data Science Education.txt
            Author:Groth, Randall E._Title:The Relevance of Statistical Knowledge for Teaching to Health Care_   Professionals_ Reflections on a COVID-19 Press Briefing.txt
            Author:Groth, Randall E._Title:Toward a theoretical structure to characterize early probabilistic_   thinking.txt
            Author:Groth, Randall_Title:Theory-based Evaluation of Lesson Study Professional Development_   Challenges, Opportunities, and Lessons Learned.txt
            Author:Habermeyer, Ryan_Title:Skin Walking.txt
            Author:Hagadorn, Mallory A._Title:RELATIONSHIP OF DUNG BEETLE (COLEOPTFRA_ SCARABAF1DAE AND GEOTRUPDME)_   ABUNDANCE AND PARASITE CONTROL IN CATTLE ON PASTURES THROUGHOUT MARYLAND.txt
            Author:Hahn, Eugene D._Title:Regression modelling with the tilted beta distribution_ A Bayesian_   approach.txt
            Author:Hahn, Eugene D._Title:The Tilted Beta-Binomial Distribution in Overdispersed Data_ Maximum_   Likelihood and Bayesian Estimation.txt
            Author:Hall, Nicole_Title:Using Toolkits to Improve Students' Skills in Advocacy.txt
            Author:Hamilton, Stuart E._Title:Mangroves and coastal topography create economic _safe havens_ from_   tropical storms.txt
            Author:Hamilton, Stuart E._Title:The use of unmanned aircraft systems and high-resolution satellite_   imagery to monitor tilapia fish-cage aquaculture expansion in Lake_   Victoria, Kenya.txt
            Author:Hamilton, Stuart_Title:Mangroves shelter coastal economic activity from cyclones.txt
            Author:Hammond, Courtney Nicole_Title:Phytoplankton carbon and nitrogen biomass estimates are robust to volume_   measurement method and growth environment.txt
            Author:Han, Eun-Jeong_Title:Stereotype formation An examination of three contesting models in_   Vietnam.txt
            Author:Hanley, Yvonne Downie_Title:Broken engagement_ The role of grit and LMX in enhancing faculty_   engagement.txt
            Author:Harrington, Gary_Title:Word_Play_ Death of a Salesman.txt
            Author:Harrington, Gary_Title:_Mystery and Magic_ Additional Biblical Allusions in Welty's _A Worn_   Path_.txt
            Author:Hart, Jennifer_Title:Improving Inpatient Education and Follow-Up in Patients with Heart_   Failure_ A Hospital-Based Quality Improvement Project.txt
            Author:Hatley, James_Title:There is Buffalo Ecocide_ A Meditation upon Homecoming in Buffalo_   Country.txt
            Author:Hensiek, Sarah_Title:Survey of Undergraduate Students' Goals and Achievement Strategies for_   Laboratory Coursework.txt
            Author:Hill, Amanda_Title:Exploring the relationship between national identity and attitudes_   towards immigrants in the United States.txt
            Author:Hill, Brian_Title:EARNED INCOME TAX CREDITS AND INFANT HEALTH_ A LOCAL EITC INVESTIGATION.txt
            Author:Hill, Brian_Title:The effects of increasing income for vulnerable families_ better birth_   outcomes, continued disparities.txt
            Author:Hill, Brian_Title:Tournament incentives and performance_ Evidence from the WNBA.txt
            Author:Hill, Brian_Title:What Factors Entice States to Manipulate Corporate Income Tax_   Apportionment Formulas_.txt
            Author:Hinderer, Katherine A._Title:Chinese Americans' attitudes toward advance directives_ An assessment of_   outcomes based on a nursing-led intervention.txt
            Author:Hoffman, Richard C._Title:Entrepreneurial Processes and industry Development_The Case of_   Baltimore's Calming Entrepreneurs.txt
            Author:Hoffman, Richard C._Title:Producer co-operatives of the Knights of Labor_ seeking worker_   independence.txt
            Author:Hoffman, Richard_Title:Modes of governance for market entry by international franchisors_   factors affecting the choice.txt
            Author:Hogue, Aaron S._Title:The greatest threats to species.txt
            Author:Hong Duong_Title:The effects of ownership structure on dividend policy_ Evidence from_   seasoned equity offerings (SEOs).txt
            Author:Hunter, Kimberly L._Title:'Crazy love'_ nonlinearity and irrationality in mate choice.txt
            Author:Hunter, Kimberly L._Title:Covariation among multimodal components in the courtship display of the_   tungara frog.txt
            Author:Hunter, Kimberly L._Title:Creosote bush (Larrea tridentata) ploidy history along its_   diploid-tetraploid boundary in southeastern Arizona-southwestern New_   Mexico, USA.txt
            Author:Hylton, Mary E._Title:The Voter Engagement Model_ Preparing the Next Generation of Social_   Workers for Political Practice.txt
            Author:Hylton, Mary_Title:A CRT Analysis of Policy Making in Nevada_ A Case Study for Social Work_   Education.txt
            Author:Irons, Jonathan_Title:Usurpation and Brooding of Least Tern (Sternula antillarum)_   Chicks by Common Terns (Sterna hirundo).txt
            Author:Jarosinski, Judith M._Title:Nurse Faculty Shortage Voices of Nursing Program Administrators.txt
            Author:Jarosinski, Judith M._Title:_Learning How to Teach_ in Nursing_ Perspectives of Clinicians After a_   Formal Academy.txt
            Author:Jauregui, Jean E._Title:Frequency of Opioid Prescribing for Acute Low Back Pain in a Rural_   Emergency Department.txt
            Author:Jeon, Kwonchan_Title:Two-megahertz impedance index prediction equation for appendicular lean_   mass in Korean older people.txt
            Author:Jewell, Jennifer R._Title:Utilizing Technology in Social Work Education_ Development of the_   Technology Effectiveness and Social Connectedness Scale.txt
            Author:Johnson, Aaron_Title:Overcoming the challenge of low familiarity_ Can a weakly familiar brand_   signal quality with exceptionally strong warranty_.txt
            Author:Johnson, David T._Title:From Silence to Story, through Listening and Healing_ Following the_   Cinematic Arc of the Returning Combat Veteran in The Yellow Birds.txt
            Author:Jung, Kyoung-Rae_Title:Korean fathers' immigration experience.txt
            Author:Karimzad, Farzad_Title:Chronotopeography_ Nostalgia and modernity in South Delhi's linguistic_   landscape.txt
            Author:Karimzad, Farzad_Title:Chronotopic resolution, embodied subjectivity, and collective learning_   A sociolinguistic theory of survival.txt
            Author:Karimzad, Farzad_Title:Metapragmatics of normalcy_ Mobility, context, and language choice.txt
            Author:Karimzad, Farzad_Title:Multilingualism, Chronotopes, and Resolutions_ Toward an Analysis of the_   Total Sociolinguistic Fact.txt
            Author:Keethaponcalan, S. I._Title:Sole Representatives in War and Peace_ The Case of Sri Lanka's_   Liberation Tigers of Tamil Eelam.txt
            Author:Keifer, David_Title:Enthalpy and the Second Law of Thermodynamics.txt
            Author:Kim, Sook Hyun_Title:An Exploration of Human Rights and Social Work Education in the United_   States.txt
            Author:Kim, Sook Hyun_Title:Borders_ An International Comparative Analysis of Social Work's Response.txt
            Author:Kim, Yun Kyoung_Title:Inefficiencies and bias in first job placement_ the case of professional_   Asian nationals in the United States.txt
            Author:Kim, Yun-Kyoung_Title:Job insecurity and subjective sleep quality_ The role of spillover and_   gender.txt
            Author:Kim, Yun-Kyoung_Title:Predictors of employees' strike attitudes in multinational corporations_   in China_ a multilevel relational model.txt
            Author:King, Carolyne M._Title:Guided Reading_ The Influence of Visual Design on Writing with Sources.txt
            Author:Koh, Bibiana D._Title:Epistemic ethics justice_ a _radical imaginary_.txt
            Author:Koh, Bibiana D._Title:Intersectional ethics_ a pedagogical heuristic for ethical deliberation.txt
            Author:Kolstoe, Sonja_Title:A tale of two samples_ Understanding WTP differences in the age of_   social media.txt
            Author:Kolstoe, Sonja_Title:Estimating habit-forming and variety-seeking behavior_ Valuation of_   recreational birdwatching.txt
            Author:Kolstoe, Sonja_Title:Leveraging the NEON Airborne Observation Platform for_   socio-environmental systems research.txt
            Author:Kotlowski, Dean J._Title:Australia's Presidents_ Herbert Hoover and Lyndon B. Johnson Remembered.txt
            Author:Kotlowski, Dean J._Title:The presidents club revisited_ Herbert Hoover, Lyndon Johnson, and the_   politics of legacy and bipartisanship.txt
            Author:Kotlowski, Dean_Title:Ratifying Greatness_ Franklin D. Roosevelt in Film and Television.txt
            Author:Koval, Michael R._Title:Encouraging Collaboration in a Business Law Classroom_ Two Activities_   That Challenge and Engage.txt
            Author:Koval, Michael R._Title:LEGAL ENVIRONMENT DE-DENSIFIED_ MAKING IT WORK BY LETTING THEM GO.txt
            Author:Krach, Noah_Title:High-resolution bathymetries and shorelines for the Great Lakes of the_   White Nile basin.txt
            Author:Kramer, Michael E._Title:Chronic migraine with aura as a neurologic manifestation of an atrial_   myxoma-A case report.txt
            Author:Laaouad-dodoo, Soraya_Title:Traditional Games and Sports of the Women in the Kabylie.txt
            Author:Labb, Samantha A._Title:Synthesis of a Water-Soluble, Soft N-Donor BTzBP Ligand Containing Only_   CHON.txt
            Author:Leaver, Echo_Title:Stress Reduction From a Musical Intervention.txt
            Author:Lei, Shan_Title:Familiarity bias in direct stock investment by individual investors.txt
            Author:Lei, Shan_Title:Financial well-being, family financial support and depression of older_   adults in China.txt
            Author:Lei, Shan_Title:Investment in financial literacy and financial advice-seeking_   Substitutes or complements_.txt
            Author:Lei, Shan_Title:Never married individuals and homeownership in urban China.txt
            Author:Lei, Shan_Title:Use of social networks in stock investment.txt
            Author:Leonel, Ronei_Title:When CEO compensation plan based on risk changes firm strategic_   variation and strategic deviation_ The moderating role of shareholder_   return.txt
            Author:Li, Ning_Title:Endocrine therapy initiation and overall survival outcomes with omission_   of radiation therapy in older Medicare patients with early-stage_   hormone-receptor-positive breast cancer.txt
            Author:Li, Ning_Title:Health and household labor supply_ instantaneous and adaptive behavior_   of an aging workforce.txt
            Author:Li, Ning_Title:Opioid and Non-Opioid Pharmacotherapy Use for Pain Management Among_   Privately Insured Pediatric Patients With Cancer in the United States.txt
            Author:Liebgold, Eric B._Title:(Not) far from home_ No sex bias in dispersal, but limited genetic patch_   size, in an endangered species, the Spotted Turtle (Clemmys_   guttata).txt
            Author:Liebgold, Eric B._Title:Density-dependent fitness, not dispersal movements, drives temporal_   variation in spatial genetic structure in dark-eyed juncos (Junco_   hyemalis).txt
            Author:Liebgold, Eric B._Title:Effects of landscape structure and land use on turtle communities across_   the eastern United States.txt
            Author:Liebgold, Eric B._Title:Is the future female for turtles_ Climate change and wetland_   configuration predict sex ratios of a freshwater species.txt
            Author:Liebgold, Eric B._Title:The Right Light_ Tiger Salamander Capture Rates and Spectral Sensitivity.txt
            Author:Liebgold, Eric B._Title:The spread of the parthenogenetic mourning gecko, Lepidodactylus_   lugubris (Dumeril and Bibron, 1836) to Paradise Island, The Bahamas,_   with comments on citizen science observations of non-native herpetofauna.txt
            Author:MacDougall, Madison_Title:SARS-CoV-2 and Multiple Sclerosis_ Potential for Disease Exacerbation.txt
            Author:Maier, Karl J._Title:Climate to COVID, global to local, policies to people_ a biopsychosocial_   ecological framework for syndemic prevention and response in behavioral_   medicine.txt
            Author:Maier, Karl J._Title:The _modest majority and big minority_ of climate change_ Believers and_   nonbelievers are inaccurate about the extent that others agree.txt
            Author:Manole, Denise_Title:Ants of the Forest and Dune Habitats of an Atlantic Coastal Barrier_   Island.txt
            Author:Marinaro, Laura_Title:How traditional undergraduate college students define and perceive_   wellness_ A qualitative phenomenological study.txt
            Author:Marquette, Lisa_Title:Autoregulated and Non-Autoregulated Blood Flow Restriction on Acute_   Arterial Stiffness.txt
            Author:Martin, Jennifer M._Title:Records, Responsibility, and Power_ An Overview of Cataloging Ethics.txt
            Author:Mathers, Ani Manakyan_Title:Shareholder coordination and corporate innovation.txt
            Author:Mathers, Ani Manakyan_Title:The impact of stakeholder orientation on tax avoidance_ Evidence from a_   natural experiment.txt
            Author:Maykrantz, Sherry A._Title:Coping with the crisis_ the effects of psychological capital and coping_   behaviors on perceived stress.txt
            Author:Maykrantz, Sherry A._Title:How Trust in Information Sources Influences Preventative Measures_   Compliance during the COVID-19 Pandemic.txt
            Author:Maykrantz, Sherry A._Title:Self-Leadership and Psychological Capital as Key Cognitive Resources for_   Shaping Health-Protective Behaviors during the COVID-19 Pandemic.txt
            Author:Maykrantz, Sherry A._Title:Self-leadership and stress among college students_ Examining the_   moderating role of coping skillsâ€ â€ .txt
            Author:McCartney, Jason_Title:Exposing the hazards of teaching 19th century genetic science.txt
            Author:McCarty, Michael_Title:A Monk for All Seasons_ Visions of Jien (1155-1225) in Medieval Japan.txt
            Author:Mcelroy, Honor B._Title:Rural Women, Creative Writing, and Resistance.txt
            Author:Miao, Chao_Title:Emotional intelligence and service quality_ a meta-analysis with initial_   evidence on cross-cultural factors and future research directions.txt
            Author:Miao, Chao_Title:Institutional factors, religiosity, and entrepreneurial activity_ A_   quantitative examination across 85 countries.txt
            Author:Miao, Chao_Title:Internationalization and family firm performance A cross-cultural_   meta-analysis of the main effect and moderating factors.txt
            Author:Miao, Chao_Title:Relative Importance of Major Job Performance Dimensions in Determining_   Supervisors' Overall Job Performance Ratings.txt
            Author:Miao, Chao_Title:Substantial Differences in Turnover Intention Between Direct Care_   Workers in Chinese Hospitals and Long-Term Care Facilities.txt
            Author:Miao, Chao_Title:The cross-cultural moderators of the influence of emotional intelligence_   on organizational citizenship behavior and counterproductive work_   behavior.txt
            Author:Miao, Chao_Title:Watching you descend, I help others rise_ the influence of leader_   humility on prosocial motivation.txt
            Author:Miller, Jerome A._Title:On the Way to Divine Providence_ From the Abyss of Time to the Throe of_   Eternity.txt
            Author:Miller, Jerome A._Title:Robust Evolution in Historical Time.txt
            Author:Miller, Stephanie_Title:Functional Degeneracy in Paracoccus denitrificans Pd1222 Is Coordinated_   via RamB, Which Links Expression of the Glyoxylate Cycle to Activity of_   the Ethylmalonyl-CoA Pathway.txt
            Author:Montgomery, Chandini B._Title:The First Confirmed Occurrence of Myotis septentrionalis_   (Northern Long-eared Bat) on the Delmarva Peninsula.txt
            Author:Morgan, Brian_Title:Effect of an 11-Week Resistance Training Program on Arterial Stiffness_   in Young Women.txt
            Author:Morningred, Connor_Title:SURVEY AND HABITAT ASSESSMENT OF KING'S HAIRSTREAK (SATYRIUM_   KINGI) IN MARYLAND COASTAL PLAIN FORESTS.txt
            Author:Munemo, Jonathan_Title:Do African resource rents promote rent-seeking at the expense of_   entrepreneurship_.txt
            Author:Munemo, Jonathan_Title:Export entrepreneurship promotion_ The role of regulation-induced time_   delays and institutions.txt
            Author:Munemo, Jonathan_Title:The effect of regulation-driven trade barriers and governance quality on_   export entrepreneurship.txt
            Author:Nan, Wenxiu (Vince)_Title:Improving the resilience of SMEs in times of crisis_ The impact of_   mobile money amid Covid-19 in Zambia.txt
            Author:Nan, Wenxiu (Vince)_Title:Internal Relevance between Analysts' Forecasts and Target_   Prices-Informativeness and Investment Value.txt
            Author:Nan, Wenxiu (Vince)_Title:To gamble or not to gamble_ The effect of mobile money on gambling in_   Kenya.txt
            Author:Nobiling, Brandye D._Title:Reported Self-Efficacy of Health Educators During COVID-19.txt
            Author:Norman, Brandon_Title:Functional Abnormalities of Cerebellum and Motor Cortex in Spinal_   Muscular Atrophy Mice.txt
            Author:Nyland, Jennifer_Title:Increased adiposity, inflammation, metabolic disruption and dyslipidemia_   in adult male offspring of DOSS treated C57BL_6 dams.txt
            Author:Okubo, Yuki_Title:Social Justice Challenges_ Students of Color and Critical Incidents in_   the Graduate Classroom.txt
            Author:Osman, Suzanne L._Title:Addressing the Overlap_ Sexual Victimization and_or Perpetration_   Experience, and Participant Gender Predicting Rape Empathy.txt
            Author:Osman, Suzanne L._Title:Knowing a Rape Victim, Personal Sexual Victimization Experience, and_   Gender Predicting Rape Victim Empathy.txt
            Author:Osman, Suzanne L._Title:Predicting Body-Esteem Based on Type of Sexual Victimization Experience.txt
            Author:Osman, Suzanne L._Title:Predicting College Women's Body-Esteem and Self-Esteem Based on Rape_   Experience, Recency, and Labeling.txt
            Author:Osman, Suzanne L._Title:Predicting College Women's Self-esteem Based on Verbal Coercion_   Experience and Verbal Tactic Items on the Revised Sexual Experiences_   Survey.txt
            Author:Osman, Suzanne L._Title:Sexual victimization by current partner is negatively associated with_   women's sexual satisfaction.txt
            Author:Osman, Suzanne L._Title:Sexual victimization experience, acknowledgment labeling and rape_   empathy among college men and women.txt
            Author:Owens-King, Allessia P._Title:Measuring Undergraduate Social Work Students' Knowledge and_   Understanding of Privilege and Oppression.txt
            Author:Owens-King, Allessia P._Title:Secondary traumatic stress and self-care inextricably linked.txt
            Author:Padgett, Stephen M._Title:_He just teaches whatever he thinks is important_ Analysis of comments_   in student evaluations of teaching.txt
            Author:Pandey, Anjali_Title:Re-Englishing 'flat-world' fiction.txt
            Author:Pandey, Anjali_Title:_Authorized to work in the US_ Examining the myth of porous borders in_   the era of populism for practicing linguists.txt
            Author:Park, Minseok_Title:Does Forecast-Accuracy-Based Allocation Induce Customers to Share_   Truthful Order Forecasts_.txt
            Author:Park, Minseok_Title:Predicting supply chain risks through big data analytics_ role of risk_   alert tool in mitigating business disruption.txt
            Author:Park, Minseok_Title:Responding to epidemic-driven demand_ the role of supply channels.txt
            Author:Pasirayi, Simbarashe_Title:#Activism_ Investor Reactions to Corporate Sociopolitical Activism.txt
            Author:Pasirayi, Simbarashe_Title:Assessing the impact of manufacturer power on private label market share_   in an equilibrium framework.txt
            Author:Pasirayi, Simbarashe_Title:Stock market reactions to store-in-store agreements.txt
            Author:Pasirayi, Simbarashe_Title:The Effect of Mobile Payments on Retailer Firm Value_ The Moderating_   Role of Promotions, Customer Segment, and Rollout Strategy.txt
            Author:Pasirayi, Simbarashe_Title:The effect of subscription-based direct-to-consumer channel additions on_   firm value.txt
            Author:Pasirayi, Simbarashe_Title:The effect of third-party delivery partnerships on firm value.txt
            Author:Patel, Shruti_Title:Networks of Power in the Nineteenth Century_ The Sampradaya, Princely_   States and Company Rule.txt
            Author:Pellinger, Thomas K._Title:Acute Lower Leg Heating Increases Exercise Capacity in Patients With_   Peripheral Artery Disease.txt
            Author:Peng, Yuqi_Title:Airline revenue management around sporting mega-events_ an application_   using data from the Super Bowl XLIX.txt
            Author:Phillips, David S._Title:Effect of Vigorous Physical Activity on Executive Control in_   Middle-School Students.txt
            Author:Phillips, Robert A._Title:Identification of Genes Required for Enzalutamide Resistance in_   Castration-Resistant Prostate Cancer Cells in Vitro.txt
            Author:Poddar, Amit_Title:False advertising or slander_ Using location based tweets to assess_   online rating-reliability.txt
            Author:Poddar, Amit_Title:Run-of-the-Mill or Avant Garde_ Identifying restaurant category_   positioning and tastemakers from digital geo-location history.txt
            Author:Polkinghorn, Brian D._Title:Collaborative Partnering for Airport Construction Projects_   State-of-Practice.txt
            Author:Pope, Alexander_Title:Civic Engagement Among Youth Exposed to Community Violence_ Directions_   for Research and Practice.txt
            Author:Pope, Alexander_Title:Making civic engagement go viral_ Applying social epidemiology_   principles to civic education.txt
            Author:Porter, Heather D._Title:Reframing and Repositioning College Readers' Assumptions About Reading_   Through Eye Movement Miscue Analysis.txt
            Author:Presotto, Andrea_Title:Establishing the relationship between non-human primates and mangrove_   forests at the global, national, and local scales.txt
            Author:Presotto, Andrea_Title:Free-Living Aquatic Turtles as Sentinels of Salmonella spp. for_   Water Bodies.txt
            Author:Presotto, Andrea_Title:Navigating in a challenging semiarid environment_ the use of a_   route-based mental map by a small-bodied neotropical primate.txt
            Author:Presotto, Andrea_Title:Rare Bearded Capuchin (Sapajus libidinosus) Tool-Use Culture is_   Threatened by Land use Changes in Northeastern Brazil.txt
            Author:Presotto, Andrea_Title:Spatial cognition in western gorillas (Gorilla gorilla)_ an_   analysis of distance, linearity, and speed of travel routes.txt
            Author:Presotto, Andrea_Title:Spatial mapping shows that some African elephants use cognitive maps to_   navigate the core but not the periphery of their home ranges.txt
            Author:Presotto, Andrea_Title:Stone tools improve diet quality in wild monkeys.txt
            Author:Presotto, Andrea_Title:The Coexistence of People and Bearded Capuchins (Sapajus libidinosus) in_   a Nonindustrial Ecosystem_ An Assessment of Tourist and Local_   Perceptions in the Coastal Area of MaranhÃ£o, Brazil.txt
            Author:Presotto, Andrea_Title:The role of hunting on Sapajus xanthosternos' landscape of fear_   in the Atlantic Forest, Brazil.txt
            Author:Quan, Jing_Title:IT Application Maturity, Management Institutional Capability and Process_   Management Capability.txt
            Author:Quan, Jing_Title:R&D investment, intellectual capital, organizational learning, and firm_   performance_ a study of Chinese software companies.txt
            Author:Quan, Jing_Title:Risk and Revenue Management in the Chinese Auto Loan Industry.txt
            Author:Quan, Jing_Title:Software Vulnerability and Application Security Risk.txt
            Author:Quan, Jing_Title:What Do Agile, Lean, and ITIL Mean to DevOps_.txt
            Author:Ramanathan, Gurupriya_Title:A Framework for Scientific Inquiry in Preschool.txt
            Author:Ramanathan, Gurupriya_Title:Engineering in Preschool_ What Little Minds Can Teach Us About Big_   Skills.txt
            Author:Ramseyer, Craig A._Title:An Empirical Study of the Relationship between Seasonal Precipitation_   and Thermodynamic Environment in Puerto Rico.txt
            Author:Ramseyer, Craig A._Title:Future precipitation variability during the early rainfall season in the_   El Yunque National Forest.txt
            Author:Ratti, Manav_Title:Intersectionality, Sikhism, and Black feminist legal theory_   Reconceptualizing Sikh precarity and minoritization in the US and India.txt
            Author:Ratti, Manav_Title:Justice, subalternism, and literary justice_ Aravind Adiga'sThe White_   Tiger.txt
            Author:Ratti, Manav_Title:The Icon and the Text_ American Book History and the Construction of the_   World's Largest-Grossing Illustrated Book, Madonna's Sex (1992).txt
            Author:Ratti, Manav_Title:The intersections of postcolonialism, postsecularism, and literary_   studies_ Potentials, limitations, bibliographies.txt
            Author:Ravizza, Dean M._Title:Effectiveness of an oral hygiene promoting program for elementary school_   students using a smartphone endomicroscope in Thailand.txt
            Author:Rexroth, Kayla S._Title:Effects of life stage on eDNA detection of the invasive European green_   crab ( Carcinus maenas ) in estuarine systems.txt
            Author:Richerson, Rob_Title:Dynamic Right-Slanted Fonts Increase the Effectiveness of Promotional_   Retail Advertising.txt
            Author:Richerson, Rob_Title:Evolutionary motives and food behavior modeling in romantic_   relationships.txt
            Author:Rittinger, Eric R._Title:Inspiring Students to Think Theoretically About International Relations_   Through the Game of Diplomacy.txt
            Author:Rocker, Amanda_Title:Acute effects of hyperglycemia on the peripheral nervous system in_   zebrafish (Danio rerio) following nitroreductase-mediated Î²-cell_   ablation.txt
            Author:Roose, Jordan J._Title:The Development of Denitrification and of the Denitrifying Community in_   a Newly-Created Freshwater Wetland.txt
            Author:Sargent, Sheridan_Title:Regenerative potential and limitations in a zebrafish model of_   hyperglycemia-induced nerve degeneration.txt
            Author:Schlehofer, Michele M. M._Title:_Things Will Get Worse Before They Get Better_ LGBTQ plus People's_   Reactions to the 2020 US Presidential Election.txt
            Author:Schlehofer, Michele M._Title:Early Reactions of Parents to Their Trans and Gender Non-Conforming_   Children.txt
            Author:Schlehofer, Michele M._Title:Experiences of Parent-Advocates of Trans and Gender Non-Conforming Youth.txt
            Author:Schlehofer, Michele M._Title:Investing in Black LGBTQ plus liberation as white people_ A call to_   action for community psychology.txt
            Author:Schlehofer, Michele M._Title:LGBTQ.txt
            Author:Schlehofer, Michele M._Title:Public Psychology_ Introduction to the Special Issue.txt
            Author:Schlehofer, Michele M._Title:Then and now_ A 50-year retrospective thematic analysis of Society for_   Community Research and Action presidential addresses.txt
            Author:Schlehofer, Michele M._Title:_If Extended Family Can't Deal horizontal ellipsis _ Disclosing Trans_   and Gender Non-Conforming Children's Identity.txt
            Author:Schneider, Gustavo_Title:The influence of visually dynamic imagery on purchase intentions_ The_   roles of arousal and lay rationalism.txt
            Author:Schneider, Lisa_Title:On the asymptotic behavior of the q-analog of Kostant's partition_   function.txt
            Author:Schneider, Lisa_Title:When is the q-multiplicity of a weight a power of q_.txt
            Author:Schuldt, Michael A._Title:Do industry specialist auditors enhance accounting quality in the EU_   Evidence from the pre-IFRS and mandatory post-IFRS periods.txt
            Author:Scott, Michael S._Title:Developing a State University System Model to Diversify Faculty in the_   Biomedical Sciences.txt
            Author:Scott, Michael_Title:The Role of Education in Increasing Awareness and Reducing Impact of_   Natural Hazards.txt
            Author:Seldomridge, Lisa A._Title:Increasing the Number of Faculty With CNE(R) Certification_ A Statewide_   Initiative.txt
            Author:Seldomridge, Lisa A._Title:Preparing New Clinical Educators_ 10-Year Outcomes of a Hybrid Program.txt
            Author:Seldomridge, Lisa A._Title:The Maryland Graduate Nurse Faculty Scholarship_ Program Evaluation of a_   Nurse Faculty Workforce Initiative.txt
            Author:Seldomridge, Lisa A._Title:The Maryland Nurse Support Program II_ A Program Evaluation of Faculty_   Workforce Initiative.txt
            Author:Sen, Argha_Title:Managing two-sided B2B electronic markets_ Governance mechanisms,_   performance implications, and boundary conditions.txt
            Author:Sen, Argha_Title:The effect of in-store electronic word of mouth on local competitor_   spillovers in the quick service restaurant industry.txt
            Author:Shakur, Asif_Title:Backward-Design Model for an Astronomy Course Using RSpec.txt
            Author:Shakur, Asif_Title:Damped Oscillations with a Smart Cart.txt
            Author:Shakur, Asif_Title:Diffraction Experiments with a Smart Cart.txt
            Author:Shakur, Asif_Title:Flyby Measurement of the Magnetic Field of a Helmholtz Coil with a Smart_   Cart.txt
            Author:Shakur, Asif_Title:Solution to the September, 2022 Challenge Second to one.txt
            Author:Shifler, Ryan M._Title:Conjecture O holds for the odd symplectic Grassmannian.txt
            Author:Shifler, Ryan M._Title:Curve Neighborhoods of Schubert Varieties in the Odd Symplectic_   Grassmannian.txt
            Author:Shifler, Ryan M._Title:Equivariant quantum cohomology of the odd symplectic Grassmannian.txt
            Author:Shifler, Ryan M._Title:Minimum Quantum Degrees for Isotropic Grassmannians in Types B and C.txt
            Author:Shifler, Ryan M._Title:ON FROBENIUS-PERRON DIMENSION.txt
            Author:Shifler, Ryan M._Title:On the spectral properties of the quantum cohomology of odd quadrics.txt
            Author:Silaphone, K._Title:An evaluation of the Chesapeake Bay management strategy to improve water_   quality in small agricultural watersheds.txt
            Author:Silaphone, Keota_Title:Accounting for misclassification of subspecies provides insights about_   habitat use and dynamics of the Florida Grasshopper Sparrow in response_   to fire.txt
            Author:Singh, Nitya P._Title:Impact of strategic and operational risk management practices on firm_   performance_ An empirical investigation.txt
            Author:Singh, Nitya P._Title:Managing environmental uncertainty for improved firm financial_   performance_ the moderating role of supply chain risk management_   practices on managerial decision making.txt
            Author:Singh, Nitya P._Title:Managing the adverse effect of supply chain risk on corporate_   reputation_ The mediating role of corporate social responsibility_   practices.txt
            Author:Singh, Nitya P._Title:Responding to pandemic challenges_ leadership lessons from multinational_   enterprises (MNEs) in India.txt
            Author:Singh, Nitya P._Title:The rise of emerging Indian multinationals_ strategic learning for EMNC_   foreign market entry and internationalization.txt
            Author:Singh, Nitya Prasad_Title:Building supply chain risk resilience Role of big data analytics in_   supply chain disruption mitigation.txt
            Author:Singh, Nitya_Title:Developing Business Risk Resilience through Risk Management_   Infrastructure_ The Moderating Role of Big Data Analytics.txt
            Author:Small, Hannah G._Title:There's No Place Like Home_ Influential Habitat Characteristics of_   Eastern Tiger Salamander (Ambystoma tigrinum tigrinum) Breeding_   Ponds in Maryland and Delaware.txt
            Author:Smith, Kenneth J._Title:Resilience as a coping strategy for reducing auditor turnover intentions.txt
            Author:Smith, Kenneth J._Title:Resilience as a coping strategy for reducing departure intentions of_   accounting students.txt
            Author:Smith, Kenneth J._Title:Resilience, Psychological Distress, and Academic Burnout among_   Accounting Students.txt
            Author:Smith, Kenneth_Title:An examination of online cheating among business students through the_   lens of the Dark Triad and Fraud Diamond.txt
            Author:Sokoloski, Joshua E._Title:_Helicase_ activity promoted through dynamic interactions between a_   ssDNA translocase and a diffusing SSB protein.txt
            Author:Stanfield, Kellie_Title:MOBILE JOURNALISM AS LIFESTYLE JOURNALISM_ Field Theory in the_   integration of mobile in the newsroom and mobile journalist role_   conception.txt
            Author:Steele, Rachel R._Title:Moral Typecasting Explains Evaluations of Undocumented Immigrants.txt
            Author:Steele, Rachel R._Title:Muslim women negotiating their identity in the era of the Muslim ban.txt
            Author:Stoner, Alexander M._Title:MARX, CRITICAL THEORY, AND THE TREADMILL OF PRODUCTION OF VALUE_ WHY_   ENVIRONMENTAL SOCIOLOGY NEEDS A CRITIQUE OF CAPITAL.txt
            Author:Stutelberg, Erin B._Title:Teaching as invasion_ emotions, boundaries and entanglements.txt
            Author:Surak, Sarah_Title:The administrative making of the recycler.txt
            Author:Taylor, Ryan C._Title:Behavioral and neural auditory thresholds in a frog.txt
            Author:Taylor, Ryan C._Title:Complex sensory environments alter mate choice outcomes.txt
            Author:Taylor, Ryan C._Title:Evolutionary and Allometric Insights into Anuran Auditory Sensitivity_   and Morphology.txt
            Author:Teller, Kyle G._Title:Determining the most recent common ancestor in a finite linear habitat_   with asymmetric dispersal.txt
            Author:Townsend, Zachary_Title:Understanding Libraries as Part of the Rural Active Living Environment_   Evidence From a Content Analysis of Library Facebook Posts Made in_   Summer 2022.txt
            Author:Treuth, Margarita S._Title:Physical Activity Levels in Six Native American Communities Using the_   FITT-VP Approach.txt
            Author:Troup, Nicholas W._Title:Close Binary Companions to APOGEE DR16 Stars_ 20,000 Binary-star Systems_   Across the Color-Magnitude Diagram.txt
            Author:Troup, Nicholas W._Title:Geometry of the Draco C1 Symbiotic Binary.txt
            Author:Troup, Nicholas W._Title:Multiplicity Statistics of Stars in the Sagittarius Dwarf Spheroidal_   Galaxy_ Comparison to the Milky Way.txt
            Author:Troup, Nicholas W._Title:Stellar multiplicity and stellar rotation_ insights from APOGEE.txt
            Author:Troup, Nicholas W._Title:The Seventeenth Data Release of the Sloan Digital Sky Surveys_ Complete_   Release of MaNGA, MaStar, and APOGEE-2 Data.txt
            Author:Troup, Nicholas W._Title:The close binary fraction as a function of stellar parameters in APOGEE_   a strong anticorrelation with Î± abundances.txt
            Author:Troup, Nicholas_Title:Final Targeting Strategy for the Sloan Digital Sky Survey IV Apache_   Point Observatory Galactic Evolution Experiment 2 North Survey.txt
            Author:Troup, Nicholas_Title:First results from the Dark Skies, Bright Kids astronomy club_   draw-a-scientist test.txt
            Author:Troup, Nicholas_Title:The Eighteenth Data Release of the Sloan Digital Sky Surveys_ Targeting_   and First Spectra from SDSS-V.txt
            Author:Tu, Junyi_Title:Synchronization of memristive FitzHugh-Nagumo neural networks.txt
            Author:Vennos, A._Title:Dedekind sums arising from newform Eisenstein series.txt
            Author:Villalobos, Laura_Title:Local Effects of Payments for Ecosystem Services on Rural Poverty.txt
            Author:Villalobos, Laura_Title:Quantifying COVID-19_s silver lining_ Avoided deaths from air quality_   improvements in Bogota.txt
            Author:Wang, Shuangquan_Title:LAX-Score_ Quantifying Team Performance in Lacrosse and Exploring IMU_   Features towards Performance Enhancement.txt
            Author:Wathen, Bailee_Title:Buchwald-Hartwig Amination, High-Throughput Experimentation, and Process_   Chemistry_ An Introduction via Undergraduate Laboratory Experimentation.txt
            Author:Webster, Debra_Title:Advocacy, Collaboration, and Conflict Management Teaching Core Skill_   Sets in Mental Health Nursing.txt
            Author:Webster, Debra_Title:Lights, Camera, Action_ Lessons Learned From a Nursing and Theater_   Collaboration.txt
            Author:Weer, Christy H._Title:Managers' Assessments of Employees' Organizational Career Growth_   Opportunities_ The Role of Extra-Role Performance, Work Engagement, and_   Perceived Organizational Commitment.txt
            Author:Wenke, John_Title:Imposture and Subversion Charles Brockden Brown's Memoirs of Carwin_   the Biloquist.txt
            Author:Werner, Timothy J._Title:Effects of a 12-Week Resistance Training Program on Arterial Stiffness_   A Randomized Controlled Trial.txt
            Author:Wesolowski, S._Title:Exploring Bayesian parameter estimation for chiral effective field_   theory using nucleon-nucleon phase shifts.txt
            Author:Wesolowski, S._Title:Quantifying correlated truncation errors in effective field theory.txt
            Author:Wesolowski, S._Title:Rigorous constraints on three-nucleon forces in chiral effective field_   theory from fast and accurate calculations of few-body observables.txt
            Author:Wesolowski, Sarah C._Title:Extreme ultraviolet quasar colours from GALEX observations of the_   SDSS DR14Q catalogue.txt
            Author:Wille, Brendan_Title:Characterization of an L-Ascorbate Catabolic Pathway with Unprecedented_   Enzymatic Transformations.txt
            Author:Willey, Amanda_Title:Exploring Factors That Contribute to Nursing Students' Willingness to_   Report Peer Academic Integrity Violations.txt
            Author:Williamson, Thea_Title:Experiences of Alienation and Intimacy_ The Work of Secondary Writing_   Instruction.txt
            Author:Williamson, Thea_Title:Illustrating linguistic dexterity in _English mostly_ spaces_ how_   translanguaging can support academic writing in secondary ELA classrooms.txt
            Author:Williamson, Thea_Title:More, Faster, Neater_ Middle School Students' Self-Assessed Literacy_   Concerns.txt
            Author:Winter, Dorothea M._Title:A Faith-Based Intervention to Improve HYPERTENSION MANAGEMENT Among_   African Americans.txt
            Author:Wu, Ying_Title:An Analysis of State Capital Share and Its Implications to the_   Efficiency-Equality Nexus.txt
            Author:Wu, Ying_Title:Macroeconomic Impacts of the US External Imbalances with Two Large_   Emerging Asian Economies_ Japan (1970-1990) versus China (2000-2018).txt
            Author:Wu, Ying_Title:Monetary sterilization response to the movements in exchange rates and_   official net foreign assets_ a case of China.txt
            Author:Wu, Ying_Title:The ownership effect on corporate investment distortion in the_   transitional economies_ Mitigating or exacerbating_.txt
            Author:Wu, Yun_Title:Enhancing Hospital Performance_ the role of interfirm dynamic_   capabilities from the information processing view.txt
            Author:Wu, Yun_Title:Organizational mindfulness towards digital transformation as a_   prerequisite of information processing capability to achieve market_   agility.txt
            Author:Wu, Yun_Title:Value co-creation in online healthcare communities.txt
            Author:Wu, Yun_Title:eWOM, what are we suspecting_ Motivation, truthfulness or identity.txt
            Author:Yoon, David J. J._Title:Relationship conflict and counterproductive work behavior_ the roles of_   affective well-being and emotional intelligence.txt
            Author:Yoon, David J._Title:Customer courtesy and service performance_ The roles of self-efficacy_   and social context.txt
            Author:Yoon, David J._Title:Leader Behavioral Integrity and Employee In-Role Performance_ The Roles_   of Coworker Support and Job Autonomy.txt
            Author:Yoon, David J._Title:Rude customers and service performance_ roles of motivation and_   personality.txt
            Author:Yoon, David J._Title:The balance between positive and negative affect in employee well-being.txt
            Author:_Title:A Pilot Study Comparing Postmortem and Antemortem CT for the_   Identification of Unknowns_ Could a Forensic Pathologist Do It_.txt
            Author:_Title:Mattress Coil Spring Fatigue and Weight-Bearing Support_ Comparison of_   Weight-Bearing and Non-Weight-Bearing Springs.txt
        [split_files_2/]
            Author:Agarwal, Vinita_Title:Ayurvedic protocols of chronic pain management_ spatiotemporality as present moment awareness and embodied time.txt
            Author:Agarwal, Vinita_Title:Whole person healthcare as social and environmental justice_ new research directions toward a paradigmatic integration of an ecological whole system medicine framework.txt
            Author:Austin, Jathan_Title:ON PYTHAGOREAN TRIPLE PRESERVING MATRICES THAT CONTAIN FIBONACCI NUMBERS.txt
            Author:Berns, Chelsea M._Title:Bill shape reflects divergent prey consumption for island compared to mainland American kestrels (Falco sparverius).txt
            Author:Chen, Xuan_Title:Food search and transport in red imported fire ants (Hymenoptera_ Formicidae) under wet conditions.txt
            Author:Cox, Jennifer Brannock_Title:The News Sourcing Practices of Solutions Journalists in Africa, Europe, and the U.S..txt
            Author:Eksi, Asli_Title:Nonstandard Errors.txt
            Author:Eksi, Asli_Title:The Importance of Risk Preference Parameters in Prospect Theory_ Evidence from Mutual Fund Flows.txt
            Author:Emerson, Jamie_Title:Interim rank and risk-taking_ Evidence from long jump competitions.txt
            Author:Flores, Brian_Title:Re-Centering Students and Teachers_ Voices from Literacy Clinics.txt
            Author:Freeman, Angela R._Title:Characterization of oxytocin and vasopressin receptors in the Southern giant pouched rat and comparison to other rodents.txt
            Author:Gang, Kwangwook_Title:Incomplete Decisions on Reward-Based Crowdfunding Platforms_ Exploring Motivations from Temporal and Social Perspectives.txt
            Author:Green, Daniel_Title:A Proposed Taxonomy for Categorizing Sexual Identities in Adolescence.txt
            Author:Hahn, Eugene D._Title:Cross-border and domestic early-stage financial investment in 3D printing_ An empirical perspective on drivers and locations.txt
            Author:Holdai, Veera_Title:Bioabsorbable, subcutaneous naltrexone implants mitigate fentanyl-induced respiratory depression at 3 months-A pilot study in male canines.txt
            Author:Irons, Jonathan_Title:Usurpation and Brooding of Least Tern (Sternula antillarum) Chicks by Common Terns (Sterna hirundo).txt
            Author:Jarosinski, Judith M._Title:Lessons learned_ Why study-abroad remains a critical component of nursing curriculums.txt
            Author:Johnson, Aaron_Title:The effect of evidence in nonprofit donation requests_ how does mindset play a role_.txt
            Author:Johnson, David T._Title:Middle of Nowhere (2012)_ Waiting Studies, Cinema, and Temporal Experience.txt
            Author:Jung, Kyoung-Rae_Title:Korean fathers' immigration experience.txt
            Author:Keifer, David Z._Title:A Mass Spectrometry Experiment on the Degrees of Freedom Effect.txt
            Author:Kim, Yun Kyoung_Title:Inefficiencies and bias in first job placement_ the case of professional Asian nationals in the United States.txt
            Author:Koh, Bibiana D._Title:Epistemic ethics justice_ a _radical imaginary_.txt
            Author:Koh, Bibiana D._Title:Insights from the lived experience of Buddhist ethics_ implications for social work ethics education.txt
            Author:Li, Ning_Title:Opioid Overdose Hospitalizations During COVID-19_ The Experience of Pennsylvania.txt
            Author:Marinaro, Laura_Title:How traditional undergraduate college students define and perceive wellness_ A qualitative phenomenological study.txt
            Author:Miao, Chao_Title:Entrepreneurship_ an extension to anti-work perspectives.txt
            Author:Miao, Chao_Title:Fear and work performance_ A meta-analysis and future research directions.txt
            Author:Miller, Katherine R._Title:Short-term and long-term exposure to combined elevated temperature and CO2 leads to differential growth, toxicity, and fatty acid profiles in the harmful dinoflagellate Karlodinium veneficum.txt
            Author:Muller, Keaghan A._Title:Continuous Wavelet Transform Analysis of Climate Variability, Resiliency, and Restoration Strategies in Mesohaline Tidal Creeks.txt
            Author:Nan, Wenxiu (Vince)_Title:To gamble or not to gamble_ The effect of mobile money on gambling in Kenya.txt
            Author:Okubo, Yuki_Title:Making Waves in Academia_ Asian_Asian American Feminist Mentoring as Activism.txt
            Author:Osman, Suzanne L._Title:Incapacitated and_or Forcible Rape Experience Predicting College Women's Rape Victim Empathy.txt
            Author:Perret, Arnaud_Title:Beyond Analogy French and Francophone Studies and the Problem of Denomination.txt
            Author:Presotto, Andrea_Title:The Impact of Land Conversion on Primate Habitats_ Refining the Extent of Occurrence Data for Four Capuchin Species in North and Northeastern Brazil.txt
            Author:Seldomridge, Lisa A._Title:Preparing New Clinical Educators_ 10-Year Outcomes of a Hybrid Program.txt
            Author:Shifler, Ryan M._Title:Positivity determines the quantum cohomology of the odd symplectic Grassmannian of lines.txt
            Author:Spillson, Christine_Title:Teaching Creative Nonfiction in the Literature Classroom_ A Proposed Framework.txt
            Author:Sutton-Ryan, Alison_Title:Soul of the Nation_ Freedom to Exist, 2023.txt
            Author:Taylor, Susannah_Title:The Impact of Physical Activity Enjoyment, Exercise Self-Efficacy, Recording Physical Activity, and Exercise Goal Setting on Physical Activity Levels of College Students.txt
            Author:Townsend, Zachary_Title:Factors Associated With Leisure-Time Bicycling Among Adults in the United States_ An Urban-Rural Comparison.txt
            Author:Tu, Junyi_Title:NEW EFFECTIVE TRANSFORMATIONAL COMPUTATIONAL METHODS.txt
            Author:Villalobos, Laura_Title:Points, cells, or polygons_ On the choice of spatial units in forest conservation policy impact evaluation.txt
            Author:Weber, Erin M._Title:An interdisciplinary assessment of information literacy instruction.txt
            Author:Willey, Jeffrey_Title:Online Repository of Genomics Educational Resources to Enhance Nursing Knowledge.txt
            Author:Wu, Ying_Title:Environmental and social disclosure, managerial entrenchment, and investment efficiency.txt
            Author:Wu, Ying_Title:Risk-averse corporate investment behavior and the effectiveness of quantitative easing.txt
            Author:Wulf, Isabel Quintana_Title:Seeing the unseen_ abjection, social death, and neoliberal implication in Hector Tobar's The Tattooed Soldier.txt
            Author:_Title:Campus Sexual Violence and the Cost of Protecting Institutions_ Carceral Systems and Trans Student Experience.txt
            Author:_Title:Unkown.txt
        [static_output_files/]
            2024_titles.json
            ALL_ABSTRACTS.json
            ALL_TITLES.txt
            CategoryMapping.xlsx
            FINAL_ForJensen.json
            REAL_verification_results_SUMMARY.txt
            SU_Category_Data.xlsx
            SU_Category_Data_Themes.xlsx
            _processed_category_data.json
            abstracts_to_categories.json
            abstracts_to_categories_2.json
            article_stats_dict.pkl
            category_dict.pkl
            faculty_stats_dict.pkl
            file_paths.json
            for_jensen.json
            missing_abstracts.txt
            processed_article_stats_data.json
            processed_category_data.json
            processed_faculty_stats_data.json
            test_processed_article_stats_data.json
            test_processed_category_data.json
            test_processed_faculty_stats_data.json
        test.json
        test_processed_article_stats_data.json
        test_processed_faculty_stats_data.json
        [testing/]
            formatted_response.txt
            professor_dump_to_pd.ipynb
            wos_api_response.json
            wos_parser_testing.ipynb
            wos_records.xls
            wos_records_output.csv
        [testing_data/]
            __init__.py
            abstracts.py
            load_pkls.py
            make_pkls.py
            [method_extraction/]
                method_extraction.pkl
                method_json_output_0.json
                method_json_output_1.json
                method_json_output_2.json
                method_json_output_3.json
            [sentence_analysis/]
                abstract_chain_output_0.json
                abstract_chain_output_1.json
                abstract_chain_output_2.json
                abstract_chain_output_3.json
                sentence_analysis.pkl
            [summary/]
                summary.pkl
                summary_chain_output_0.json
                summary_chain_output_1.json
                summary_chain_output_2.json
                summary_chain_output_3.json
        title-abstract.json
        to_csv.py
        vis2.py
    [tests/]
        __init__.py


CrossRefClassificationOrder.md:
```
# Cross-Reference Classification Order

Query cross for data  
-> Go to Wos_classification.py  
    -> For loop through the objects  
    -> extract abstract  
    -> Categorize the abstract  
    -> Insert the category into the object  
    -> Run rest of chain as normal  

```

README.md:
```
# Academic Metrics

> [!IMPORTANT]  
> ðŸŽ‰ **Now Available on PyPI!**  
> Install with: `pip install academic-metrics`
>
> This is the recommended installation method for most users.
>
> See [**Installation and Setup Steps**](#installation-and-setup-steps)

**What is it?**

This repository (COSC425-DATA) hosts the source code for the **Academic Metrics** package.

**Academic Metrics** is an AI-powered toolkit for collecting and classifying academic research publications.

The system:

- Collects publication data from Crossref API based on institutional affiliation
- Uses LLMs to classify research into NSF PhD research focus areas
- Extracts and analyzes themes and methodologies from abstracts
- Generates comprehensive analytics at article, author, and category levels
- Stores results in MongoDB (local or live via atlas), local JSON files, and optionally Excel files

## Table of Contents

- [Academic Metrics](#academic-metrics)
  - [Table of Contents](#table-of-contents)
  - [Features](#features)
  - [Documentation](#documentation)
  - [Example Site and Demo](#example-site-and-demo)
  - [Installation and Setup Steps](#installation-and-setup-steps)
    - [0. External Setup](#0-external-setup)
    - [1. Installation](#1-installation)
    - [2. Creating the directory and necessary files](#2-creating-the-directory-and-necessary-files)
    - [3. Virtual Environment (Optional but Recommended)](#3-virtual-environment-optional-but-recommended)
    - [4. Environment Variables](#4-environment-variables)
    - [5. Setting required environment variables](#5-setting-required-environment-variables)
      - [1. Open the `.env` file you just created, and add the following variables](#1-open-the-env-file-you-just-created-and-add-the-following-variables)
      - [2. Retrieve and set your MongoDB URI](#2-retrieve-and-set-your-mongodb-uri)
      - [3. Set your database name](#3-set-your-database-name)
      - [4. Set your OpenAI API Key](#4-set-your-openai-api-key)
    - [6. Using the package](#6-using-the-package)
      - [Option 1 (Short Script)](#option-1-short-script)
        - [1. Create the python file](#1-create-the-python-file)
        - [2. Copy paste the following code into the file you just created](#2-copy-paste-the-following-code-into-the-file-you-just-created)
        - [3. Run the script](#3-run-the-script)
      - [Option 2 (Command Line Interface)](#option-2-command-line-interface)
        - [1. Create the python file](#1-create-the-python-file-1)
        - [2. Copy and paste the following code into the file you just created](#2-copy-and-paste-the-following-code-into-the-file-you-just-created)
        - [3. Run the script](#3-run-the-script-1)
        - [Examples](#examples)
  - [Wrapping Up](#wrapping-up)

## Features

- **Data Collection**: Automated fetching of publications via Crossref API
- **AI Classification**: LLM-powered analysis of research abstracts
- **Multi-level Analytics**:
  - Article-level metrics and classifications
  - Author/faculty publication statistics
  - Category-level aggregated data
- **Flexible Storage**: MongoDB integration, local JSON output, and optionally Excel files
- **Configurable Pipeline**: Customizable date ranges, models, and processing options
- **And more!**: There are many useful tools within the academic metrics package that can be used for much more than just classification of academic research data, and they're all quite intuitive to use. See [Other Uses](./additional_information/OtherUses.md) for more information.

## Documentation

To be able to see any and all implementation details regarding code logic, structure, prompts, and more you can check out our documentation. The documentation is built with [*Sphinx*](https://github.com/sphinx-doc/sphinx), allowing for easy use and a sense of famliarity.

[**Academic Metrics Documentation**](https://cosc425-data.readthedocs.io/en/latest/)

## Example Site and Demo

We also built an example site with the data we collected so that you can get a small idea of the potential uses for the data. This is by no means the only use case, but it does serve as a nice introduction to decide if this package would be useful for you.

> [!NOTE]
> The source code for the example site is available [here](https://github.com/cbarbes1/AITaxonomy-Front)

[**Example Site**](https://ai-taxonomy-front.vercel.app/)

> [!TIP]
> You can use our site source code for your own site!
> To easily launch your own website using the data you collect and classify via *Academic Metrics* see [**Site Creation Guide**](./additional_information/SiteCreationGuide.md)

To see a demo of the site, you can watch the below video:

[![Demo Video](https://img.youtube.com/vi/LojIwEvFgrk/maxresdefault.jpg)](https://youtu.be/LojIwEvFgrk)

---

## Installation and Setup Steps

Hey all, Spencer here, we are pleased to announce as of January 1st, 2025, you can now install the *Academic Metrics* package via *pip* and easily run the entire system via a short script or command line interface. Below are instructions outlining step by step how to do it. The steps walkthrough each piece of the process starting with installing python and setting up your environment, if you do not need help with those type of steps or want to jump straight to the code, first see [1. Installation](#1-installation), then you can skip to [6. Using the package](#6-using-the-package).

</br>

### 0. External Setup

1. **Installing and setting up Python 3.12:**

    While you should be able to use any version of Python >= 3.7, we recommend using Python 3.12 as that is the version we used to develop the system, and the one it's been tested on.

    For a detailed Python installation guide, see our [Python Installation Guide](./additional_information/_guides/_python_install.md).

2. **Installing and setting up MongoDB:**

    For a detailed MongoDB installation and setup guide, see our [MongoDB Installation Guide](./additional_information/_guides/_mongodb_install.md).

    Once you have MongoDB installed and running, you can create a database to store your data in, if you haven't already.

    To create a new database, you can run:

    ```bash
    use <db_name>
    ```

    If you need more help, the MongoDB Installation Guide goes into more detail on how to create a database and verify it exists.

    Collection creation is handled by the system, you do not need to create them.

    </br>

    ---

### 1. Installation

Install `academic_metrics>=1.0.98` via pip.

To install the latest version of the package, you can run the following command:

```bash
pip install academic-metrics
```

### 2. Creating the directory and necessary files

1. **Create the directory and navigate into it:**

   For this example we will be using `am_data_collection` as the name of the directory, but you can name it whatever you want.

    **All systems (seperate commands):**

    ```bash
    mkdir am_data_collection
    cd am_data_collection
    ```

    Or as a single line:

    **Linux / Mac / Windows Command Prompt**:

    ```bash
    mkdir am_data_collection && cd am_data_collection
    ```

    **Windows Powershell**:

    ```powershell
    mkdir am_data_collection; cd am_data_collection
    ```

</br>

### 3. Virtual Environment (Optional but Recommended)

Now that you've created and entered your project directory, you can set up a virtual environment.

For detailed instructions on setting up and using virtual environments, see our [Python Installation Guide - Virtual Environments Section](./additional_information/_guides/_python_install.md#setting-up-virtual-environments).

After setting up your virtual environment, return here to continue with the next steps.

</br>

### 4. Environment Variables

**Create a `.env` file inside the directory you just created.**

**Linux/Mac**:

```bash
touch .env
```

**Windows** (Command Prompt):

```cmd
type nul > .env
```

**Windows** (PowerShell):

```powershell
New-Item -Path .env -Type File
```

You should now have a `.env` file in your directory.

</br>

### 5. Setting required environment variables

</br>

#### 1. Open the `.env` file you just created, and add the following variables

- a variable to store your MongoDB URI, I recommend `MONGODB_URI`
- a variable to store your database name, I recommend `DB_NAME`
- a variable to store your OpenAI API Key, I recommend `OPENAI_API_KEY`

 After each variable you should add `=""` to the end of the variable.

 Once you've done this, your `.env` file should look something like this:

```python
MONGODB_URI=""
DB_NAME=""
OPENAI_API_KEY=""
```

</br>

#### 2. Retrieve and set your MongoDB URI

For local MongoDB it's typically:

```python
MONGODB_URI="mongodb://localhost:27017"
```

For live MongoDB:

For a live version you should use the MongoDB Atlas URI. It should look something like this:

```bash
mongodb+srv://<username>:<password>@<cluster-name>.<unique-id>.mongodb.net/?retryWrites=true&w=majority&appName=<YourAppNameOnAtlas>
```

So in the `.env` file you should have something that looks like this:

Local:

```python
MONGODB_URI="mongodb://localhost:27017"
```

Live:

```python
MONGODB_URI="mongodb+srv://<username>:<password>@<cluster-name>.<unique-id>.mongodb.net/?retryWrites=true&w=majority&appName=<YourAppNameOnAtlas>"
```

</br>

> [!WARNING]
> I recommend starting locally unless you need to use a live MongoDB instance.
> This will avoid the need to deal with setting up MongoDB Atlas, which while not difficult, it is an added step.

</br>

#### 3. Set your database name

You can pick any name you want for `DB_NAME`, but it needs to be a name of a valid database on your mongodb server. To make one on the command line you can run:

```bash
mongosh
use <db_name>
```

For this demonstration we will be using `academic_metrics_data` as the `DB_NAME`.

First we'll create the database on the command line:

```bash
mongosh
use academic_metrics_data
```

This is to ensure the database actually exists so that the system can access it.

Now that the database exists, we'll set the `DB_NAME` in the `.env` file.

```python
DB_NAME="academic_metrics_data"
```

</br>

#### 4. Set your OpenAI API Key

If you do not have an OpenAI API key you will need to create one, but do not worry, it's easy.

Go to the following link and click on "+ Create new secret key":

[https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)

Give the key a name, and then copy the key.

Then in the `.env` file paste the key in the `OPENAI_API_KEY` variable.

It should look similar to this, but with the full key instead of `sk-proj...`:

```python
OPENAI_API_KEY="sk-proj..."
```

</br>

> [!IMPORTANT]
> You will need to add funds to your OpenAI account to use the API.
>
> When using the default model for the system (gpt-4o-mini), it cost us about $3-4 dollars to process all of the data from Salisbury University from 2009-2024.
>
> For larger models such as gpt-4o, the cost will be much higher.
>
> We saw good results using gpt-4o-mini, and it's also the most cost effective. So I recommend starting with that.
>
> Additionally, whether you opt to use our command line interface or your own script, the data is processed one month at a time and saved to the database, so if you run out of funds on your OpenAI account you will not lose data for the entire run, only the current month being processed. Simply add funds to your account and continue.
>
> You do not have to change anything in the code once you run it again, the system checks for existing data and only processes data that has not yet been processed.

</br>

All together your `.env` file should look like this:

```python
MONGODB_URI="mongodb://localhost:27017"
DB_NAME="academic_metrics_data"
OPENAI_API_KEY="sk-proj..."
```

</br>

### 6. Using the package

To use the system, you have 2 options:

1. Writing a short script (code provided) to loop over a range of dates you'd like to collect.

2. Using a provided function to run a command line interface version.

For most users, I recommend the second option, it's only a few lines of code which you can copy and paste, the rest of the usage is handled by the command line interface and doesn't require any additional coding, you can find the second option in the [Option 2 (Command Line Interface)](#option-2-command-line-interface) section.

On the other hand, if you plan on using the main system, or other tools within the package within your own scripts, or just don't enjoy using command line interfaces, I recommend the first option.

While I recommend the second option unless you're planning on using the package's offerings in a more complex manner, the basic code to run the system for the first option is provided in full in [Option 1 (Short Script)](#option-1-short-script) section.

To see some examples of more complex use cases with examples, you can check out the [Other Uses](./additional_information/OtherUses.md) section.

</br>

#### Option 1 (Short Script)

For this option you need to do the following:
</br>
</br>

##### 1. Create the python file

Within your directory, create a new python file, for this example we will be using `run_am.py`, but you can name it whatever you want.

**Linux/Mac**:

```bash
touch run_am.py
```

**Windows (Command Prompt):**

```cmd
type nul > run_am.py
```

**Windows (PowerShell):**

```powershell
New-Item -Path run_am.py -Type File
```

You should now have a python file in your directory whose name matches the one you created.

</br>

##### 2. Copy paste the following code into the file you just created

```python

# dotenv is the python package responsible for handling env files
from dotenv import load_dotenv

# os is used to get the environment variables from the .env file
import os

# PipelineRunner is the main class used to run the pipeline
from academic_metrics.runners import PipelineRunner

# load_dotenv is used to load the environment variables from the .env file
load_dotenv()

# Get the environment variables from the .env file
ai_api_key = os.getenv("OPENAI_API_KEY")
mongodb_uri = os.getenv("MONGODB_URI")
db_name = os.getenv("DB_NAME")

# Set the date range you want to process
# Years is a list of years as strings you want to process
# Months is a list of strings representing the months you want processed for each year
# For example if you want to process data from 2009-2024 for all months out of the year, you would do:
# Note: the process runs left to right, so from beginning of list to the end of the list,
# so this will process 2024, then 2023, then 2022, etc.
# Data will be saved after each month is processed.
years = [
    "2024",
    "2023",
    "2022",
    "2021",
    "2020",
    "2019",
    "2018",
    "2017",
    "2016",
    "2015",
    "2014",
    "2013",
    "2012",
    "2011",
    "2010",
    "2009",
]
months = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"]

# Loop over the years and months and run the pipeline for each month
# New objects are created for each month to avoid memory issues as well as to avoid overwriting data
for year in years:
    for month in months:

        # Create a new PipelineRunner object for each month
        # parameters:
        # ai_api_key: the OpenAI API key
        # crossref_affiliation: the affiliation to use for the Crossref API
        # data_from_month: the month to start collecting data from
        # data_to_month: the month to end collecting data on
        # data_from_year: the year to start collecting data from
        # data_to_year: the year to end collecting data on
        # mongodb_uri: the URL of the MongoDB server
        # db_name: the name of the database to use
        pipeline_runner = PipelineRunner(
            ai_api_key=ai_api_key,
            crossref_affiliation="Salisbury University",
            data_from_month=int(month),
            data_to_month=int(month),
            data_from_year=int(year),
            data_to_year=int(year),
            mongodb_uri=mongodb_uri,
            db_name=db_name,
        ) 

        # Run the pipeline for the current month
        pipeline_runner.run_pipeline()
```

If you'd like to save the data to excel files in addition to the other data formats, you can do so via importing the function `get_excel_report` from `academic_metrics.runners` and calling it at the end of the script.

Full code for convenience:

```python

# dotenv is the python package responsible for handling env files
from dotenv import load_dotenv

# os is used to get the environment variables from the .env file
import os

# PipelineRunner is the main class used to run the pipeline
# get_excel_report is the function used to save the data to excel files
# it takes in a DatabaseWrapper object as a parameter, which connects to the database
# and retrives the data before writing it to 3 seperate excel files. One for each data type.
from academic_metrics.runners import PipelineRunner, get_excel_report

# DatabaseWrapper is the class used to connect to the database and retrieve the data
from academic_metrics.DB import DatabaseWrapper

# load_dotenv is used to load the environment variables from the .env file
load_dotenv()

# Get the environment variables from the .env file
# If you used the same names as the ones in the examples, you can just copy paste these
# if you used different names, you will need to change them to match the ones in your .env file
ai_api_key = os.getenv("OPENAI_API_KEY")
mongodb_uri = os.getenv("MONGODB_URI")
db_name = os.getenv("DB_NAME")

# Set the date range you want to process
# Years is a list of years as strings you want to process
# Months is a list of strings representing the months you want processed for each year
# For example if you want to process data from 2009-2024 for all months out of the year, you would do:
# Note: the process runs left to right, so from beginning of list to the end of the list,
# so this will process 2024, then 2023, then 2022, etc.
# Data will be saved after each month is processed.
years = [
    "2024",
    "2023",
    "2022",
    "2021",
    "2020",
    "2019",
    "2018",
    "2017",
    "2016",
    "2015",
    "2014",
    "2013",
    "2012",
    "2011",
    "2010",
    "2009",
]
months = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"]

# Loop over the years and months and run the pipeline for each month
#
# New objects are created for each month 
# to avoid memory issues as well as to avoid overwriting data
for year in years:
    for month in months:

        # Create a new PipelineRunner object for each month
        # parameters:
        # ai_api_key: the OpenAI API key
        # crossref_affiliation: the affiliation to use for the Crossref API
        # data_from_month: the month to start collecting data from
        # data_to_month: the month to end collecting data on
        # data_from_year: the year to start collecting data from
        # data_to_year: the year to end collecting data on
        # mongodb_uri: the URL of the MongoDB server
        # db_name: the name of the database to use
        pipeline_runner = PipelineRunner(
            ai_api_key=ai_api_key,
            crossref_affiliation="Salisbury University",
            data_from_month=int(month),
            data_to_month=int(month),
            data_from_year=int(year),
            data_to_year=int(year),
            mongodb_uri=mongodb_uri,
            db_name=db_name,
        ) 

        # Run the pipeline for the current month
        pipeline_runner.run_pipeline()

# Create a new DatabaseWrapper object so it can be given to get_excel_report
db = DatabaseWrapper(db_name=db_name, mongo_uri=mongodb_uri)

# Call the get_excel_report function, passing in the db object, to save the data to excel files
#
# Once this finishes running, you should have 3 excel files in your directory:
# article_data.xlsx, faculty_data.xlsx, and category_data.xlsx
get_excel_report(db)
```

</br>

##### 3. Run the script

```bash
python run_am.py
```

</br>

#### Option 2 (Command Line Interface)

For this options you will still need to create a python file, but the code will only be a couple lines long as you'll be passing in your arguments via the command line.

</br>

##### 1. Create the python file

Within your directory, create a new python file, for this example we will be using `run_am.py`, but you can name it whatever you want.

**Linux/Mac**:

```bash
touch run_am.py
```

**Windows (Command Prompt):**

```cmd
type nul > run_am.py
```

**Windows (PowerShell):**

```powershell
New-Item -Path run_am.py -Type File
```

You should now have a python file in your directory whose name matches the one you created.

</br>

##### 2. Copy and paste the following code into the file you just created

```python
from dotenv import load_dotenv
from academic_metrics.runners import command_line_runner

load_dotenv()

command_line_runner()
```

> [!WARNING]
> If you did not use `MONGODB_URI` and `OPENAI_API_KEY` as the variable names in the .env file, you will need to make a couple changes to the above code.

**How to use with different variable names:**

The `command_line_runner` function takes in 2 optional arguments:

- `openai_api_key_env_var_name`
- `mongodb_uri_env_var_name`

Which correspond to the names of the environment variables you used in your .env file.

To use the different names, do the following:

```python
from dotenv import load_dotenv
from academic_metrics.runners import command_line_runner

load_dotenv()

# The strings should be changes to match the names you used in your .env file
command_line_runner(
    openai_api_key_env_var_name="YOUR_OPENAI_API_KEY_ENV_VAR_NAME",
    mongodb_uri_env_var_name="YOUR_MONGODB_URI_ENV_VAR_NAME",
)
```

</br>

##### 3. Run the script

For this option you will still run the script from command line, but you will also be passing in arguments, details laid out below.

There are various command line arguments you can pass in, almost all are detailed here, but to see a complete list you can run:

```bash
python run_am.py --help
```

When running the script, you can configure the pipeline by passing in the following arguments:

- `--from-month` - The month to start collecting data from, defaults to 1
- `--to-month` - The month to end collecting data on, defaults to 12
- `--from-year` - The year to start collecting data from, defaults to 2024
- `--to-year` - The year to end collecting data on, defaults to 2024
- `--db-name` - The name of the database to use (required)
- `--crossref-affiliation` - The affiliation to use for the Crossref API, defaults to Salisbury University (required)

If you want to save the data to excel files you can pass in the `--as-excel` argument.

>[!NOTE]
> The `--as-excel` argument is an additional action, it doesn't remove the the saving to other data formats, but merely adds the excel saving functionality.

</br>

##### Examples

Say you want to collect data for every month from 2019 to 2024 for Salisbury University and save it to excel files. You would run the following command:

```bash
python run_am.py --from-month=1 \
--to-month=12 \
--from-year=2019 \
--to-year=2024 \
--crossref-affiliation="Salisbury University" \
--as-excel \
--db-name="Your_Database_Name"
```

To make this simpler, we can actually take advantage of the default values for some of the arguments.

Recall from before:

- `--from-month` defaults to `1`
- `--to-month` defaults to `12`
- `--from-year` defaults to `2024`
- `--to-year` defaults to `2024`
- `--crossref-affiliation` defaults to `Salisbury University`

Using the defaults, we can make that command much more concise:

```bash
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name"
```

</br>

**On AI Models**:

The default AI (LLM) model used for all phases is `gpt-4o-mini`. You can specify a different model for each phase independently by passing in the following arguments:

- `--pre-classification-model` - The model to use for the pre-classification step
- `--classification-model` - The model to use for the classification step
- `--theme-model` - The model to use for the theme extraction step

Here's how you would run the pipeline using the larger `gpt-4o` model:

```bash
python run_am.py --from-month=1 \
--to-month=12 \
--from-year=2019 \
--to-year=2024 \
--crossref-affiliation="Salisbury University" \
--as-excel \
--db-name="Your_Database_Name" \
--pre-classification-model="gpt-4o" \
--classification-model="gpt-4o" \
--theme-model="gpt-4o"
```

and taking advantage of the defaults:

```bash
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name" \
--pre-classification-model="gpt-4o" \
--classification-model="gpt-4o" \
--theme-model="gpt-4o"
```

>[!WARNING]
> This process consumes a lot of tokens, and OpenAI API service usage is based off the number of input/output tokens used, with each model having different cost per input/output token.
>
> You can check the cost of each model at [https://openai.com/api/pricing/](https://openai.com/api/pricing/).
>
> During testing we found that using `gpt-4o-mini` was the most cost effective.
>
> In addition we spent a lot of time testing prompts and models, our prompts have been tuned to a point where they elicit good results from `gpt-4o-mini`, thus a larger model may not be necessary to get the results you want.
>
> If you want to use a larger model like `gpt-4o` you can do so, but be warned it could end up running through your funds very quickly, depending on the size of the date range you're processing, and how many articles Crossref covers for the institution being processed.
>
> If you are interested in using a larger model, I recommend you first start with a smaller model on a limited date range to see if you're satisfied with the results.
>
> If you then decide to use a larger model such as `gpt-4o`, whether it be out of curiosity or you want to see if it provides better results, I still recommend you start with a smaller date range to get an idea of what it will cost. If you find the cost to be acceptable, then you can start expanding the date range.

</br>

**Other institutions**:

Our system uses the Crossref API to collect available data, then it scrapes the DOI link to get any missing data as well as any additional data that may be available.

We found that the Crossref API sometimes misses some Abstracts for example, our scraping process will fill in nearly all, if not all, of the missing abstracts.

Due to this, and the wealth of institutions Crossref covers, you can use the system for any institution that has a DOI link.

Here's how you'd run the same query on the system but for **University of Maryland** data:

```bash
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

You can even go back as far as you want, for example say you want to collect all data from the beginning of the 21st century:

```bash
python run_am.py \
--from-year=2000 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

Or maybe you want to collect all data as far back as possible, so you can see longterm trends and history of the institution:

```bash
python run_am.py \
--from-year=1900 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

The from year does not require that there be data that far back, it simply means that is the cutoff point for the data you want to collect.

So say you're not entirely sure what year your University started, or aren't sure how far back Crossref covers, you can simply enter a very far back year, like 1900, and the system will collect all data from that year and onwards.

---

## Wrapping Up

That's it! You've now successfully installed and run the system.

If you have any questions, need help, or have interest in collaborating on this project or others, feel free to reach out to me, contact information is provided below.

If you are a potential employer, please reach out to me by email or linkedin, contact information is provided below.

Contact information:

- Email: [spencerpresley96@gmail.com](mailto:spencerpresley96@gmail.com)
- LinkedIn: [https://www.linkedin.com/in/spencerpresley96/](https://www.linkedin.com/in/spencerpresley96/)

Happy coding!

```

__init__.py:
```

```

    additional_information/OtherUses.md:
    ```
# In progress

Will be added soon

    ```

    additional_information/SiteCreationGuide.md:
    ```
# In progress

Will be added soon

    ```

        additional_information/_guides/_mongodb_install.md:
        ```
# MongoDB Installation Guide

## Introduction

MongoDB is a popular NoSQL database that stores data in flexible, [_JSON-like_](https://www.json.org/json-en.html) [_documents_](https://www.mongodb.com/docs/manual/core/document/). This guide will walk you through:

- Installing MongoDB locally on your machine
- Setting up MongoDB Atlas (cloud version)
- Basic operations and commands
- Troubleshooting common issues

## Table of Contents

- [MongoDB Installation Guide](#mongodb-installation-guide)
  - [Introduction](#introduction)
  - [Table of Contents](#table-of-contents)
  - [Local Installation](#local-installation)
    - [Windows Installation](#windows-installation)
      - [1. Download MongoDB Community Server](#1-download-mongodb-community-server)
      - [2. Run the Installer](#2-run-the-installer)
      - [3. Start MongoDB](#3-start-mongodb)
      - [4. Verify Installation and Connection](#4-verify-installation-and-connection)
    - [macOS Installation](#macos-installation)
      - [1. Using Homebrew (Recommended)](#1-using-homebrew-recommended)
      - [2. Verify Installation and Connection](#2-verify-installation-and-connection)
    - [Linux Installation](#linux-installation)
      - [1. Ubuntu/Debian](#1-ubuntudebian)
      - [2. Verify Installation and Connection](#2-verify-installation-and-connection-1)
  - [MongoDB Atlas Setup](#mongodb-atlas-setup)
    - [Creating an Account](#creating-an-account)
    - [Setting up a Cluster](#setting-up-a-cluster)
    - [Getting Connection String](#getting-connection-string)
  - [Basic Operations](#basic-operations)
    - [Starting MongoDB](#starting-mongodb)
    - [Using mongosh](#using-mongosh)
    - [Creating Databases](#creating-databases)
    - [Basic Commands](#basic-commands)
  - [Troubleshooting](#troubleshooting)
    - [Common Issues](#common-issues)
    - [Security Considerations](#security-considerations)
  - [Additional Resources](#additional-resources)

## Local Installation

### Windows Installation

#### 1. Download MongoDB Community Server

- Visit [MongoDB Download Center](https://www.mongodb.com/try/download/community)
- Select "Windows" as your platform
- Choose "msi" as the package
- Click "Download"

#### 2. Run the Installer

- Double-click the downloaded .msi file
- Choose "Complete" installation
- âœ… Check "Install MongoDB as a Service"
- Click "Install"

#### 3. Start MongoDB

```bash
# Check if MongoDB is running
net start MongoDB
```

- If you see "The MongoDB Server service is starting.", wait a moment.

- If you see "The MongoDB Server service was started successfully.", you're ready to go.

- If you see "The MongoDB Server service is not started.", run:

  ```bash
  net start MongoDB
  ```

#### 4. Verify Installation and Connection

```bash
# Try to connect to MongoDB
mongosh
```

You should see something like:

```bash
Current Mongosh Log ID: ...
Connecting to: mongodb://127.0.0.1:27017...
Using MongoDB: x.x.x
```

### macOS Installation

#### 1. Using Homebrew (Recommended)

```bash
# Check if Homebrew is installed
brew --version

# Install Homebrew if not already installed
#
#
# Bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
#
#
# Zsh
/bin/zsh -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# If Homebrew is already installed, check if MongoDB is installed
brew list | grep mongodb-community

# If MongoDB is already installed, check if it is running
brew services list | grep mongodb-community

# If MongoDB is not installed, install it
brew tap mongodb/brew
brew install mongodb-community

# If you just installed MongoDB, or it wasn't running, start it
brew services start mongodb-community
```

#### 2. Verify Installation and Connection

```bash
# Try to connect to MongoDB
mongosh
```

### Linux Installation

#### 1. Ubuntu/Debian

```bash
# Check if MongoDB is installed
sudo apt-get install -y mongodb-org

# If MongoDB is already installed, check if it is running
sudo systemctl status mongod

# If MongoDB installed but not running, start it
sudo systemctl start mongod

# If MongoDB is not installed, install it
#
#
# Import MongoDB public key
curl -fsSL https://pgp.mongodb.com/server-7.0.asc | \
sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg \
--dearmor

# Create list file
echo "deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse" | \
sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list

# Update package list
sudo apt-get update

# Install MongoDB
sudo apt-get install -y mongodb-org

# Start MongoDB
sudo systemctl start mongod
```

#### 2. Verify Installation and Connection

```bash
# Try to connect to MongoDB
mongosh
```

## MongoDB Atlas Setup

### Creating an Account

1. Visit [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/register)

2. Sign up for a free account

3. Choose the free tier option ("Shared" cluster)

### Setting up a Cluster

1. Choose your preferred cloud provider (AWS, Google Cloud, or Azure)

2. Select a region closest to you

3. Choose "M0 Sandbox" (free tier)

4. Click "Create Cluster"

### Getting Connection String

1. Click "Connect" on your cluster

2. Choose "Connect your application"

3. Select your driver and version

4. Copy the connection string

5. Replace `<password>` with your database user password

## Basic Operations

### Starting MongoDB

If for whatever reason you still need to start MongoDB, this section will provide you with the commands to do so.

**Windows**:

```bash
# MongoDB should start automatically as a service
# To start manually:
net start MongoDB
```

**macOS**:

```bash
brew services start mongodb-community
```

**Linux**:

Most common services to try:

```bash
sudo systemctl start mongod
```

```bash
sudo systemctl start mongodb
```

You can verify the correct service name on your system with:

```bash
systemctl list-units | grep mongo
```

If that gives you an `invalid permission` error, you can try:

```bash
sudo systemctl list-units | grep mongo
```

### Using mongosh

Connect to MongoDB:

```bash
mongosh
```

### Creating Databases

```bash
# List existing databases
show dbs

# Create/switch to a database
use your_database_name

# The database won't appear in show dbs until you add data.
#
# The system will automatically add data to the database, but if you'd like to verify it was created 
# you can add data to it, then check it by running `show dbs` again.
db.createCollection("your_collection_name")
```

### Basic Commands

```bash
# Show current database
db

# Show collections
show collections

# Insert a document
db.your_collection_name.insertOne({ name: "test" })

# Find documents
db.your_collection_name.find()
```

## Troubleshooting

### Common Issues

1. **Connection Refused**

   - Check if MongoDB is running:

     ```bash
     # Windows
     net start MongoDB
     
     # macOS
     brew services list
     
     # Linux
     sudo systemctl status mongod
     ```

2. **Permission Issues**

   - Ensure proper directory permissions:

     ```bash
     # Linux/macOS
     sudo chown -R `id -un` /data/db
     ```

3. **Port Already in Use**

   - Check if another process is using port 27017:

     ```bash
     # Windows
     netstat -ano | findstr 27017
     
     # Linux/macOS
     lsof -i :27017
     ```

### Security Considerations

1. **Enable Authentication (Optional)**

    If for whatever reason you need added security for your MongoDB instance, you can conduct the following to add authentication to your database:

    ```bash
    use admin
    db.createUser({
        user: "adminUser",
        pwd: "securePassword",
        roles: ["userAdminAnyDatabase"]
    })
    ```

2. **Firewall Settings**

   - Allow port 27017 for MongoDB
   - Restrict access to trusted IP addresses

## Additional Resources

- [MongoDB Documentation](https://www.mongodb.com/docs/)
- [MongoDB University](https://university.mongodb.com/) (free courses)
- [MongoDB Community Forums](https://www.mongodb.com/community/forums/)

        ```

        additional_information/_guides/_python_install.md:
        ```
# Python Installation Guide

## Introduction

This guide covers the installation of Python 3.12, a powerful and widely-used programming language. Python is known for its:

- Clear, readable syntax
- Large ecosystem of packages and libraries
- Strong community support
- Cross-platform compatibility

We'll walk through:

1. Installing Python on your operating system
2. Verifying the installation
3. Setting up your development environment
4. Managing packages and dependencies

## Table of Contents

- [Python Installation Guide](#python-installation-guide)
  - [Introduction](#introduction)
  - [Table of Contents](#table-of-contents)
  - [Installation by Operating System](#installation-by-operating-system)
    - [Windows Installation](#windows-installation)
      - [Method 1: Microsoft Store (Simplest)](#method-1-microsoft-store-simplest)
      - [Method 2: Python.org Installer (Recommended)](#method-2-pythonorg-installer-recommended)
      - [Post-Installation (Windows)](#post-installation-windows)
    - [macOS Installation](#macos-installation)
      - [Method 1: Homebrew (Recommended)](#method-1-homebrew-recommended)
      - [Method 2: Python.org Installer](#method-2-pythonorg-installer)
      - [Post-Installation (macOS)](#post-installation-macos)
    - [Linux Installation](#linux-installation)
      - [Ubuntu/Debian](#ubuntudebian)
      - [Fedora](#fedora)
      - [Post-Installation (Linux)](#post-installation-linux)
  - [Understanding Virtual Environments](#understanding-virtual-environments)
  - [Setting Up Virtual Environments](#setting-up-virtual-environments)
    - [Using venv (Built-in)](#using-venv-built-in)
      - [1. Create a new virtual environment](#1-create-a-new-virtual-environment)
      - [1. Create a new virtual environment (Aliased)](#1-create-a-new-virtual-environment-aliased)
      - [2. Activate the environment](#2-activate-the-environment)
    - [Using Conda (Recommended)](#using-conda-recommended)
      - [1. Install Miniconda](#1-install-miniconda)
      - [2. Create a new environment](#2-create-a-new-environment)
      - [3. Activate the environment](#3-activate-the-environment)
  - [Troubleshooting](#troubleshooting)
    - [Common Issues](#common-issues)
      - [1. Python not found in PATH](#1-python-not-found-in-path)
      - [2. Permission Issues](#2-permission-issues)
      - [3. Multiple Python Versions](#3-multiple-python-versions)
  - [Package Management](#package-management)
    - [Using pip (Python Package Installer)](#using-pip-python-package-installer)
  - [Additional Resources](#additional-resources)
  - [Basic Commands](#basic-commands)
    - [Running a Python Script](#running-a-python-script)
  - [Aliasing Python and Pip](#aliasing-python-and-pip)
  - [Getting Help](#getting-help)

</br>

## Installation by Operating System

### Windows Installation

#### Method 1: Microsoft Store (Simplest)

1. Open the Microsoft Store
2. Search for "Python 3.12"
3. Click "Get" or "Install"
4. Open Command Prompt and verify with `python --version`

#### Method 2: Python.org Installer (Recommended)

1. Visit [Python Downloads](https://www.python.org/downloads/)
2. Download Python 3.12 installer
3. Run the installer with these important settings:
   - âœ… Add python.exe to PATH (Required)
   - âœ… Install pip (Should be selected by default)
   - âœ… Install for all users (Recommended)
4. Click "Install Now" for standard installation or "Customize installation" for advanced options

#### Post-Installation (Windows)

1. Open Command Prompt (cmd) or PowerShell
2. Verify installation:

   ```bash
   python --version    # Should show: Python 3.12.x
   pip --version      # Should show: pip X.Y.Z
   ```

</br>

### macOS Installation

#### Method 1: Homebrew (Recommended)

1. Install Homebrew if not present (you can check if it's installed by running `brew --version`):

   ```bash
   /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
   ```

2. Install Python:

   ```bash
   brew install python@3.12
   ```

3. Add to PATH (if not automatically added):

   ```bash
   echo 'export PATH="/usr/local/opt/python@3.12/bin:$PATH"' >> ~/.zshrc
   source ~/.zshrc
   ```

#### Method 2: Python.org Installer

1. Visit [Python Downloads](https://www.python.org/downloads/)
2. Download macOS installer
3. Run the package installer
4. Follow the installation wizard

#### Post-Installation (macOS)

1. Open Terminal
2. Verify installation:

   ```bash
   python3 --version   # Should show: Python 3.12.x
   pip3 --version     # Should show: pip X.Y.Z
   ```

### Linux Installation

#### Ubuntu/Debian

```bash
sudo apt update
sudo apt install software-properties-common
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt install python3.12 python3.12-venv python3-pip
```

#### Fedora

```bash
sudo dnf install python3.12 python3.12-pip python3.12-devel
```

#### Post-Installation (Linux)

```bash
python3.12 --version  # Should show: Python 3.12.x
pip3 --version       # Should show: pip X.Y.Z
```

</br>

## Understanding Virtual Environments

> [!IMPORTANT]  
> Now that you have Python installed, it's important to understand virtual environments. They are a crucial tool for Python development that:
>
> - Isolate project dependencies to avoid conflicts
> - Make projects reproducible across different machines
> - Allow you to work on multiple projects with different Python versions
> - Prevent system-wide Python installation issues
>
> Using a virtual environment is optional, but is highly recommended, if you'd like to use a virtual environment, the below section will guide you step by step through the process of setting one up. It is quite simple and easy to do, and can be completed in only a couple of minutes.

</br>

## Setting Up Virtual Environments

> [!NOTE]  
> This section of the guide should be followed only after you have created a directory and entered it.
>
> The section that covers that can be found in the [README.md](../../README.md) file's [2. Creating the directory and necessary files](../../README.md#2-creating-the-directory-and-necessary-files) section.
>
> You'll find the link back to this section in the [README.md](../../README.md) file's [3. Setting up a Virtual Environment (Optional but Recommended)](../../README.md#3-setting-up-a-virtual-environment-optional-but-recommended) section.

</br>

### Using venv (Built-in)

> [!NOTE]  
> Though you will still be using pip to install the Academic Metrics package, I still recommend using Conda for your virtual environments.
>
> Conda in addition to being a more powerful package manager that helps alleviate some of the issues that can arise from using pip, it also provides a more user friendly interface for managing different virtual environments; including creation, activation, and deactivation.
>
> If you'd like to use Conda, skip to the [Using Conda Section](#using-conda-recommended)
>
> Otherwise, continue with the following steps to create a new virtual environment using the built-in venv module.

</br>

#### 1. Create a new virtual environment

**Windows**:

```bash
python -m venv <env_name>
```

**macOS/Linux**:

> [!NOTE]  
> Before continuing, I'd like to make you aware of a useful tip outlined in [Aliasing Python and Pip](#aliasing-python-and-pip) section, which covers **aliasing** the `python` and `pip` commands to use the version you'd like to use, it will streamline the below process and only takes a couple of seconds to do.
>
> If you'd like to take advantage of aliasing, follow the above link to that section, complete the steps outlined in the tip, then skip to the [1. Create a new virtual environment (Aliased)](#1-create-a-new-virtual-environment-aliased) section to create a new virtual environment.

If you did not follow the steps to alias the `python` and `pip` commands, you can use the following command to check which version of python `python3` is aliased to:

```bash
which python3
```

If `python3` fails with the error:

```bash
python3: command not found
```

Try:

```bash
which python
```

If neither of `python3` or `python` are aliased to `python3.12` first check if `python3.12` is installed:

```bash
which python3.12
```

If it is not installed please return to the [Installation by Operating System](#installation-by-operating-system) section and follow the instructions for your operating system to install Python 3.12.

If `python3.12` is installed, but isn't aliased to `python3`, then you can instead use this command:

```bash
python3.12 -m venv <env_name>
```

</br>

#### 1. Create a new virtual environment (Aliased)

This section is if you followed [Aliasing Python and Pip](#aliasing-python-and-pip) section linked to in [1. Create a new virtual environment](#1-create-a-new-virtual-environment) section.

If you did not follow those steps, you can skip this section and continue to the [2. Activate the environment](#2-activate-the-environment) section.

For those who did follow those steps, you can use the following command to create a new virtual environment:

```bash
python -m venv <venv_name>
```

</br>

#### 2. Activate the environment

**Windows (Command Prompt)**:

```bash
<env_name>\Scripts\activate
```

**Windows (PowerShell)**:

```bash
<env_name>\Scripts\Activate.ps1
```

**macOS/Linux**:

```bash
source <env_name>/bin/activate
```

</br>

### Using Conda (Recommended)

#### 1. Install Miniconda

> [!NOTE]  
> For all systems, you can use install Miniconda from the [Conda Downloads](https://www.anaconda.com/download/) page, for macOS/Linux there's a slightly more streamlined process using `curl` to download and install Miniconda, details are below.

**Windows**:

- Download and run the installer from [Conda Downloads](https://www.anaconda.com/download/)

</br>

Below are the alternatives for macOS and Linux using `curl`.

**macOS Intel**:

```bash
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh
bash Miniconda3-latest-MacOSX-x86_64.sh
```

**macOS Apple Silicon (M1/M2)**:

```bash
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh
```

**Linux**:

```bash
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
```

#### 2. Create a new environment

```bash
conda create -n <env_name> python=3.12
```

#### 3. Activate the environment

```bash
conda activate <env_name>
```

</br>

## Troubleshooting

### Common Issues

#### 1. Python not found in PATH

- **Windows**: Reinstall with "Add to PATH" checked

- **macOS/Linux**: Add to shell profile:

    **For Zsh**:

    ```bash
    echo 'export PATH="/usr/local/opt/python@3.12/bin:$PATH"' >> ~/.zshrc
    source ~/.zshrc
    ```

    **For Bash**:

    ```bash
    echo 'export PATH="/usr/local/opt/python@3.12/bin:$PATH"' >> ~/.bashrc
    source ~/.bashrc
    ```

#### 2. Permission Issues

- **Windows**: Run installer as Administrator

- **macOS/Linux**: Use `sudo` for system-wide installation

- **Virtual Environments**: Avoid using `sudo`

#### 3. Multiple Python Versions

- Use virtual environments (recommended)

- Specify version explicitly: `python3.12` instead of `python`

- Check active version:
  - `which python` or `where python` (Windows)
  - `which python3` or `where python3` (macOS/Linux)

</br>

For a simpler approach, see [Aliasing Python and Pip](#aliasing-python-and-pip)

## Package Management

### Using pip (Python Package Installer)

```bash
# Install a package
pip install package_name

# Install specific version
pip install package_name==1.2.3

# Install from requirements.txt
pip install -r requirements.txt

# List installed packages
pip list
```

</br>

## Additional Resources

- [Official Python Documentation](https://docs.python.org/3.12/)
- [Python Virtual Environments Guide](https://docs.python.org/3/tutorial/venv.html)
- [pip Documentation](https://pip.pypa.io/en/stable/)

## Basic Commands

### Running a Python Script

A useful tip to know, which will standardize and simplify the process of running scripts, is to alias the `python` command to the version you'd like to use. You can find more information on how to do this in the [Aliasing Python and Pip](#aliasing-python-and-pip) section.

If you've aliased the `python` or `python3` commands to `python3.12`, you can simply use the following command to run a python script:

```bash
python script.py
```

If you haven't aliased the `python` or `python3` commands to `python3.12` for whatever reason, then you can follow the below steps to learn how to run a python script:

- If `python` is the command tied to your python installation (doesn't give the error `python: command not found`), and `python` is aliased to `python3.12` when running the command `which python`, you can use the following command to run a python script:

    ```bash
    python3.12 script.py
    ```

- If `python3` is the command tied to your python installation (doesn't give the error `python3: command not found`), and `python3` is aliased to `python3.12` when running the command `which python3`, you can use the following command to run a python script:

    ```bash
    python3 script.py
    ```

- If none of the above works, and you're sure you have python 3.12 installed (you can check this by running the command `which python3.12`), you can use the following command to run a python script:

    ```bash
    python3.12 script.py
    ```

</br>

## Aliasing Python and Pip

**Aliasing the `python` and `pip` commands**

For **macOS/Linux** If you'd like to be able to use `python` instead of `python3`/`python3.12`, you can add the following to your shell profile:

```bash
alias python=python3.12
alias pip="python3.12 -m pip"
```

To do so run the following command:

**Zsh**:

```bash
echo 'alias python=python3.12' >> ~/.zshrc
echo 'alias pip="python3.12 -m pip"' >> ~/.zshrc
source ~/.zshrc
```

**Bash**:

```bash
echo 'alias python=python3.12' >> ~/.bashrc
echo 'alias pip="python3.12 -m pip"' >> ~/.bashrc
source ~/.bashrc
```

Then when installing packages and running scripts instead of:

**Zsh**:

```bash
pip3.12 install package_name
python3.12 script.py
```

**Bash**:

```bash
pip3 install package_name
python3 script.py
```

You can simply use:

```bash
pip install package_name
python script.py
```

If at a later date you'd like to revert back you can simply remove the alias by running the following command:

**Zsh**:

```bash
unalias python
unalias pip
```

**Bash**:

```bash
unalias python
unalias pip
```

or if you'd like to change which version of python and pip you're using you can simply change the alias to the version you'd like to use. You can change the alias to the version you'd like to use by running the following command:

**Zsh**:

```bash
alias python=python3.x
alias pip="python3.x -m pip"
```

**Bash**:

```bash
alias python=python3.x
alias pip="python3.x -m pip"
```

Where `x` is the version extension you'd like to use, such as `3.12`, `3.11`, `3.9`, etc.

</br>

## Getting Help

If you encounter issues:

1. Check the error message carefully
2. Consult the official documentation
3. Search for similar issues on Stack Overflow
4. Ensure you're using the correct Python version
5. Verify your virtual environment is activated if using one
6. If all else fails, feel free to contact me, you can find my contact information in the [Wrapping Up](../../README.md#wrapping-up) section of the [README.md](../../README.md) file.

        ```

check_deps.py:
```
import os
import ast
from collections import defaultdict
from pathlib import Path


def is_external_import(module_name):
    """Check if an import is from an external library."""
    # List of Python standard library modules
    stdlib_modules = set(sys.stdlib_module_names)
    base_module = module_name.split(".")[0]
    return base_module not in stdlib_modules and not module_name.startswith(
        "academic_metrics"
    )


def analyze_imports(file_path):
    """Analyze imports in a Python file."""
    with open(file_path, "r", encoding="utf-8") as file:
        try:
            tree = ast.parse(file.read())
        except:
            return []

    external_imports = set()

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for name in node.names:
                if is_external_import(name.name):
                    external_imports.add(name.name)
        elif isinstance(node, ast.ImportFrom):
            if node.module and is_external_import(node.module):
                external_imports.add(node.module)

    return external_imports


def find_all_external_imports(directory):
    """Find all external imports in Python files within a directory."""
    imports_by_file = defaultdict(set)

    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".py"):
                file_path = Path(root) / file
                external_imports = analyze_imports(file_path)
                if external_imports:
                    imports_by_file[file_path] = external_imports

    return imports_by_file


if __name__ == "__main__":
    import sys

    # Use the directory path from the code blocks
    directory = "src/academic_metrics"

    # Get all external imports
    all_imports = find_all_external_imports(directory)

    # Print results
    print("\nExternal Libraries Used by File:\n")
    for file_path, imports in all_imports.items():
        print(f"\n{file_path}:")
        for imp in sorted(imports):
            print(f"  - {imp}")

    # Print summary of all unique external libraries
    all_unique_imports = sorted(set().union(*all_imports.values()))
    print("\nAll Unique External Libraries:\n")
    for imp in all_unique_imports:
        print(f"- {imp}")

```

compare_deps.py:
```
import subprocess
from typing import Set, Dict
import tomli
from pathlib import Path


def load_pyproject_dependencies() -> Set[str]:
    """Load dependencies from pyproject.toml."""
    try:
        with open("pyproject.toml", "rb") as f:
            pyproject = tomli.load(f)
            return set(pyproject["project"]["dependencies"])
    except FileNotFoundError:
        print("Error: pyproject.toml not found")
        return set()
    except KeyError:
        print("Error: Could not find dependencies in pyproject.toml")
        return set()


def get_pipdeptree_deps() -> Dict[str, Set[str]]:
    """Get both direct and indirect dependencies from pipdeptree."""
    try:
        result = subprocess.run(
            ["pipdeptree", "-p", "academic_metrics", "--json"],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            print("Error running pipdeptree:", result.stderr)
            return {}, {}

        import json

        tree = json.loads(result.stdout)

        direct_deps = set()
        indirect_deps = set()

        # Process the dependency tree
        for package in tree:
            if package["package"]["key"] == "academic_metrics":
                for dep in package["dependencies"]:
                    direct_deps.add(dep["package"]["key"])
                    # Get indirect dependencies
                    for indirect in dep["dependencies"]:
                        indirect_deps.add(indirect["package"]["key"])

        return {"direct": direct_deps, "indirect": indirect_deps}
    except subprocess.CalledProcessError:
        print("Error: Failed to run pipdeptree. Make sure it's installed.")
        return {}, {}
    except json.JSONDecodeError:
        print("Error: Failed to parse pipdeptree JSON output")
        return {}, {}


def scan_imports(directory: str = "src") -> Set[str]:
    """Scan Python files for import statements."""
    import ast
    import glob

    imports = set()

    def extract_imports(node):
        if isinstance(node, ast.Import):
            for name in node.names:
                imports.add(name.name.split(".")[0])
        elif isinstance(node, ast.ImportFrom):
            if node.module:
                imports.add(node.module.split(".")[0])

    for py_file in glob.glob(f"{directory}/**/*.py", recursive=True):
        try:
            with open(py_file, "r", encoding="utf-8") as f:
                tree = ast.parse(f.read())
                for node in ast.walk(tree):
                    extract_imports(node)
        except Exception as e:
            print(f"Error parsing {py_file}: {e}")

    return imports


def main():
    # Get all dependencies
    pyproject_deps = load_pyproject_dependencies()
    dep_tree = get_pipdeptree_deps()
    actual_imports = scan_imports()

    # Analysis
    unused_in_code = pyproject_deps - actual_imports
    not_in_tree = pyproject_deps - dep_tree["direct"] - dep_tree["indirect"]

    # Output results
    print("=== Dependency Analysis ===\n")

    print("Dependencies in pyproject.toml:", len(pyproject_deps))
    print("Direct dependencies (pipdeptree):", len(dep_tree["direct"]))
    print("Indirect dependencies (pipdeptree):", len(dep_tree["indirect"]))
    print("Actually imported packages:", len(actual_imports))

    print("\n=== Potential Issues ===")

    print("\nDependencies listed in pyproject.toml but not imported in code:")
    for dep in sorted(unused_in_code):
        print(f"- {dep}")

    print("\nDependencies in pyproject.toml but not found in dependency tree:")
    for dep in sorted(not_in_tree):
        print(f"- {dep}")

    print("\nNote:")
    print("1. Some dependencies might be runtime-only")
    print(
        "2. Some might be development dependencies that should be in [tool.poetry.dev-dependencies]"
    )
    print("3. Some might use different import names than their package names")
    print("4. Some might be transitive dependencies that could be removed")


if __name__ == "__main__":
    main()

```

        docs/source/AI.rst:
        ```
AI package
==========

Submodules
----------

AI.abstract\_classifier module
------------------------------

.. automodule:: academic_metrics.AI.abstract_classifier
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.AI
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/ChainBuilder.rst:
        ```
ChainBuilder package
====================

Submodules
----------

ChainBuilder.ChainBuilder module
--------------------------------

.. automodule:: academic_metrics.ChainBuilder.ChainBuilder
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.ChainBuilder
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/DB.rst:
        ```
DB package
==========

Submodules
----------

DB.DatabaseSetup module
-----------------------

.. automodule:: academic_metrics.DB.DatabaseSetup
   :members:
   :undoc-members:
   :show-inheritance:
   :special-members:

Module contents
---------------

.. automodule:: academic_metrics.DB
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/ai_prompts.rst:
        ```
Prompt Templates
=================

This package contains the various prompt templates used for LLM interactions.

Pre-classification prompts
--------------------------

Method Extraction
~~~~~~~~~~~~~~~~~~

Method Extraction System Message
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: python

   METHOD_EXTRACTION_SYSTEM_MESSAGE: str = """
   You are a method extraction AI whose purpose is to identify and extract method keywords from an academic abstract. Your role is to locate the specific methodologies, techniques, or approaches mentioned in the abstract and provide them in the JSON format specified.

   ### Definition of Methods:

   - "Methods" refers to the **specific processes**, **techniques**, **procedures**, or **approaches** used in conducting the research. This includes techniques for data collection, data analysis, algorithms, experimental procedures, or any other specific methodology employed by the researchers. Methods should **not** include general descriptions, conclusions, or research themes.

   ### What You Should Do:

   1. **Ponder** the meaning of methods and what they refer to in the context of a research paper.
   2. **Extract** keywords that refer to the **methods** used in the abstract.
   3. **Present** the results in the required **JSON format** with a list of methods identified.

   ### JSON Output Requirements:

   - **Response Format**: You must return your output as a JSON object.
   - The JSON object must contain:
   - A key `"methods"` whose value is a list of extracted **method keywords**.

   ### JSON Structure you must follow:

   {METHOD_JSON_FORMAT}

   ### Examples:

   **Abstract:**

   â€œDrawing on expectation states theory and expertise utilization literature, we examine the effects of team membersâ€™ actual expertise and social status on the degree of influence they exert over team processes via perceived expertise. We also explore the conditions under which teams rely on perceived expertise versus social status in determining influence relationships in teams. To do so, we present a contingency model in which the salience of expertise and social status depends on the types of intragroup conflicts. **Using multiwave survey data from 50 student project teams with 320 members** at a large national research institute located in South Korea, we found that both actual expertise and social status had direct and indirect effects on member influence through perceived expertise. Furthermore, perceived expertise at the early stage of team projects is driven by social status, whereas perceived expertise at the later stage of a team project is mainly driven by actual expertise. Finally, we found that members who are being perceived as experts are more influential when task conflict is high or when relationship conflict is low. We discuss the implications of these findings for research and practice.â€

   #### Example 1: Correct Extraction for the Abstract Above

   **Output:**

   {METHOD_EXTRACTION_CORRECT_EXAMPLE_JSON}

   **Explanation for Correct Extraction:**

   - **"Multiwave survey data collection"**:
   - **Why this is a method**: This is a method because it refers to how data was gathered from research subjects over multiple time points.

   - **"Contingency modeling"**:
   - **Why this is a method**: This is a method because it describes the analytical process used to explore relationships between variables like expertise and social status.
   
   #### Example 2: Incorrect Extraction for the Abstract Above

   **Incorrect Output:**

   {METHOD_EXTRACTION_INCORRECT_EXAMPLE_JSON}

   **Explanation for Incorrect Extraction:**

   - **"Intragroup conflict"**:
   - This is incorrect because it is a variable or condition examined in the research, not a method.
   - **"Perceived expertise"**:
   - This is incorrect because it is a measured variable, not a method.
   - **"Social status"**:
   - This is incorrect because it is a variable the study investigates, not a methodological process.

   **Important Notes:**

   - **JSON Output Only**:
   - Do not include the markdown JSON code block notation in your response.
   - Simply return the JSON object, do not surround it with ```json and ```.

   - **Properly Escape Special Notations**:
   - If you use any special notation (e.g., LaTeX) within the JSON values, ensure it is properly escaped.
   - Failure to do so may result in a JSON parsing error, which is considered a critical failure.

   - **Compliance is Mandatory**:
   - You must return the output in the specified JSON format.
   - Failure to do so will be considered a failure to complete the task.

   """

Method JSON Format
^^^^^^^^^^^^^^^^^^^

.. code-block:: python

   METHOD_JSON_FORMAT: str = """
   {
      "methods": [
         "<method_keyword_1>",
         "<method_keyword_2>"
      ]
   }
   """

Method Extraction Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Correct Extraction
""""""""""""""""""""

.. code-block:: python

   METHOD_EXTRACTION_CORRECT_EXAMPLE_JSON: str = """
   {
      "methods": [
         "multiwave survey data collection",
         "contingency modeling"
      ]
   }
   """

Incorrect Extraction
""""""""""""""""""""""

.. code-block:: python

   METHOD_EXTRACTION_INCORRECT_EXAMPLE_JSON: str = """    
   {
      "methods": [
         "intragroup conflict",
         "perceived expertise",
         "social status",
         "multiwave survey data collection"
      ]
   }
   """

Sentence Analysis
~~~~~~~~~~~~~~~~~~

Sentence Analysis System Message
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: python

   ABSTRACT_SENTENCE_ANALYSIS_SYSTEM_MESSAGE: str = """
   You are tasked with analyzing an abstract of a research paper. Your task involves the following steps:

   Steps to follow:
   1. **Record each sentence in the abstract**: 
   2. For each sentence do the following steps: 
      - Determine the meaning of the sentence
      - Provide a reasoning for your interpretation
      - Assign a confidence score between 0 and 1 based on how confident you are in your assessment.
   3. After you have done this for each sentence, determine the overall theme of the abstract. This should be a high-level overview of the main idea of the research.
   4. Provide a detailed summary of the abstract. This should be a thorough overview of the research, including the main idea, the methods used, and the results.
         
   Your output should follow this structure:

   {SENTENCE_ANALYSIS_JSON_EXAMPLE}

   IMPORTANT: Be concise but clear in your meanings and reasonings.
   IMPORTANT: Ensure that the confidence score reflects how certain you are about the meaning of the sentence in context.
   IMPORTANT: Do not include the markdown json code block notation in your response. Simply return the JSON object. The markdown json code block notation is: ```json\n<your json here>\n```, do not include the ```json\n``` in your response.
   IMPORTANT: If within the values to the keys in the json, you use any other notation such as **Latex** ensure you properly escape. If you do not then the JSON will not be able to be parsed, which is a **critical failure**.
   IMPORTANT: You must return the output in the specified JSON format. If you do not return the output in the specified JSON format, you have failed.
   """

Sentence Analysis JSON Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: python

   SENTENCE_ANALYSIS_JSON_EXAMPLE: str = """
      {
         "sentence_details": [
         {
            "sentence": "Original sentence 1",
            "meaning": "Meaning of the sentence.",
            "reasoning": "Why this is the meaning of the sentence.",
            "confidence_score": Confidence score (0.0 - 1.0)
         },
         {
            "sentence": "Original sentence 2",
            "meaning": "Meaning of the sentence.",
            "reasoning": "Why this is the meaning of the sentence.",
            "confidence_score": Confidence score (0.0 - 1.0)
         }
         ],
         "overall_theme": "Overall theme of the abstract",
         "summary": "Detailed summary of the abstract"
      }
   """

Abstract Summarization
~~~~~~~~~~~~~~~~~~~~~~~

Abstract Summarization System Message
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: python

   ABSTRACT_SUMMARY_SYSTEM_MESSAGE: str = """
   You are an expert AI researcher tasked with summarizing academic research abstracts. Your task is to analyze the abstract and extract the main ideas and themes. The summary should focus on what the research is doing rather than how it is doing it; do not include specific methods used to conduct the research.

   To assist you, the following data is provided:

   1. **Methodologies:**

      - A previous AI assistant has extracted methodologies from the abstract.
      - For each methodology, it provides:
      - Reasoning for why it identified it as a methodology.
      - The passage(s) from the abstract supporting its identification.
      - A confidence score for its identification.
      - Here is the format of the methodologies assistant's output:
      {METHOD_JSON_FORMAT}

   2. **Abstract Sentence Level Analysis:**

      - Another assistant has analyzed each sentence in the abstract.
      - For each sentence, it provides:
      - The identified meaning.
      - The reasoning for the identified meaning.
      - A confidence score.
      - It also provides:
      - An overall theme of the abstract.
      - A detailed summary of the abstract.
      - Here is the format of the abstract sentence level analysis assistant's output:
      {SENTENCE_ANALYSIS_JSON_EXAMPLE}

   **Outputs from Previous Assistants:**

   - **Methodologies Assistant Output:**
   {method_json_output}

   - **Abstract Sentence Level Analysis Assistant Output:**
   {sentence_analysis_output}

   ### Your Output Should Contain:

   - **summary:** A detailed summary of the abstract that captures the main idea of the research without focusing on the specific methods used.
   - **reasoning:** A detailed explanation for the summary you have provided.
   - **feedback_for_methodologies_assistant:** Specific feedback on any issues that affected your ability to accurately summarize the abstract, and any requests for improvements in their analysis. If you have no feedback, simply provide "The analysis is correct and sufficient, I have no feedback at this time."
   - **feedback_for_abstract_sentence_level_analysis_assistant:** Specific feedback on any issues that affected your ability to accurately summarize the abstract, and any requests for improvements in their analysis. If you have no feedback, simply provide "The analysis is correct and sufficient, I have no feedback at this time."

   ### Steps to Complete Your Task:

   1. Carefully read and understand the methodologies identified by the previous assistant.
   2. Carefully read and understand the sentence-level analysis of the abstract provided by the previous assistant.
   3. Carefully read and understand the abstract as a whole.
   4. Form a detailed summary of the abstract that captures the main idea of the research without focusing on specific methods.

   ### Output Format:

   Your output should be a JSON object with the following structure:

   {SUMMARY_JSON_STRUCTURE}

   **Important Notes:**

   - **Focus on the Main Idea:**

   - Your summary should focus on the main idea of the research without including specific methods.
   - If your summary mentions methodologies used, you are not following the instructions.

   - **Provide Specific Feedback:**

   - Ensure that your feedback is specific and helpful to the methodologies assistant and the abstract sentence-level analysis assistant.
   - Do not provide feedback for the sake of it; only include feedback that will help them improve their analysis.

   - **JSON Output Only:**

   - Do not include the markdown JSON code block notation in your response.
   - Simply return the JSON object without surrounding it with ```json and ```.

   - **Properly Escape Special Notations:**

   - If you use any special notation (e.g., LaTeX) within the JSON values, ensure it is properly escaped.
   - Failure to do so may result in a JSON parsing error, which is considered a critical failure.

   - **Compliance is Mandatory:**

   - You must return the output in the specified JSON format.
   - Failure to do so will be considered a failure to complete the task.

   """

Summary JSON Structure
^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: python

   SUMMARY_JSON_STRUCTURE: str = """
   {
      "summary": "<Detailed summary of the abstract>",
   }
   """

Classification Prompts
-----------------------

Classification System Message
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   CLASSIFICATION_SYSTEM_MESSAGE: str = """
   You are an expert in topic classification of research paper abstracts. Your task is to classify the provided abstract into one or more of the specified categories. Use only the categories provided; do not create new ones. Focus on capturing the main idea of the research, not the specific methods used, unless the methods are central to the research or provide essential context.

   ## Categories You Can Classify the Abstract Into:

   {categories}

   ### Taxonomy Item Example:

   Use this example to understand the academic nature of the categories.

   {TAXONOMY_EXAMPLE}

   ### Additional Information:

   Previous AI assistants have processed the abstract to provide extra context. Here is their output:

   ### Methodologies:

   Extracted methodologies from the abstract.

   Methodologies Format Example:
   {METHOD_JSON_FORMAT}

   Output:
   {method_json_output}

   ### Abstract Summary:

   An overall in-depth summary of the abstract.

   Includes:
   - Summary.

   Abstract Summary Format Example:
   {SUMMARY_JSON_STRUCTURE}

   Output:
   {abstract_summary_output}

   ## Steps to Follow:

   1. Understand the Categories:
   - Carefully read and comprehend the provided categories.
   - Remember, these are ACADEMIC RESEARCH CATEGORIES (e.g., â€œeducationâ€ involves research around education).

   2. Review Additional Information:
   - Examine the outputs from previous assistants.
   - Use this information to gain a deeper understanding of the abstract.

   3. Classify the Abstract:
   - Assign the abstract to one or more of the provided categories.

   Output Format:

   Your output must be a JSON object with the following structure:

   {CLASSIFICATION_JSON_FORMAT}

   Important Notes:
   - Use Only Provided Categories:
   - Do not create new categories.
   - Label categories exactly as they appear in the list.
   - Focus on Research Themes:
   - Base your classification on the themes of the research described in the abstract.
   - Do not focus on specific methods unless they are central to the research.
   - Your response should be only the JSON object following the provided structure.
   - Do not include markdown code block notation or additional text.
   - Properly Escape Special Notations:
   - If using special notations (e.g., LaTeX) within JSON values, ensure they are properly escaped to prevent parsing errors.

   ## Compliance is Critical:

   Failure to follow the instructions and output format is considered a critical failure.
   """

Taxonomy Example
~~~~~~~~~~~~~~~~~~

.. code-block:: python

   TAXONOMY_EXAMPLE: str = """
   'Education': {
      'Education leadership and administration': [
         'Educational leadership and administration, general',
         "Higher education and community college administration",
         "Education leadership and administration nec"
      ],
      'Education research': [
         'Curriculum and instruction',
         'Educational assessment, evaluation, and research methods',
         'Educational/ instructional technology and media design',
         'Higher education evaluation and research',
         'Student counseling and personnel services',
         'Education research nec'
      ],
      "Teacher education": [
         "Adult, continuing, and workforce education and development",
         "Bilingual, multilingual, and multicultural education",
         "Mathematics teacher education",
         "Music teacher education",
         "Special education and teaching",
         "STEM educational methods",
         "Teacher education, science and engineering",
         "Teacher education, specific levels and methods",
         "Teacher education, specific subject areas"
      ],
      "Education, other": [
         "Education, general",
         "Education nec"
      ]
   }
   """

Classification JSON Format
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   CLASSIFICATION_JSON_FORMAT: str = """
   {
      "classifications": [
         {
               "categories": [
                  "<first category you decided to classify the abstract into>",
                  "<second category you decided to classify the abstract into>",
                  "<third category you decided to classify the abstract into>"
               ]
         }
      ]
   }
   """

Human Prompts
--------------

Human Message Prompt
~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   HUMAN_MESSAGE_PROMPT: str = """
   ## Abstract:
   {abstract}
   ## Extra Context:
   {extra_context}
   """

Notes
^^^^^

- Abstract: The abstract of the research paper.
- Extra Context: Optional information that can be injected into the prompt to help the LLM understand the task at hand. Current usage comes the scraping executed by the :class:`~academic_metrics.data_collection.scraper.Scraper` class's :meth:`~academic_metrics.data_collection.scraper.Scraper.setup_chain` method.

Theme Recognition
------------------

Theme Recognition System Message
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   THEME_RECOGNITION_SYSTEM_MESSAGE: str = """
   You are an AI assistant who is an expert in recognizing themes present in research paper abstracts. Your task is to identify the main themes present in the abstract. A theme is a main idea or central concept that the research is exploring; it should not be driven by the specific methods used to conduct the research.

   Previous AI assistants have processed the abstract in the following ways:

   - **Identifying and Extracting Methodologies Used in the Research**
   - **Creating a Summary of the Abstract**
   - **Classifying the Abstract into a Hierarchical Taxonomy**

   You will be provided with the outputs from these previous AI assistants.

   ### How to Use the Provided Information:

   - **Methodologies:**

   - Use the extracted methodologies to be aware of the methods present in the abstract.
   - This helps ensure your focus is on the themes related to the overall purpose of the research rather than the specific methods.

   - **Abstract Summary:**

   - Use the summary to understand the main points of the abstract.

   - **Categories (Hierarchical Taxonomy):**

   - Use the categories and their hierarchical components to understand where this abstract fits into the academic landscape.

   ### Your Task:

   - Identify the main themes present in the abstract.
   - First, determine if the abstract aligns with any of the provided themes (categories).
   - If you identify additional themes not covered by the current categories, add them to your output.

   ### Provided Outputs:

   #### Methodologies:

   - **Format of the Methodologies Assistant's Output:**

   {METHOD_JSON_FORMAT}

   - **Output from the Methodologies Assistant:**

   {method_json_output}

   #### Abstract Summary:

   - **Format of the Abstract Summary Assistant's Output:**

   {SUMMARY_JSON_STRUCTURE}

   - **Output from the Abstract Summary Assistant:**

   {abstract_summary_output}

   #### Categories the Abstract Has Been Classified Into:

   **Note**: Top means top-level category, Mid means mid-level category, and Low means low-level category. The levels refer to the hierarchy of the categories, it does not imply any ranking of relevance or importance, all are equally important.

   {categories}

   ### Output Format:

   Your output should be a JSON object following the provided structure:

   {THEME_RECOGNITION_JSON_FORMAT}

   **Important Notes:**

   - **Focus on Identifying Main Themes:**

   - Concentrate on the central ideas of the research, not the specific methods.
   - Use keywords as a guide but do not rely solely on them.

   - **Use of Categories:**

   - Start by identifying any current themes the abstract aligns with.
   - If additional themes are identified, include them in your output.

   - **JSON Output Requirements:**

   - Your output must be a JSON object following the provided structure exactly.
   - Do not change any keys or create your own keys.
   - Fill in all the values for each key, even if some are empty strings.

   - **Formatting:**

   - Do not include the markdown JSON code block notation in your response.
   - Simply return the JSON object.

   - **Special Notations:**

   - If you use any special notation (e.g., LaTeX) within the JSON values, ensure it is properly escaped to avoid parsing errors, which are considered a critical failure.

   """

Theme Recognition JSON Format
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   THEME_RECOGNITION_JSON_FORMAT: str = """
   {
      "themes": ["<list of all the themes you identified to be present in the abstract>"]
   }
   """
        ```

        docs/source/conf.py:
        ```
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
import os
import sys

sys.path.insert(0, os.path.abspath("../../src/academic_metrics/"))

project = "Academic Metrics"
copyright = "2024, Spencer Presley, Cole Barbes"
author = "Spencer Presley, Cole Barbes"
release = "0.1.0-beta"

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.napoleon",
    "sphinx.ext.viewcode",
    "sphinx.ext.doctest",
    "sphinx_autodoc_typehints",
    "sphinx.ext.intersphinx",
    "sphinx.ext.autosummary",
    "sphinx.ext.autosectionlabel",
    "sphinx.ext.extlinks",
]

templates_path = ["_templates"]
exclude_patterns = []

intersphinx_mapping = {
    "python": ("https://docs.python.org/3", None),
    "langchain": ("https://python.langchain.com/api_reference/", None),
    "pydantic": ("https://docs.pydantic.dev/latest/", None),
    "pylatexenc": ("https://pylatexenc.readthedocs.io/en/latest/", None),
}

# Add extlinks config
extlinks = {
    "pypi": ("https://pypi.org/project/%s/", "%s"),
}

# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output

# THIS IS THE DEFAULT THEME
# html_theme = 'alabaster'

# html_theme = 'sphinx_rtd_theme'
# html_theme = "piccolo_theme"
html_theme = "pydata_sphinx_theme"
html_theme_options = {
    "github_url": "https://github.com/spencerpresley/COSC425-DATA",
    "navbar_start": ["navbar-logo"],
    "logo": {"text": "Academic Metrics"},
    "navbar_align": "left",
    # "navigation_with_keys": True,
    "navigation_depth": 1,
    "show_toc_level": 2,
    "show_nav_level": 1,
    "primary_sidebar_end": ["indices"],
}

html_sidebars = {
    "**": [
        "searchbox.html",
        "relations.html",
        "globaltoc.html",
        "sourcelink.html",
    ],
    "index": [],
}

# html_sidebars = {
#     "*.html": ["sidebar-nav-bs.html"]
# }

html_static_path = ["_static"]

# Napoleon settings
napoleon_google_docstring = True
napoleon_numpy_docstring = False
napoleon_include_init_with_doc = True
napoleon_include_private_with_doc = True
napoleon_include_special_with_doc = True
napoleon_use_admonition_for_examples = True
napoleon_use_admonition_for_notes = True
napoleon_use_admonition_for_references = True
napoleon_use_ivar = False
napoleon_use_param = True
napoleon_use_rtype = True
napoleon_type_aliases = None

# add_module_names = False
toc_object_entries_show_parents = "hide"

autodoc_member_order = "bysource"

        ```

        docs/source/config.rst:
        ```
Global Configuration
======================

Overview
----------
.. automodule:: academic_metrics.configs.global_config
   :no-members:

Module Constants
-----------------
.. py:data:: LOG_TO_CONSOLE
   :type: bool
   
   Controls whether log messages are displayed in the console. Default is True.

.. py:data:: LOG_LEVEL
   :type: int

   Sets the default logging level. Default is DEBUG.

.. py:data:: DEBUG
   :type: int
.. py:data:: INFO
   :type: int
.. py:data:: WARNING
   :type: int
.. py:data:: ERROR
   :type: int
.. py:data:: CRITICAL
   :type: int

   Log level constants exported for use across the package.

Classes
---------

Color Formatter
~~~~~~~~~~~~~~~~
.. autoclass:: academic_metrics.configs.global_config.ColorFormatter
   :members:
   :undoc-members:
   :show-inheritance:
   :private-members:
   :special-members: __init__

Functions
-----------

Configure Logging
~~~~~~~~~~~~~~~~~~
.. autofunction:: academic_metrics.configs.global_config.configure_logging

Set Log to Console
~~~~~~~~~~~~~~~~~~~
.. autofunction:: academic_metrics.configs.global_config.set_log_to_console

        ```

        docs/source/constants.rst:
        ```
constants package
=================

Submodules
----------

constants.dir\_paths module
---------------------------

.. automodule:: constants.dir_paths
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: constants
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/core.rst:
        ```
core package
============

Submodules
----------

core.category\_processor module
-------------------------------

.. automodule:: academic_metrics.core.category_processor
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.core
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/data_collection.rst:
        ```
data\_collection package
========================

Submodules
----------

data\_collection.CrossrefWrapper module
---------------------------------------

.. automodule:: academic_metrics.data_collection.CrossrefWrapper
   :members:
   :undoc-members:
   :show-inheritance:

data\_collection.scraper module
-------------------------------

.. automodule:: academic_metrics.data_collection.scraper
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.data_collection
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/dataclass_models.rst:
        ```
dataclass\_models package
=========================

Submodules
----------

dataclass\_models.abstract\_base\_dataclass module
--------------------------------------------------

.. automodule:: academic_metrics.dataclass_models.abstract_base_dataclass
   :members:
   :undoc-members:
   :show-inheritance:

dataclass\_models.concrete\_dataclasses module
----------------------------------------------

.. automodule:: academic_metrics.dataclass_models.concrete_dataclasses
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.dataclass_models
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/enums.rst:
        ```
enums package
=============

Submodules
----------

enums.dataclass\_enums module
-----------------------------

.. automodule:: academic_metrics.enums.dataclass_enums
   :members:
   :undoc-members:
   :show-inheritance:

enums.enums module
------------------

.. automodule:: academic_metrics.enums.enums
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.enums
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/factories.rst:
        ```
factories package
=================

Submodules
----------

factories.abstract\_classifier\_factory module
----------------------------------------------

.. automodule:: academic_metrics.factories.abstract_classifier_factory
   :members:
   :undoc-members:
   :show-inheritance:

factories.dataclass\_factory module
-----------------------------------

.. automodule:: academic_metrics.factories.dataclass_factory
   :members:
   :undoc-members:
   :show-inheritance:

factories.strategy\_factory module
----------------------------------

.. automodule:: academic_metrics.factories.strategy_factory
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.factories
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/index.rst:
        ```
.. Academic Metrics documentation master file, created by
   sphinx-quickstart on Sun Nov 24 12:41:01 2024.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Academic Metrics Documentation
============================================

AI System
----------
.. toctree::
   :maxdepth: 2
   :caption: Generative AI Classification System:

   AI

Prompts
--------
.. toctree::
   :maxdepth: 2
   :caption: Prompts used in the AI system:

   ai_prompts

ChainBuilder
--------------
.. toctree::
   :maxdepth: 2
   :caption: Langchain wrapper for quickly building LLM chains:

   ChainBuilder

Core
----
.. toctree::
   :maxdepth: 2
   :caption: Core data processing and formatting:

   core

Configuration
-------------
.. toctree::
   :maxdepth: 2
   :caption: Logging configuration and constants:

   config

Constants
---------
.. toctree::
   :maxdepth: 2
   :caption: Constants used for directory paths and execution time:

   constants

Data Collection
---------------
.. toctree::
   :maxdepth: 2
   :caption: Crossref API and Scraping:

   data_collection

Data Classes
-------------
.. toctree::
   :maxdepth: 2
   :caption: Data class models used throughout the system:

   dataclass_models

Database
--------
.. toctree::
   :maxdepth: 2
   :caption: Class for interacting with the database:

   DB

Enums
------
.. toctree::
   :maxdepth: 2
   :caption: Enums used for type safety in factory/decorator/strategy patterns:

   enums

Factories
----------
.. toctree::
   :maxdepth: 2
   :caption: Factory classes for creating objects:

   factories

Orchestrators
--------------
.. toctree::
   :maxdepth: 2
   :caption: Orchestrator classes for AI system and data processing:

   orchestrators

Other
------
.. toctree::
   :maxdepth: 2
   :caption: Taxonomy string representation:

   other

Postprocessing
---------------
.. toctree::
   :maxdepth: 2
   :caption: Duplicate detection and removal:

   postprocessing

Runners
-------
.. toctree::
   :maxdepth: 2
   :caption: Runner classes for executing the entire pipeline:

   runners

Strategies
-----------
.. toctree::
   :maxdepth: 2
   :caption: Strategy classes:

   strategies

Utils
------
.. toctree::
   :maxdepth: 2
   :caption: Utility classes and functions:

   utils

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

        ```

        docs/source/orchestrators.rst:
        ```
orchestrators package
=====================

Submodules
----------

orchestrators.category\_data\_orchestrator module
-------------------------------------------------

.. automodule:: academic_metrics.orchestrators.category_data_orchestrator
   :members:
   :undoc-members:
   :show-inheritance:

orchestrators.classification\_orchestrator module
-------------------------------------------------

.. automodule:: academic_metrics.orchestrators.classification_orchestrator
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.orchestrators
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/other.rst:
        ```
other package
=============

Submodules
----------

other.in\_memory\_taxonomy module
---------------------------------

This module contains the taxonomy made available by **NSF NCSES** as a string so it can be easily loaded into memory by :class:`~academic_metrics.utils.taxonomy_util.Taxonomy`.

Credits:
~~~~~~~~

`NSF NCSES Higher Education Research and Development Survey (HERD) <https://ncses.nsf.gov/pubs/ncses23200>`_.

TAXONOMY_AS_STRING
~~~~~~~~~~~~~~~~~

.. code-block:: python

   TAXONOMY_AS_STRING: str = """
   {
      "Agricultural sciences and natural resources": {
         "Agricultural, animal, plant, and veterinary sciences": [
               "Agronomy and crop science",
               "Animal sciences",
               "Food science and technology",
               "Plant sciences",
               "Soil sciences",
               "Veterinary biomedical and clinical sciences",
               "Agricultural, animal, plant, and veterinary sciences nec"
         ],
         "Natural resources and conservation": [
               "Environmental science",
               "Environmental natural resources management and policy",
               "Forestry",
               "Natural resources conservation and research",
               "Natural resources and conservation nec"
         ]
      },
      "Biological and biomedical sciences": {
         "Biochemistry, biophysics, and molecular biology": [
               "Biochemistry",
               "Biochemistry and molecular biology",
               "Biophysics",
               "Molecular biology",
               "Biochemistry, biophysics, and molecular biology nec"
         ],
         "Bioinformatics, biostatistics, and computational biology": [
               "Bioinformatics",
               "Biostatistics",
               "Computational biology",
               "Bioinformatics, biostatistics, and computational biology nec"
         ],
         "Cell cellular biology and anatomy": [
               "Cell cellular and molecular biology",
               "Developmental biology and embryology",
               "Cell cellular biology and anatomy nec"
         ],
         "Ecology, evolutionary biology, and epidemiology": [
               "Ecology",
               "Ecology and evolutionary biology",
               "Epidemiology",
               "Epidemiology and biostatistics",
               "Evolutionary biology",
               "Ecology, evolutionary biology, and epidemiology nec"
         ],
         "Genetics and genomics": [
               "Genetics, general",
               "Genome sciences and genomics",
               "Human medical genetics",
               "Molecular genetics",
               "Genetics nec"
         ],
         "Microbiology and immunology": [
               "Immunology",
               "Microbiology and immunology",
               "Microbiology, general",
               "Virology",
               "Microbiology and immunology nec"
         ],
         "Neurobiology and neurosciences": [
               "Cognitive neuroscience",
               "Neurobiology and anatomy",
               "Neuroscience",
               "Neurobiology and neurosciences nec"
         ],
         "Pharmacology and toxicology": [
               "Pharmacology",
               "Toxicology",
               "Pharmacology and toxicology nec"
         ],
         "Physiology, oncology and cancer biology": [
               "Biomechanics",
               "Exercise physiology and kinesiology",
               "Oncology and cancer biology",
               "Physiology, general",
               "Physiology, oncology and cancer biology nec"
         ],
         "Biological and biomedical sciences, other": [
               "Biology biological sciences, general",
               "Biomedical sciences, general",
               "Botany and plant biology",
               "Entomology",
               "Plant pathology and phytopathology",
               "Plant physiology and biology nec",
               "Zoology and animal biology",
               "Biological and biomedical sciences nec"
         ]
      },
      "Computer and information sciences": {
         "Computer science": [
               "Computer science"
         ],
         "Computer and information sciences, other": [
               "Artificial intelligence",
               "Computer and information sciences, general",
               "Computer systems networking and telecommunications",
               "Informatics and information technology ",
               "Information science studies",
               "Computer and information sciences nec"
         ]
      },
      "Engineering": {
         "Biological, biomedical, and biosystems engineering": [
               "Bioengineering and biomedical engineering",
               "Biological and biosystems engineering and biomedical technology"
         ],
         "Chemical and petroleum engineering": [
               "Chemical and biomolecular engineering",
               "Chemical engineering",
               "Petroleum engineering",
               "Chemical and petroleum engineering nec"
         ],
         "Civil, environmental, and transportation engineering": [
               "Civil engineering",
               "Environmental environmental health engineering",
               "Geotechnical and geoenvironmental engineering",
               "Structural engineering",
               "Transportation and highway engineering",
               "Civil, environmental, and transportation engineering nec"
         ],
         "Electrical and computer engineering": [
               "Computer engineering",
               "Electrical and computer engineering",
               "Electrical and electronics engineering",
               "Electrical and computer engineering nec"
         ],
         "Engineering technologies": [
               "Electrical and electronic engineering technologies",
               "Electromechanical technologies",
               "Environmental control technologies",
               "Engineering technologies nec"
         ],
         "Industrial engineering and operations research": [
               "Industrial engineering",
               "Operations research",
               "Systems and manufacturing engineering"
         ],
         "Materials and mining engineering": [
               "Materials engineering",
               "Materials science and engineering",
               "Materials and mining engineering nec"
         ],
         "Mechanical engineering": [
               "Mechanical engineering, general"
         ],
         "Engineering, other": [
               "Aerospace, aeronautical, astronautical, and space engineering",
               "Engineering mechanics, physics, and science",
               "Nanotechnology",
               "Nuclear engineering",
               "Engineering nec"
         ]
      },
      "Geosciences, atmospheric, and ocean sciences": {
         "Geological and earth sciences": [
               "Geochemistry",
               "Geology",
               "Geology earth science, general",
               "Geophysics and seismology",
               "Hydrology and water resources science",
               "Geological and earth sciences nec"
         ],
         "Ocean marine sciences and atmospheric science": [
               "Atmospheric sciences and meteorology, general",
               "Climatology, atmospheric chemistry and physics",
               "Marine biology and biological oceanography",
               "Marine sciences",
               "Oceanography, chemical and physical",
               "Atmospheric sciences and meteorology nec"
         ]
      },
      "Health sciences": {
         "Nursing and nursing science": [
               "Nursing education",
               "Nursing science",
               "Nursing specialties and practice"
         ],
         "Pharmacy and pharmaceutical sciences": [
               "Medicinal and pharmaceutical chemistry",
               "Pharmaceutical sciences",
               "Pharmacy, pharmaceutical sciences, and administration nec"
         ],
         "Public health": [
               "Environmental health",
               "Health services research",
               "Health medical physics",
               "Public health education and promotion",
               "Public health, general",
               "Public health nec"
         ],
         "Health sciences, other": [
               "Communication disorders sciences",
               "Exercise science and kinesiology",
               "Health sciences, general",
               "Marriage and family therapy counseling",
               "Medical clinical science",
               "Medical, biomedical, and health informatics",
               "Mental health, counseling, and therapy services and sciences",
               "Rehabilitation and therapeutic sciences",
               "Health sciences nec"
         ]
      },
      "Mathematics and statistics": {
         "Applied mathematics": [
               "Applied mathematics, general",
               "Computational and applied mathematics",
               "Applied mathematics nec"
         ],
         "Mathematics": [
               "Algebra and number theory",
               "Mathematics, general",
               "Mathematics nec"
         ],
         "Statistics": [
               "Applied statistics, general",
               "Mathematics and statistics",
               "Statistics",
               "Statistics nec"
         ]
      },
      "Multidisciplinary interdisciplinary sciences": {
         "Interdisciplinary computer sciences": [
               "Computer science and engineering",
               "Electrical engineering and computer science",
               "Interdisciplinary computer sciences nec"
         ],
         "Multidisciplinary interdisciplinary sciences, other": [
               "Behavioral and cognitive sciences",
               "Computational science and engineering",
               "Data science and data analytics",
               "History philosophy of science, technology and society",
               "Nanoscience nanoscale science",
               "Nutrition sciences",
               "Multidisciplinary interdisciplinary sciences nec"
         ]
      },
      "Physical sciences": {
         "Astronomy and astrophysics": [
               "Astronomy",
               "Astrophysics",
               "Astronomy and astrophysics nec"
         ],
         "Chemistry": [
               "Analytical chemistry",
               "Chemical biology",
               "Chemistry, general",
               "Inorganic chemistry",
               "Organic chemistry",
               "Physical chemistry",
               "Polymer chemistry",
               "Theoretical chemistry",
               "Chemistry nec"
         ],
         "Materials sciences": [
               "Materials science",
               "Materials chemistry and materials science nec"
         ],
         "Physics": [
               "Applied physics",
               "Atomic molecular physics",
               "Condensed matter and materials physics",
               "Elementary particle physics",
               "Nuclear physics",
               "Optics optical sciences",
               "Physics and astronomy",
               "Physics, general",
               "Plasma and high-temperature physics",
               "Theoretical and mathematical physics",
               "Physics and physical sciences nec"
         ]
      },
      "Psychology": {
         "Clinical psychology": [
               "Clinical child psychology",
               "Clinical psychology"
         ],
         "Counseling and applied psychology": [
               "Applied behavior analysis",
               "Counseling psychology",
               "Educational psychology",
               "Industrial and organizational psychology",
               "School psychology",
               "Counseling and applied psychology nec"
         ],
         "Research and experimental psychology": [
               "Behavioral neuroscience",
               "Cognitive psychology and psycholinguistics",
               "Developmental and child psychology",
               "Experimental psychology",
               "Social psychology",
               "Research and experimental psychology nec"
         ],
         "Psychology, other": [
               "Human development",
               "Psychology, general",
               "Psychology nec"
         ]
      },
      "Social sciences": {
         "Anthropology": [
               "Anthropology, general",
               "Cultural anthropology",
               "Physical and biological anthropology",
               "Anthropology nec"
         ],
         "Area, ethnic, cultural, gender, and group studies": [
               "Area studies",
               "Ethnic studies",
               "Area, ethnic, cultural, gender, and group studies nec"
         ],
         "Economics": [
               "Agricultural economics",
               "Applied economics",
               "Development economics and international development",
               "Econometrics and quantitative economics",
               "Economics, general",
               "Environmental natural resource economics",
               "Economics nec"
         ],
         "Political science and government": [
               "Political science and government, general",
               "Political science and government nec"
         ],
         "Public policy analysis": [
               "Education policy analysis",
               "Health policy analysis",
               "Public policy analysis, general",
               "Public policy nec"
         ],
         "Sociology, demography, and population studies": [
               "Sociology, general",
               "Sociology, demography, and population studies nec"
         ],
         "Social sciences, other": [
               "Applied linguistics",
               "Archeology",
               "Criminal justice and corrections",
               "Criminology",
               "Geography and cartography",
               "International relations and national security studies",
               "Linguistics",
               "Social sciences nec"
         ]
      },
      "Business": {
         "Business administration and management": [
               "Business management and administration",
               "Organizational leadership",
               "Business administration and management nec"
         ],
         "Business, other": [
               "Accounting and accounting-related",
               "Finance and financial management",
               "Management information systems",
               "Management sciences",
               "Marketing",
               "Organizational behavior studies",
               "Business nec"
         ]
      },
      "Education": {
         "Education leadership and administration": [
               "Educational leadership and administration, general",
               "Higher education and community college administration",
               "Education leadership and administration nec"
         ],
         "Education research": [
               "Curriculum and instruction",
               "Educational assessment, evaluation, and research methods",
               "Educational instructional technology and media design",
               "Higher education evaluation and research",
               "Student counseling and personnel services",
               "Education research nec"
         ],
         "Teacher education": [
               "Adult, continuing, and workforce education and development",
               "Bilingual, multilingual, and multicultural education",
               "Mathematics teacher education",
               "Music teacher education",
               "Special education and teaching",
               "STEM educational methods",
               "Teacher education, science and engineering",
               "Teacher education, specific levels and methods",
               "Teacher education, specific subject areas"
         ],
         "Education, other": [
               "Education, general",
               "Education nec"
         ]
      },
      "Humanities": {
         "English language and literature, letters": [
               "American literature (United States)",
               "Creative writing",
               "English language and literature, general",
               "English literature (British and commonwealth)",
               "Rhetoric and composition, and writing studies",
               "English language and literature nec"
         ],
         "Foreign languages, literatures, and linguistics": [
               "Comparative literature",
               "Hispanic Latin American languages, literatures, and linguistics",
               "Romance languages, literatures, and linguistics",
               "Spanish language and literature",
               "Foreign languages, literatures, and linguistics nec"
         ],
         "History": [
               "American history (United States)",
               "European history",
               "History, general",
               "History, regional focus",
               "History nec"
         ],
         "Philosophy and religious studies": [
               "Philosophy",
               "Religion religious studies",
               "Philosophy and religious studies nec"
         ],
         "Humanities, other": [
               "Bible biblical studies",
               "Humanities and humanistic studies",
               "Theological and ministerial studies"
         ]
      },
      "Visual and performing arts": {
         "Performing arts": [
               "Dance, drama, theatre arts and stagecraft",
               "Music performance",
               "Music theory and composition",
               "Musicology and ethnomusicology",
               "Music nec"
         ],
         "Visual arts, media studies, and design": [
               "Art history, criticism and conservation",
               "Film, cinema, and media studies",
               "Visual arts, media studies design, and arts management nec"
         ]
      },
      "Other non-science and engineering": {
         "Communications and journalism": [
               "Applied communication, advertising, and public relations",
               "Communication and media studies",
               "Communication, general",
               "Mass communication media studies",
               "Communications and journalism nec"
         ],
         "Multidisciplinary interdisciplinary studies": [
               "Classical and ancient studies",
               "Multidisciplinary interdisciplinary studies nec"
         ],
         "Public administration and social services": [
               "Public administration",
               "Social work and human services"
         ],
         "Non-science and engineering, other": [
               "Architecture and architectural studies ",
               "City urban, community, and regional planning",
               "Family, consumer sciences and human sciences",
               "Homeland security and protective services",
               "Law, legal studies and research",
               "Parks, recreation, leisure, fitness, and sport studies and management",
               "Other non-science and engineering nec"
         ]
      }
   }
   """

        ```

        docs/source/postprocessing.rst:
        ```
postprocessing package
======================

Submodules
----------

postprocessing.BasePostprocessor module
---------------------------------------

.. automodule:: academic_metrics.postprocessing.BasePostprocessor
   :members:
   :undoc-members:
   :show-inheritance:

postprocessing.DepartmentPostprocessor module
---------------------------------------------

.. automodule:: academic_metrics.postprocessing.DepartmentPostprocessor
   :members:
   :undoc-members:
   :show-inheritance:

postprocessing.FacultyPostprocessor module
------------------------------------------

.. automodule:: academic_metrics.postprocessing.FacultyPostprocessor
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.postprocessing
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/runners.rst:
        ```
runners package
===============

Submodules
----------

runners.pipeline module
-----------------------

.. automodule:: academic_metrics.runners.pipeline
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.runners
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/strategies.rst:
        ```
strategies package
==================

Submodules
----------

strategies.AttributeExtractionStrategies module
-----------------------------------------------

.. automodule:: academic_metrics.strategies.AttributeExtractionStrategies
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.strategies
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/source/utils.rst:
        ```
utils package
=============

Submodules
----------

utils.api\_key\_validator module
--------------------------------

.. automodule:: academic_metrics.utils.api_key_validator
   :members:
   :undoc-members:
   :show-inheritance:

utils.minhash\_util module
--------------------------

.. automodule:: academic_metrics.utils.minhash_util
   :members:
   :undoc-members:
   :show-inheritance:

utils.taxonomy\_util module
---------------------------

.. automodule:: academic_metrics.utils.taxonomy_util
   :members:
   :undoc-members:
   :show-inheritance:

utils.unicode\_chars\_dict module
---------------------------------

.. automodule:: academic_metrics.utils.unicode_chars_dict
   :members:
   :undoc-members:
   :show-inheritance:

utils.utilities module
----------------------

.. automodule:: academic_metrics.utils.utilities
   :members:
   :undoc-members:
   :show-inheritance:

utils.warning\_manager module
-----------------------------

.. automodule:: academic_metrics.utils.warning_manager
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: academic_metrics.utils
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/temp_source/abstract_classifier.rst:
        ```
Abstract Classifier Module
============================

.. automodule:: academic_metrics.AI.AbstractClassifier
   :members:
   :undoc-members:
   :show-inheritance:
   :private-members:
   :special-members: __init__
        ```

        docs/temp_source/category_processor.rst:
        ```
Category Processor Module
============================

.. automodule:: academic_metrics.core.category_processor
   :members:
   :undoc-members:
   :show-inheritance:
   :private-members:
   :special-members: __init__
        ```

        docs/temp_source/configs.rst:
        ```
configs package
===============

Submodules
----------

configs.global\_config module
-----------------------------

.. automodule:: configs.global_config
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: configs
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/temp_source/constants.rst:
        ```
constants package
=================

Submodules
----------

constants.dir\_paths module
---------------------------

.. automodule:: constants.dir_paths
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: constants
   :members:
   :undoc-members:
   :show-inheritance:

        ```

        docs/temp_source/faculty_set_postprocessor.rst:
        ```
Faculty Set Processing
========================

Overview
----------
.. automodule:: academic_metrics.core.faculty_set_postprocessor
   :no-members:

Classes
--------

Faculty Postprocessor
~~~~~~~~~~~~~~~~~~~~~~
.. autoclass:: academic_metrics.core.faculty_set_postprocessor.FacultyPostprocessor
   :members:
   :undoc-members:
   :show-inheritance:
   :private-members:
   :special-members: __init__
   :noindex:

MinHash Utility
~~~~~~~~~~~~~~~~
.. autoclass:: academic_metrics.core.faculty_set_postprocessor.MinHashUtility
   :members:
   :undoc-members:
   :show-inheritance:
   :private-members:
   :special-members: __init__
   :noindex:

Name Variation
~~~~~~~~~~~~~~~
.. autoclass:: academic_metrics.core.faculty_set_postprocessor.NameVariation
   :members:
   :undoc-members:
   :show-inheritance:
   :private-members:
   :noindex:
        ```

        docs/temp_source/modules.rst:
        ```
AI
==

.. toctree::
   :maxdepth: 4

   AI

        ```

setup_environment.py:
```
import subprocess
import sys
import os


def run_command(command, error_message, cwd=None):
    try:
        subprocess.run(command, check=True, cwd=cwd)
        return True
    except subprocess.CalledProcessError:
        print(f"Error: {error_message}")
        return False
    except FileNotFoundError:
        print(
            f"Error: Required command not found. Make sure all dependencies are installed."
        )
        return False


def setup_git_hooks():
    """Set up git hooks by appending black formatting to the pre-commit hook"""
    # Get the project root directory (where pyproject.toml is)
    project_root = os.path.dirname(os.path.abspath(__file__))
    git_hooks_dir = os.path.join(project_root, ".git", "hooks")
    pre_commit_path = os.path.join(git_hooks_dir, "pre-commit")

    # Ensure the hooks directory exists
    os.makedirs(git_hooks_dir, exist_ok=True)

    # If the file doesn't exist, create it with a shebang
    if not os.path.exists(pre_commit_path):
        with open(pre_commit_path, "w") as f:
            f.write("#!/bin/bash\n\n")

    # Append our black formatting commands
    with open(pre_commit_path, "a") as f:
        f.write("\n# Added by setup_environment.py - Black formatting\n")
        f.write("PROJECT_ROOT=$(git rev-parse --show-toplevel)\n")
        f.write('cd "$PROJECT_ROOT"\n')
        f.write("black .\n")
        f.write("# Stage only Python files that were modified by black\n")
        f.write("git diff --name-only | grep '.py$' | xargs -I {} git add {}\n")

    # Make the script executable
    os.chmod(pre_commit_path, 0o755)

    return True


def main():
    # Get the project root directory
    project_root = os.path.dirname(os.path.abspath(__file__))

    # Check if we're in the correct directory (where pyproject.toml exists)
    if not os.path.exists(os.path.join(project_root, "pyproject.toml")):
        print(
            "Error: pyproject.toml not found. Please run this script from the project root directory."
        )
        sys.exit(1)

    print("Setting up development environment...")

    # Install package in development mode
    print("\n1. Installing package...")
    if not run_command(
        [sys.executable, "-m", "pip", "install", "-e", "."],
        "Failed to install package",
        cwd=project_root,
    ):
        sys.exit(1)

    # Install git hooks
    print("\n2. Setting up git hooks...")
    if not setup_git_hooks():
        print("Failed to set up git hooks")
        sys.exit(1)

    print("\nSetup complete! Your development environment is ready.")
    print("Black formatting will run automatically on commits.")


if __name__ == "__main__":
    main()

```

    src/__init__.py:
    ```

    ```

            src/academic_metrics/AI/__init__.py:
            ```
from .abstract_classifier import AbstractClassifier

            ```

            src/academic_metrics/AI/abstract_classifier.py:
            ```
from __future__ import annotations

import json
import logging
import os
import traceback
from collections import defaultdict
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Self, Tuple, Union, cast

from academic_metrics.ai_data_models.ai_pydantic_models import (
    AbstractSentenceAnalysis,
    AbstractSummary,
    ClassificationOutput,
    MethodExtractionOutput,
    ThemeAnalysis,
)
from academic_metrics.ai_prompts import (
    # Method prompts
    METHOD_EXTRACTION_CORRECT_EXAMPLE_JSON,
    METHOD_EXTRACTION_INCORRECT_EXAMPLE_JSON,
    METHOD_EXTRACTION_SYSTEM_MESSAGE,
    METHOD_JSON_FORMAT,
    # Sentence analysis prompts
    ABSTRACT_SENTENCE_ANALYSIS_SYSTEM_MESSAGE,
    SENTENCE_ANALYSIS_JSON_EXAMPLE,
    # Summary prompts
    ABSTRACT_SUMMARY_SYSTEM_MESSAGE,
    SUMMARY_JSON_STRUCTURE,
    # Classification prompts
    CLASSIFICATION_JSON_FORMAT,
    CLASSIFICATION_SYSTEM_MESSAGE,
    TAXONOMY_EXAMPLE,
    # Theme prompts
    THEME_RECOGNITION_JSON_FORMAT,
    THEME_RECOGNITION_SYSTEM_MESSAGE,
    # Human prompt
    HUMAN_MESSAGE_PROMPT,
)
from academic_metrics.ChainBuilder import ChainManager
from academic_metrics.configs import (
    configure_logging,
    DEBUG,
    LOG_TO_CONSOLE,
)

if TYPE_CHECKING:
    from academic_metrics.utils.taxonomy_util import Taxonomy


class AbstractClassifier:
    """A class for processing research paper abstracts through AI-powered analysis and classification.

    This class manages a complete pipeline for analyzing academic paper abstracts, including:
    - Method extraction from abstracts
    - Sentence-by-sentence analysis
    - Abstract summarization
    - Hierarchical taxonomy classification
    - Theme recognition and analysis

    The pipeline uses three separate chain managers for different stages of processing:
    1. Pre-classification: Method extraction, sentence analysis, and summarization
    2. Classification: Hierarchical taxonomy classification
    3. Theme Recognition: Theme identification and analysis

    Args:
        taxonomy (Taxonomy): Taxonomy instance containing the classification hierarchy
        doi_to_abstract_dict (Dict[str, str]): Mapping of DOIs to abstract texts
        api_key (str): API key for LLM access
        log_to_console (bool, optional): Whether to log output to console. Defaults to True
        extra_context (Dict[str, Any], optional): Additional context for classification. Defaults to None
        pre_classification_model (str, optional): Model name for pre-classification tasks. Defaults to "gpt-4o-mini"
        classification_model (str, optional): Model name for classification tasks. Defaults to "gpt-4o-mini"
        theme_model (str, optional): Model name for theme recognition tasks. Defaults to "gpt-4o-mini"
        max_classification_retries (int, optional): Maximum retries for failed classifications. Defaults to 3

    Attributes:
        classification_results (Dict[str, Dict]): Processed results by DOI, containing categories and themes
        raw_classification_outputs (List[Dict]): Raw outputs from the classification chain
        raw_theme_outputs (Dict[str, Dict]): Raw theme analysis results by DOI

    Methods:
        classify(): Process all abstracts through the complete pipeline
        get_classification_results_by_doi(doi, return_type): Get results for a specific DOI
        get_raw_classification_outputs(): Get all raw classification outputs
        get_raw_theme_results(): Get all raw theme analysis results
        save_classification_results(output_path): Save processed results to JSON
        save_raw_classification_results(output_path): Save raw classification outputs
        save_raw_theme_results(output_path): Save raw theme results

    Raises:
        ValueError: If required attributes are missing or invalid
        TypeError: If api_key cannot be converted to string
    """

    def __init__(
        self,
        taxonomy: Taxonomy,
        doi_to_abstract_dict: Dict[str, str],
        api_key: str,
        log_to_console: bool | None = LOG_TO_CONSOLE,
        extra_context: Dict[str, Any] | None = None,
        pre_classification_model: str | None = "gpt-4o-mini",
        classification_model: str | None = "gpt-4o-mini",
        theme_model: str | None = "gpt-4o-mini",
        max_classification_retries: int | None = 3,
    ) -> None:
        """Initializes a new AbstractClassifier instance.

        Sets up the complete classification pipeline including chain managers for pre-classification,
        classification, and theme recognition. Initializes data structures for storing results and
        configures logging.

        Args:
            taxonomy (Taxonomy): Taxonomy instance containing the hierarchical category structure.
                Type: :class:`academic_metrics.utils.taxonomy_util.Taxonomy`
            doi_to_abstract_dict (Dict[str, str]): Dictionary mapping DOIs to their abstract texts.
            api_key (str): API key for accessing the language model service.
            log_to_console (bool | None): Whether to output logs to console.
                Type: bool | None
                Defaults to LOG_TO_CONSOLE config value.
            extra_context (Dict[str, Any] | None): Additional context for classification.
                Type: Dict[str, Any] | None
                Defaults to None.
            pre_classification_model (str | None): Model name for pre-classification tasks.
                Type: str | None
                Defaults to "gpt-4o-mini".
            classification_model (str | None): Model name for classification tasks.
                Type: str | None
                Defaults to "gpt-4o-mini".
            theme_model (str | None): Model name for theme recognition tasks.
                Type: str | None
                Defaults to "gpt-4o-mini".
            max_classification_retries (int | None): Maximum attempts for failed classifications.
                Type: int | None
                Defaults to 3.

        Raises:
            ValueError: If api_key is empty or invalid.
            TypeError: If api_key cannot be converted to string.
        """

        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="abstract_classifier",
            log_level=DEBUG,
        )
        self.log_to_console = log_to_console
        self.logger.info("Initializing AbstractClassifier")
        self.logger.info("Performing setup")

        self.banned_categories: List[str] = []

        self.logger.info("Setting API key")
        self._run_initial_api_key_validation(api_key)
        self.api_key = api_key
        self.logger.info("API key set")

        self.logger.info("Setting taxonomy")
        self.taxonomy = taxonomy
        self.logger.info("Taxonomy set")

        self.logger.info("Setting DOI to abstract dictionary")
        self.doi_to_abstract_dict = doi_to_abstract_dict
        self.logger.info("DOI to abstract dictionary set")

        self.logger.info("Setting extra context")
        self.extra_context = extra_context
        self.logger.info("Extra context set")

        self.logger.info("Setting models")
        self.logger.debug(
            f"pre_classification_model (before setting): {pre_classification_model}"
        )
        self._pre_classification_model = pre_classification_model
        self.logger.debug(
            f"_pre_classification_model (after setting): {self._pre_classification_model}"
        )
        self._classification_model = classification_model
        self._theme_model = theme_model
        self.logger.info("Models set")

        self.logger.info("Setting max classification retries")
        self.max_classification_retries = max_classification_retries
        self.logger.info(
            f"Max classification retries set to: {self.max_classification_retries}"
        )

        self.logger.info("Initialized taxonomy and abstracts")
        self.classification_results: Dict[str, Dict[str, Any]] = {
            doi: defaultdict(  # Top categories
                lambda: defaultdict(list)  # Mid categories with lists of low categories
            )
            for doi in self.doi_to_abstract_dict.keys()
        }
        self.logger.info("Initialized classification results")

        self.logger.info(
            "Initializing raw outputs list used to store raw outputs from chain layers"
        )
        self.raw_classification_outputs: List[Dict[str, Any]] = []
        self.logger.info("Initialized raw classification outputs list")

        self.logger.info(
            "Initializing raw theme outputs dictionary used to store raw outputs from theme recognition chain layers"
        )
        self.raw_theme_outputs = {doi: {} for doi in self.doi_to_abstract_dict.keys()}
        self.logger.info("Initialized raw theme outputs dictionary")

        self.logger.info("Initializing chain managers")
        self.logger.info(
            "Initializing and adding layers to pre-classification chain manager"
        )
        self.pre_classification_chain_manager: ChainManager = (
            self._initialize_pre_classification_chain_manager()
        )
        self._add_method_extraction_layer(
            self.pre_classification_chain_manager
        )._add_sentence_analysis_layer(
            self.pre_classification_chain_manager
        )._add_summary_layer(
            self.pre_classification_chain_manager
        )

        self.logger.info(
            "Pre-classification chain manager initialized and layers added"
        )

        self.logger.info(
            "Initializing and adding layers to classification chain manager"
        )
        self.classification_chain_manager: ChainManager = (
            self._initialize_classification_chain_manager()
        )
        self._add_classification_layer(self.classification_chain_manager)
        self.logger.info("Classification chain manager initialized and layers added")

        self.logger.info(
            "Initializing and adding layers to theme recognition chain manager"
        )
        self.theme_chain_manager: ChainManager = self._initialize_theme_chain_manager()
        self._add_theme_recognition_layer(self.theme_chain_manager)
        self.logger.info("Theme recognition chain manager initialized and layers added")

    def _run_initial_api_key_validation(self, api_key: str) -> None:
        """Validates the API key format and presence during initialization.

        Performs initial validation of the API key to ensure it exists and can be
        converted to a string type. Called during class initialization before any
        API operations are attempted.

        Args:
            api_key (str): The API key to validate. Should be a non-empty string or a value that can be converted to a string.
                Type: str

        Raises:
            ValueError: If the API key is empty, None, cannot be converted to a string,
                or if the conversion fails for any reason.

        Returns:
            None
        """
        if not api_key:
            raise ValueError(
                "API key is required"
                f"Received type: {type(api_key)}, "
                f"Value: {'<empty>' if not api_key else 'Value present but may not be a string'}"
            )

        try:
            api_key: str = cast(str, str(api_key))
        except Exception as e:
            raise ValueError(
                f"API key must be a string or be convertible to a string. "
                f"Received type: {type(api_key)}, "
                f"Value: {'<empty>' if not api_key else '<redacted>'}, "
                f"Error: {str(e)}"
            ) from e

    def _initialize_pre_classification_chain_manager(self) -> ChainManager:
        """Initializes a new ChainManager instance for pre-classification tasks.

        Creates and configures a ChainManager specifically for the pre-classification stage
        of the pipeline, which includes method extraction, sentence analysis, and abstract
        summarization.

        Returns:
            ChainManager: A new ChainManager instance configured with:
                Type: :class:`academic_metrics.ChainBuilder.ChainBuilder.ChainManager`
                - Model: self._pre_classification_model
                - Temperature: 0.0 (deterministic outputs)
                - Console logging: Based on self.log_to_console setting
        """
        self.logger.debug(
            f"pre_classification_model type: {type(self._pre_classification_model)}"
        )
        self.logger.debug(
            f"pre_classification_model value: {self._pre_classification_model}"
        )
        return ChainManager(
            llm_model=self._pre_classification_model,
            api_key=self.api_key,
            llm_temperature=0.0,
            log_to_console=self.log_to_console,
        )

    def _initialize_classification_chain_manager(self) -> ChainManager:
        """Initializes a new ChainManager instance for taxonomy classification tasks.

        Creates and configures a ChainManager specifically for the classification stage
        of the pipeline, which handles the hierarchical taxonomy classification of abstracts
        at all levels (top, mid, and low).

        Returns:
            ChainManager: A new ChainManager instance configured with:
                Type: :class:`academic_metrics.ChainBuilder.ChainBuilder.ChainManager`
                - Model: self._classification_model
                - Temperature: 0.0 (deterministic outputs)
                - Console logging: Based on self.log_to_console setting
        """
        return ChainManager(
            llm_model=self._classification_model,
            api_key=self.api_key,
            llm_temperature=0.0,
            log_to_console=self.log_to_console,
        )

    def _initialize_theme_chain_manager(self) -> ChainManager:
        """Initializes a new ChainManager instance for theme recognition tasks.

        Creates and configures a ChainManager specifically for the theme recognition stage
        of the pipeline, which identifies key themes and concepts from classified abstracts.

        Args:
            None

        Returns:
            ChainManager: A new ChainManager instance configured with:
                Type: :class:`academic_metrics.ChainBuilder.ChainBuilder.ChainManager`
                - Model: self._theme_model
                - Temperature: 0.9 (creative theme generation)
                - Console logging: Based on self.log_to_console setting
        """
        return ChainManager(
            llm_model=self._theme_model,
            api_key=self.api_key,
            llm_temperature=0.9,
            log_to_console=self.log_to_console,
        )

    def _add_method_extraction_layer(self, chain_manager: ChainManager) -> Self:
        """Adds the method extraction processing layer to the chain manager.

        This layer analyzes abstracts to identify and extract research methods, techniques,
        and approaches used in the paper.

        Args:
            chain_manager (ChainManager): The chain manager to add the layer to.
                Type: :class:`academic_metrics.ChainBuilder.ChainBuilder.ChainManager`

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            - System prompt: METHOD_EXTRACTION_SYSTEM_MESSAGE
            - Human prompt: HUMAN_MESSAGE_PROMPT
            - Primary parser: JSON with MethodExtractionOutput Pydantic model
            - Fallback parser: String output if JSON parsing fails
            - Output key: "method_json_output"
            - No preprocessor or postprocessor
            - No output key error ignoring
        """
        chain_manager.add_chain_layer(
            system_prompt=METHOD_EXTRACTION_SYSTEM_MESSAGE,
            human_prompt=HUMAN_MESSAGE_PROMPT,
            parser_type="json",
            fallback_parser_type="str",
            pydantic_output_model=MethodExtractionOutput,
            output_passthrough_key_name="method_json_output",
        )
        return self

    def _add_sentence_analysis_layer(self, chain_manager: ChainManager) -> Self:
        """Adds the sentence-by-sentence analysis layer to the chain manager.

        This layer performs detailed analysis of each sentence in the abstract,
        identifying key components like objectives, methods, results, and conclusions.

        Args:
            chain_manager (ChainManager): The chain manager to add the layer to.
                Type: :class:`academic_metrics.ChainBuilder.ChainBuilder.ChainManager`

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            - System prompt: ABSTRACT_SENTENCE_ANALYSIS_SYSTEM_MESSAGE
            - Human prompt: HUMAN_MESSAGE_PROMPT
            - Primary parser: JSON with AbstractSentenceAnalysis Pydantic model
            - Fallback parser: String output if JSON parsing fails
            - Output key: "sentence_analysis_output"
            - No preprocessor or postprocessor
            - No output key error ignoring
        """
        chain_manager.add_chain_layer(
            system_prompt=ABSTRACT_SENTENCE_ANALYSIS_SYSTEM_MESSAGE,
            human_prompt=HUMAN_MESSAGE_PROMPT,
            parser_type="json",
            fallback_parser_type="str",
            pydantic_output_model=AbstractSentenceAnalysis,
            output_passthrough_key_name="sentence_analysis_output",
        )
        return self

    def _add_summary_layer(self, chain_manager: ChainManager) -> Self:
        """Adds the abstract summarization layer to the chain manager.

        This layer generates a concise summary of the abstract, capturing the main
        points and key findings in a structured format.

        Args:
            chain_manager (ChainManager): The chain manager to add the layer to.
                Type: :class:`academic_metrics.ChainBuilder.ChainBuilder.ChainManager`

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            - System prompt: ABSTRACT_SUMMARY_SYSTEM_MESSAGE
            - Human prompt: HUMAN_MESSAGE_PROMPT
            - Primary parser: JSON with AbstractSummary Pydantic model
            - Fallback parser: String output if JSON parsing fails
            - Output key: "abstract_summary_output"
            - No preprocessor or postprocessor
            - No output key error ignoring
        """
        chain_manager.add_chain_layer(
            system_prompt=ABSTRACT_SUMMARY_SYSTEM_MESSAGE,
            human_prompt=HUMAN_MESSAGE_PROMPT,
            parser_type="json",
            fallback_parser_type="str",
            pydantic_output_model=AbstractSummary,
            output_passthrough_key_name="abstract_summary_output",
        )
        return self

    def _add_classification_layer(self, chain_manager: ChainManager) -> Self:
        """Adds the taxonomy classification layer to the chain manager.

        This layer performs hierarchical classification of abstracts according to the
        taxonomy structure, categorizing content at top, mid, and low levels.

        Args:
            chain_manager (ChainManager): The chain manager to add the layer to.
                Type: :class:`academic_metrics.ChainBuilder.ChainBuilder.ChainManager`

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            - System prompt: CLASSIFICATION_SYSTEM_MESSAGE
            - Human prompt: HUMAN_MESSAGE_PROMPT
            - Primary parser: JSON with ClassificationOutput Pydantic model
            - No fallback parser (classification must succeed)
            - Output key: "classification_output"
            - No preprocessor or postprocessor
            - No output key error ignoring
        """
        chain_manager.add_chain_layer(
            system_prompt=CLASSIFICATION_SYSTEM_MESSAGE,
            human_prompt=HUMAN_MESSAGE_PROMPT,
            parser_type="json",
            pydantic_output_model=ClassificationOutput,
            output_passthrough_key_name="classification_output",
        )
        return self

    def _add_theme_recognition_layer(self, chain_manager: ChainManager) -> Self:
        """Adds the theme recognition layer to the chain manager.

        This layer identifies and extracts key themes, concepts, and patterns from
        the abstract, providing a higher-level thematic analysis.

        Args:
            chain_manager (ChainManager): The chain manager to add the layer to.
                Type: :class:`academic_metrics.ChainBuilder.ChainBuilder.ChainManager`

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            - System prompt: THEME_RECOGNITION_SYSTEM_MESSAGE
            - Human prompt: HUMAN_MESSAGE_PROMPT
            - Primary parser: JSON with ThemeAnalysis Pydantic model
            - No fallback parser
            - Output key: "theme_output"
            - No preprocessor or postprocessor
            - No output key error ignoring
            - Uses higher temperature setting for creative theme generation
        """
        chain_manager.add_chain_layer(
            system_prompt=THEME_RECOGNITION_SYSTEM_MESSAGE,
            human_prompt=HUMAN_MESSAGE_PROMPT,
            parser_type="json",
            pydantic_output_model=ThemeAnalysis,
            output_passthrough_key_name="theme_output",
        )
        return self

    def _get_classification_results_by_doi(self, doi: str) -> Dict[str, Any]:
        """Retrieves the raw classification results for a specific DOI.

        This private method provides direct access to the classification results dictionary
        for a given DOI, without theme processing. It's used internally during the
        classification pipeline, particularly before theme recognition processing.

        Args:
            doi (str): The DOI identifier for the abstract to retrieve results for.
                Type: str

        Returns:
            Dict[str, Any]: The raw classification results dictionary containing:
                Type: Dict[str, Any]
                - Top-level categories as keys
                - Nested dictionaries of mid-level categories
                - Lists of low-level categories

        Notes:
            - Returns the raw defaultdict structure
            - Does not include theme information
            - Does not support different return types
            - Used internally during classification pipeline
            - Does not include themes (unlike the public get_classification_results_by_doi)
        """
        return self.classification_results.get(doi, {})

    def get_classification_results_by_doi(
        self, doi: str, return_type: type[dict] | type[tuple] = dict
    ) -> Union[Tuple[str, ...], Dict[str, Any]]:
        """Retrieves all categories and themes for a specific abstract via a DOI lookup.

        This method provides access to the complete classification results for an abstract,
        including all taxonomy levels (top, mid, low) and identified themes. Results can be
        returned either as a dictionary or as a tuple of lists.

        Args:
            doi (str): The DOI identifier for the abstract to retrieve results for.
                Type: str
            return_type (type[dict] | type[tuple]): The desired return type class.
                Type: type[dict] | type[tuple]
                Use dict for dictionary return or tuple for tuple return.
                Defaults to dict.

        Returns:
            Union[Tuple[str, ...], Dict[str, Any]]: The classification results in the requested format:
                Type: Union[Tuple[str, ...], Dict[str, Any]]

                If dict return type:
                    - top_categories (List[str]): Top-level taxonomy categories
                    - mid_categories (List[str]): Mid-level taxonomy categories
                    - low_categories (List[str]): Low-level taxonomy categories
                    - themes (List[str]): Identified themes for the abstract

                If tuple return type:
                    Tuple of (top_categories, mid_categories, low_categories, themes)
                    where each element is a List[str]

        Notes:
            - Categories at each level are returned in order of classification
            - Low-level categories are deduplicated while preserving order
            - Returns empty lists for categories/themes if DOI not found
            - Theme list will be empty if theme recognition hasn't been run
        """
        top_categories: List[str] = []
        mid_categories: List[str] = []
        low_categories: List[str] = []

        abstract_result: Dict[str, Any] = self.classification_results.get(doi, {})

        def extract_categories(result: Dict[str, Any], level: str) -> None:
            """Recursively extracts categories from nested classification
            results."""
            for key, value in result.items():
                if isinstance(value, dict):
                    # Handle top level categories
                    if level == "top":
                        top_categories.append(key)
                        # Recurse into mid level
                        extract_categories(value, "mid")
                elif isinstance(value, list):
                    # Handle mid and low level categories
                    if level == "mid":
                        mid_categories.append(key)
                        # Flatten and extend low categories
                        for item in value:
                            if isinstance(item, list):
                                low_categories.extend(item)
                            else:
                                low_categories.append(item)
                    elif level == "low":
                        # Flatten any nested lists
                        if isinstance(value[0], list):
                            low_categories.extend(value[0])
                        else:
                            low_categories.extend(value)

        # Start extraction from top level
        extract_categories(abstract_result, "top")

        # Remove any duplicates while preserving order
        low_categories: List[str] = list(dict.fromkeys(low_categories))

        result: Dict[str, Any] = {
            "top_categories": top_categories,
            "mid_categories": mid_categories,
            "low_categories": low_categories,
            "themes": abstract_result.get("themes", []),
        }

        return result if return_type is dict else tuple(result.values())

    def classify_abstract(
        self,
        abstract: str,
        doi: str,
        prompt_variables: Dict[str, Any],
        level: str | None = "top",
        parent_category: str | None = None,
        current_dict: Dict[str, Any] | None = None,
    ) -> None:
        """Recursively classifies an abstract through the taxonomy hierarchy.

        This method implements a depth-first traversal of the taxonomy tree, classifying
        the abstract at each level and recursively processing subcategories. It maintains
        state using a nested defaultdict structure that mirrors the taxonomy hierarchy.

        Args:
            abstract (str): The text of the abstract to classify.
                Type: str
            doi (str): The DOI identifier for the abstract.
                Type: str
            prompt_variables (Dict[str, Any]): Variables required for classification.
                Type: Dict[str, Any]
                Pre-classification requirements:
                - method_json_output: Method extraction results
                - sentence_analysis_output: Sentence analysis results
                - abstract_summary_output: Abstract summary
                Classification requirements:
                - abstract: The abstract text
                - categories: Available categories for current level
                - CLASSIFICATION_JSON_FORMAT: Format specification
                - TAXONOMY_EXAMPLE: Example classifications
            level (str | None): Current taxonomy level ("top", "mid", or "low").
                Type: str | None
                Defaults to "top".
            parent_category (str | None): The parent category from previous level.
                Type: str | None
                Defaults to None.
            current_dict (Dict[str, Any] | None): Current position in classification results.
                Type: Dict[str, Any] | None
                Defaults to None.

        Returns:
            None

        Raises:
            ValueError: If classification fails validation after max retries.
            Exception: If any other error occurs during classification.

        Notes:
            - Pre-classification must run method extraction, sentence analysis, and summarization
            - Top level classification processes into top categories then recursively into subcategories
            - Mid level classification processes into mid categories under parent then into low categories
            - Low level classification appends results to parent mid category's list
            - Validates all classified categories against taxonomy
            - Retries classification up to max_classification_retries times
            - On final retry, bans invalid categories to force valid results
        """
        self.logger.info(f"Classifying abstract at {level} level")

        # Start at the top level of our defaultdict if not passed in
        if current_dict is None:
            current_dict = self.classification_results[doi]

        try:
            classification_output: Dict[str, Any] = (
                self.classification_chain_manager.run(
                    prompt_variables_dict=prompt_variables
                ).get("classification_output", {})
            )
            self.logger.debug(f"Raw classification output: {classification_output}")

            # Use **kwargs to unpack the dictionary into keyword arguments for the Pydantic model.
            # '**classification_output' will fill in the values for the keys in the Pydantic model
            # even if there are more keys present in the output which are not part of the pydantic model.
            # This is critical as the outputs here will have all prompt variables from the ones passed to run()
            # as well as the output of the chain layer.
            classification_output: ClassificationOutput = ClassificationOutput(
                **classification_output
            )
            self.raw_classification_outputs.append(classification_output.model_dump())

            # Extract out just the classified categories from the classification output.
            # When the level is top and mid these extracted categories will be used to recursively classify child categories
            # When the level is low these extracted categories will be used to update the current mid category's list of low categories
            classified_categories: List[str] = self.extract_classified_categories(
                classification_output
            )
            self.logger.info(
                f"Classified categories at {level} level: {classified_categories}"
            )

            # Validate categories before proceeding
            retry_count: int = 0
            while not all(
                self.is_valid_category(category, level)
                for category in classified_categories
            ):
                # Find the invalid categories
                invalid_categories: List[str] = [
                    category
                    for category in classified_categories
                    if not self.is_valid_category(category, level)
                ]
                if retry_count >= self.max_classification_retries:
                    raise ValueError(
                        f"Failed to get valid category after {self.max_classification_retries} retries. Invalid categories at {level} level. "
                        f"Invalid categories: {invalid_categories}"
                    )
                self.logger.warning(
                    f"Invalid categories at {level} level, retry {retry_count + 1} "
                    f"Invalid categories: {invalid_categories}"
                )

                # Only set banned words on the final retry.
                # This is done as words may be split into multiple tokens
                # leading to pieces of words being banned rather than the entire word.
                # This could lead to conflict with actual valid categories, and lead
                # the LLM to not classify into categories that it would otherwise.
                # This is done as a last resort to try and elicit valid categories.
                if retry_count == self.max_classification_retries - 1:
                    self.logger.warning("Final retry - attempting with token banning")
                    self.banned_categories.extend(invalid_categories)
                    self.classification_chain_manager.set_words_to_ban(
                        self.banned_categories
                    )

                # Increment retry count
                retry_count += 1

                # Retry classification at this level
                classification_output = self.classification_chain_manager.run(
                    prompt_variables_dict=prompt_variables
                ).get("classification_output", {})

                # Update the classification output with the new output
                classification_output = ClassificationOutput(**classification_output)

                # Update the classified categories with the new output
                classified_categories = self.extract_classified_categories(
                    classification_output
                )

            self.logger.info(
                f"Classified categories at {level} level: {classified_categories}"
            )

            result: Dict[str, Any] = {}

            for category in classified_categories:
                if level == "top":
                    # Get the mid categories for the current top category
                    subcategories: List[str] = self.taxonomy.get_mid_categories(
                        category
                    )

                    # Set the next level to mid so the recursive call will classify the mid categories extracted above
                    next_level: str = "mid"

                    # Move to this category's dictionary in the defaultdict
                    next_dict: Dict[str, Any] = current_dict[category]

                elif level == "mid":
                    # Get the low categories for the current mid category
                    subcategories: List[str] = self.taxonomy.get_low_categories(
                        parent_category, category
                    )

                    # Set the next level to low so the recursive call will classify the low categories extracted above
                    next_level: str = "low"

                    # Move to this category's dictionary in the defaultdict
                    next_dict: Dict[str, Any] = current_dict[category]

                elif level == "low":
                    # Append the low category to the parent (mid) category's list
                    current_dict.append(category)
                    continue

                if subcategories:
                    # Update prompt variables with new subcategories
                    prompt_variables.update(
                        {
                            "categories": subcategories,
                        }
                    )

                    # Recursively classify the subcategories
                    result[category] = self.classify_abstract(
                        abstract=abstract,
                        doi=doi,
                        prompt_variables=prompt_variables,
                        level=next_level,
                        parent_category=category,
                        current_dict=next_dict,
                    )

        except Exception as e:
            self.logger.error(
                f"Error during classification at {level} level:\n"
                f"DOI: {doi}\n"
                f"Current category: {category if 'category' in locals() else 'N/A'}\n"
                f"Parent category: {parent_category}\n"
                f"Exception: {str(e)}\n"
                f"Traceback: {traceback.format_exc()}"
            )
            raise e

    def extract_classified_categories(
        self, classification_output: ClassificationOutput
    ) -> List[str]:
        """Extracts category names from a classification output object.

        Flattens the nested structure of ClassificationOutput into a simple list of
        category names. Handles multiple classifications within the output object.

        Args:
            classification_output (ClassificationOutput): Pydantic model containing classification results.
                Type: :class:`academic_metrics.AI.models.ClassificationOutput`
                Structure:
                {
                    "classifications": [
                        {
                            "categories": ["category1", "category2"],
                            "confidence": 0.95
                        },
                        {
                            "categories": ["category3"],
                            "confidence": 0.85
                        }
                    ]
                }

        Returns:
            List[str]: Flattened list of all classified category names.
                Type: List[str]

        Notes:
            - Extracts categories from all classification entries
            - Maintains the order of categories as they appear
            - Ignores confidence scores in the output
            - Does not deduplicate categories
        """
        self.logger.info("Extracting classified categories")
        categories: List[str] = [
            cat
            for classification in classification_output.classifications
            for cat in classification.categories
        ]
        self.logger.info("Extracted classified categories")
        return categories

    def is_valid_category(self, category: str, level: str) -> bool:
        """Validates if a category exists in the taxonomy at the specified level.

        This method delegates category validation to the taxonomy instance, checking
        whether a given category exists at the specified taxonomy level.

        Args:
            category (str): The category name to validate.
                Type: str
            level (str): The taxonomy level to check against.
                Type: str
                Must be one of: "top", "mid", or "low".

        Returns:
            bool: True if the category exists at the specified level, False otherwise.
                Type: bool

        Notes:
            - Used to validate classified categories before processing
            - Triggers retry logic if invalid categories are found
            - Supports the category banning mechanism on final retries
        """
        return self.taxonomy.is_valid_category(category, level)

    def classify(self) -> Self:
        """Orchestrates the complete classification pipeline for all abstracts.

        This method manages the end-to-end processing of all abstracts present in the
        doi_to_abstract_dict dictionary through three stages: pre-classification,
        classification, and theme recognition.

        Args:
            None

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            Pipeline Stages:
            - Pre-classification:
                - Method extraction: Identifies research methods and techniques
                - Sentence analysis: Analyzes abstract structure and components
                - Summarization: Generates structured abstract summary

            - Classification:
                - Uses enriched data from pre-classification
                - Recursively classifies through taxonomy levels
                - Validates and retries invalid classifications

            - Theme Recognition:
                - Processes classified abstracts
                - Identifies key themes and concepts
                - Uses higher temperature for creative analysis

            State Updates:
            - classification_results: Nested defaultdict structure:
            {
                "doi1": {
                    "top_category1": {
                        "mid_category1": ["low1", "low2"],
                        "mid_category2": ["low3", "low4"]
                    },
                    "themes": ["theme1", "theme2"]
                }
            }
            - raw_classification_outputs: List of raw outputs from classification
            - raw_theme_outputs: Dictionary mapping DOIs to theme analysis results

            Processing Details:
            - Processes abstracts sequentially
            - Requires initialized chain managers
            - Updates multiple result stores
            - Maintains logging throughout process
            - Chains data between processing stages
        """
        # Track total abstracts for progress logging
        n_abstracts: int = len(self.doi_to_abstract_dict.keys())

        # Process each abstract through the complete pipeline
        for i, (doi, abstract) in enumerate(self.doi_to_abstract_dict.items()):
            # Log progress and abstract details for monitoring
            self.logger.info(f"Processing abstract {i+1} of {n_abstracts}")
            self.logger.info(f"Current DOI: {doi}")
            self.logger.info(
                f"Current abstract:\n{abstract[:10]}...{abstract[-10:]}\n\n"
            )

            #######################
            # 1. Pre-classification
            #######################

            # Initialize initial prompt variables used in the system and human prompts for the pre-classification chain layers
            initial_prompt_variables: Dict[str, Any] = {
                "abstract": abstract,
                "METHOD_JSON_FORMAT": METHOD_JSON_FORMAT,
                "METHOD_EXTRACTION_CORRECT_EXAMPLE_JSON": METHOD_EXTRACTION_CORRECT_EXAMPLE_JSON,
                "METHOD_EXTRACTION_INCORRECT_EXAMPLE_JSON": METHOD_EXTRACTION_INCORRECT_EXAMPLE_JSON,
                "SENTENCE_ANALYSIS_JSON_EXAMPLE": SENTENCE_ANALYSIS_JSON_EXAMPLE,
                "SUMMARY_JSON_STRUCTURE": SUMMARY_JSON_STRUCTURE,
                "extra_context": self.extra_context,
            }

            # Execute pre-classification chain (method extraction -> sentence analysis -> summarization)
            self.pre_classification_chain_manager.run(
                prompt_variables_dict=initial_prompt_variables
            )

            # Call this (pre_classification_chain_manager) ChainManager instance's get_chain_variables() method to get the current
            # chain variables which includes all initial_prompt_variables and the outputs of the
            # The new items inserted have a key which matches the layers output_passthrough_key_name value.
            prompt_variables: Dict[str, Any] = (
                self.pre_classification_chain_manager.get_chain_variables()
            )
            method_extraction_output: Dict[str, Any] = prompt_variables.get(
                "method_json_output", {}
            )
            self.logger.debug(f"Method extraction output: {method_extraction_output}")
            sentence_analysis_output: Dict[str, Any] = prompt_variables.get(
                "sentence_analysis_output", {}
            )
            self.logger.debug(f"Sentence analysis output: {sentence_analysis_output}")
            summary_output: Dict[str, Any] = prompt_variables.get(
                "abstract_summary_output", {}
            )
            self.logger.debug(f"Summary output: {summary_output}")

            ######################
            # 2. Classification
            ######################

            # Update the prompt variables by adding classification-specific variables.
            # Start with top-level categories - recursive classification will handle lower levels.
            prompt_variables.update(
                {
                    "categories": self.taxonomy.get_top_categories(),
                    "CLASSIFICATION_JSON_FORMAT": CLASSIFICATION_JSON_FORMAT,
                    "TAXONOMY_EXAMPLE": TAXONOMY_EXAMPLE,
                }
            )

            # Execute recursive classification through taxonomy levels
            self.classify_abstract(
                abstract=abstract,
                doi=doi,
                prompt_variables=prompt_variables,
            )

            ######################
            # 3. Theme Recognition
            ######################

            # Get updated variables after classification.
            # Details:
            #   Once classify_abstract returns it will have classified the abstract into top categories
            #   then recursively classified mid and low level categories within each classified top category
            #   so now this abstract has been classified into all relevant categories and subcategories within the taxonomy.
            #   Given this, we can now process the themes for this abstract.
            #   Like before fetch this ChainManager (classification_chain_manager this time) instance's chain variables and update them:
            prompt_variables: Dict[str, Any] = (
                self.classification_chain_manager.get_chain_variables()
            )

            # Add in the theme recognition specific variables
            # The only one not already present in prompt_variables which is present as a placeholder
            # in the theme_recognition_system_prompt is THEME_RECOGNITION_JSON_FORMAT, so we add that in.
            # Then update the categories key with the categories from the classification results.
            prompt_variables.update(
                {
                    "THEME_RECOGNITION_JSON_FORMAT": THEME_RECOGNITION_JSON_FORMAT,
                    "categories": self._get_classification_results_by_doi(doi),
                }
            )

            # Execute theme recognition on the current abstract.
            # Details:
            #   Here we actually store the result as we want to want to store this raw output into the raw theme outputs dictionary
            #   We don't need to pull out prompt_variables again as we can just extract the themes directly out the theme_results
            #   Remember, before we had to pull out the prompt_variables as we needed all variables to propagate through to the
            #   future chains which weren't the same ChainManager instance.
            theme_results: Dict[str, Any] = self.theme_chain_manager.run(
                prompt_variables_dict=prompt_variables
            ).get("theme_output", {})

            theme_results = ThemeAnalysis(**theme_results)

            # Store raw theme results
            self.raw_theme_outputs[doi] = theme_results.model_dump()

            # Update final classification results with themes
            # Details:
            #   Done in an if statement to avoid killing the live service if this happens, though it shouldn't,
            #   or at least a more explicit and detailed error should be thrown much earlier.
            #   Due to the context here not much detail is known, so throwing an error isn't particularly helpful.
            if doi in self.classification_results:
                self.classification_results[doi]["themes"] = theme_results.themes
            else:
                # Log error if DOI missing from results (as mentioned before, this shouldn't happen in normal operation, but just in case)
                self.logger.error(
                    f"DOI not found in classification results: {doi}, class results: {self.classification_results}"
                )

        return self

    def _make_dirs_helper(self, output_path: str) -> None:
        """Creates necessary directories for an output file path.

        This private helper method ensures that all directories in the path exist,
        creating them if necessary. Used by save methods before writing files.

        Args:
            output_path (str): The full path where a file will be saved.
                Type: str
                Can be either absolute or relative path.

        Notes:
            - Creates directories recursively
            - Uses exist_ok=True to handle existing directories
            - Creates parent directories only (not the file itself)
        """
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

    def save_classification_results(self, output_path: str) -> Self:
        """Saves processed classification results to a JSON file.

        Writes the complete classification results dictionary to a JSON file,
        creating any necessary directories in the process. The output includes
        all categories and themes for all processed abstracts.

        Args:
            output_path (str): Path where the JSON file should be saved.
                Type: str
                Can be absolute or relative path.

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            Output Format:
            {
                "doi1": {
                    "top_category1": {
                        "mid_category1": ["low1", "low2"],
                        "mid_category2": ["low3", "low4"]
                    },
                    "themes": ["theme1", "theme2"]
                }
            }
        """
        self.logger.info("Saving classification results")
        self._make_dirs_helper(output_path)
        with open(output_path, "w") as f:
            json.dump(self.classification_results, f, indent=4)
        return self

    def get_classification_results_dict(self) -> Dict[str, Dict[str, Any]]:
        """Retrieves processed classification results for all processed abstracts.

        Provides direct access to the complete classification results dictionary,
        containing all categories and themes for every processed abstract.

        Returns:
            Dict[str, Dict[str, Any]]: A dictionary where:
                Type: Dict[str, Dict[str, Any]]
                - Keys are DOI strings
                - Values are nested dictionaries containing:
                {
                    "top_category1": {
                        "mid_category1": ["low1", "low2"],
                        "mid_category2": ["low3", "low4"]
                    },
                    "themes": ["theme1", "theme2"]
                }

        Notes:
            - Returns the raw defaultdict structure
            - Includes theme information if theme recognition was run
            - Structure matches the save_classification_results output format
        """
        self.logger.info("Getting classification results")
        return self.classification_results

    def get_raw_classification_outputs(self) -> List[Dict[str, Any]]:
        """Retrieves raw classification outputs from all processed abstracts.

        Provides access to the complete, unprocessed outputs from the classification
        chain, including all prompt variables and intermediate results.

        Returns:
            List[Dict[str, Any]]: List of raw classification outputs, where each output contains:
                Type: List[Dict[str, Any]]
                - classifications: List of classifications with categories and confidence scores
                - abstract: The original abstract text
                - method_json_output: Output from method extraction
                - sentence_analysis_output: Output from sentence analysis
                - abstract_summary_output: Output from abstract summarization
                - Other chain variables and outputs

        Notes:
            - Contains all chain variables and outputs
            - Includes pre-classification results
            - Useful for debugging and analysis
            - May contain large amounts of data
        """
        self.logger.info("Getting raw classification outputs")
        return self.raw_classification_outputs

    def get_raw_theme_results(self) -> Dict[str, Dict[str, Any]]:
        """Retrieves raw theme analysis results for all processed abstracts.

        Provides access to the complete, unprocessed outputs from the theme recognition
        chain for each abstract.

        Args:
            None

        Returns:
            Dict[str, Dict[str, Any]]: Dictionary where:
                Type: Dict[str, Dict[str, Any]]
                - Keys are DOI strings
                - Values are raw theme analysis results with structure:
                {
                    "themes": ["theme1", "theme2"],
                    "confidence_scores": {
                        "theme1": 0.95,
                        "theme2": 0.85
                    },
                    "analysis": "Theme analysis text...",
                    # Other theme recognition outputs
                }

        Notes:
            - Contains complete theme recognition outputs
            - Includes confidence scores and analysis text
            - Available after theme recognition stage
            - Empty dictionaries for unprocessed DOIs
        """
        self.logger.info("Getting raw theme results")
        return self.raw_theme_outputs

    def save_raw_classification_results(self, output_path: str) -> Self:
        """Saves raw classification outputs to a JSON file.

        Writes the complete, unprocessed outputs from the classification chain to a JSON file,
        creating any necessary directories in the process. Includes all prompt variables and
        intermediate results.

        Args:
            output_path (str): Path where the JSON file should be saved.
                Type: str
                Can be absolute or relative path.

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            Output Format:
            [
                {
                    "classifications": [
                        {
                            "categories": ["category1", "category2"],
                            "confidence": 0.95
                        }
                    ],
                    "abstract": "original abstract text",
                    "method_json_output": {...},
                    "sentence_analysis_output": {...},
                    "abstract_summary_output": {...},
                    # Other chain variables and outputs
                },
                # Additional classification outputs...
            ]
        """
        self.logger.info("Saving raw classification results")
        self._make_dirs_helper(output_path)
        with open(output_path, "w") as f:
            json.dump(self.raw_classification_outputs, f, indent=4)
        return self

    def save_raw_theme_results(self, output_path: str) -> Self:
        """Saves raw theme analysis results to a JSON file.

        Writes the complete, unprocessed outputs from the theme recognition chain to a JSON file,
        creating any necessary directories in the process. Includes theme analysis results for
        each processed abstract.

        Args:
            output_path (str): Path where the JSON file should be saved.
                Type: str
                Can be absolute or relative path.

        Returns:
            Self: Returns self for method chaining.
                Type: :class:`academic_metrics.AI.AbstractClassifier.AbstractClassifier`

        Notes:
            Output Format:
            {
                "10.1234/example": {
                    "themes": ["theme1", "theme2"],
                    "confidence_scores": {
                        "theme1": 0.95,
                        "theme2": 0.85
                    },
                    "analysis": "Theme analysis text...",
                    # Other theme recognition outputs
                },
                # Additional DOIs and their theme results...
            }
        """
        self.logger.info("Saving raw theme results")
        self._make_dirs_helper(output_path)
        with open(output_path, "w") as f:
            json.dump(self.raw_theme_outputs, f, indent=4)
        return self


if __name__ == "__main__":
    """Simple demonstration of AbstractClassifier functionality.

    This script shows basic usage of the AbstractClassifier class by:
    1. Loading environment variables for API key
    2. Creating a sample abstract
    3. Initializing and running the classifier
    4. Saving results to files
    """
    from academic_metrics.utils.taxonomy_util import Taxonomy
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found in environment variables")

    # Create sample data
    sample_abstract = {
        "10.1234/example": (
            "This paper presents a novel machine learning approach for natural "
            "language processing. We introduce a new neural network architecture "
            "that combines transformer models with reinforcement learning. Our "
            "results show significant improvements in language understanding tasks."
        )
    }

    extra_context = {
        "keywords": [
            "machine learning",
            "natural language processing",
            "neural networks",
            "reinforcement learning",
        ]
    }

    # Initialize classifier
    taxonomy = Taxonomy()
    classifier = AbstractClassifier(
        taxonomy=taxonomy,
        doi_to_abstract_dict=sample_abstract,
        api_key=api_key,
        extra_context=extra_context,
    )

    # Run classification and save results
    try:
        classifier.classify().save_classification_results(
            "outputs/classification_results.json"
        ).save_raw_theme_results(
            "outputs/theme_results.json"
        ).save_raw_classification_results(
            "outputs/raw_classification_outputs.json"
        )
        print("Classification completed successfully. Results saved to outputs/")
    except Exception as e:
        print(f"Error during classification: {str(e)}")

            ```

            src/academic_metrics/ChainBuilder/ChainBuilder.py:
            ```
from __future__ import annotations

import json
import logging
import os
import warnings
from pathlib import Path
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    Tuple,
    TypeVar,
    Union,
    cast,
    TypeAlias,
)

from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain.schema.runnable import Runnable, RunnablePassthrough
from langchain_anthropic import ChatAnthropic
from langchain_core.output_parsers import (
    JsonOutputParser,
    PydanticOutputParser,
    StrOutputParser,
)
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, ValidationError

import tiktoken

# ! THIS NEEDS TO BE REMOVED WHEN THIS IS MADE A STANDALONE PACKAGE
from academic_metrics.configs import (
    configure_logging,
    LOG_TO_CONSOLE,
    DEBUG,
    INFO,
    WARNING,
    ERROR,
    CRITICAL,
)

FirstCallRequired = TypeVar("FirsCallRequired", bound=Dict[str, Any])
"""TypeVar representing a dictionary that is required on first call but optional after.

Type Structure:
    TypeVar bound to Dict[str, Any]

Usage:
    Used to enforce that a dictionary parameter must be provided on first call
    but can be optional on subsequent calls.
"""

ParserUnion: TypeAlias = Union[PydanticOutputParser, JsonOutputParser, StrOutputParser]
"""Union type representing valid parser types.

Type Structure:
    Union[PydanticOutputParser, JsonOutputParser, StrOutputParser]
"""

ParserType = TypeVar("ParserType", bound=ParserUnion)
"""TypeVar representing the main parser type.

Type Structure:
    TypeVar bound to Union[PydanticOutputParser, JsonOutputParser, StrOutputParser]

Usage:
    Used to enforce type consistency for the primary parser in chain operations.
"""

FallbackParserType = TypeVar("FallbackParserType", bound=ParserUnion)
"""TypeVar representing the fallback parser type.

Type Structure:
    TypeVar bound to Union[PydanticOutputParser, JsonOutputParser, StrOutputParser]

Usage:
    Used to enforce type consistency for the fallback parser in chain operations.
"""


class ChainBuilder:
    """A builder class for constructing and configuring LangChain chains with logging and parsing capabilities.

    This class provides functionality to construct LangChain chains with customizable prompts,
    language models, and output parsers. It includes support for logging, fallback chains,
    and various parser types.

    Attributes:
        log_file_path (Path): Path to the log file.
        logger (Logger): Logger instance for the chain builder.
        chat_prompt (ChatPromptTemplate): Template for chat prompts.
        parser (ParserType | None): Primary output parser.
        fallback_parser (ParserType | None): Fallback output parser.
        llm (Union[:class:`ChatOpenAI`, :class:`ChatAnthropic`, :class:`ChatGoogleGenerativeAI`]): Language model instance.
        chain (:class:`Runnable`): Primary chain instance.
        fallback_chain (:class:`Runnable`): Fallback chain instance.

    Methods:
        get_chain() -> :class:`Runnable`:
            Returns the primary chain instance.

        get_fallback_chain() -> :class:`Runnable`:
            Returns the fallback chain instance.

        __str__() -> str:
            Returns a string representation of the ChainBuilder.

        __repr__() -> str:
            Returns a string representation of the ChainBuilder.

        _run_pydantic_parser_logging() -> None:
            Logs Pydantic parser configuration.

        _build_chain() -> None:
            Constructs the primary chain.

        _build_fallback_chain() -> None:
            Constructs the fallback chain.
    """

    def __init__(
        self,
        *,
        chat_prompt: ChatPromptTemplate,
        llm: Union[ChatOpenAI, ChatAnthropic, ChatGoogleGenerativeAI],
        parser: ParserType | None = None,
        fallback_parser: FallbackParserType | None = None,
        logger: logging.Logger | None = None,
        log_to_console: bool | None = LOG_TO_CONSOLE,
    ) -> None:
        """Initialize a ChainBuilder instance.

        Args:
            chat_prompt (:class:`ChatPromptTemplate`): Template for structuring chat interactions.
            llm (:class:`ChatOpenAI`, :class:`ChatAnthropic`, :class:`ChatGoogleGenerativeAI`): Language model to use.
            parser (ParserType | None): Primary output parser.
                Defaults to None.
            fallback_parser (FallbackParserType | None): Fallback output parser.
                Defaults to None.
            logger (:class:`~python:logging.Logger` | None): Custom logger instance.
                Defaults to None.
            log_to_console (bool | None): Whether to log to console.
                Defaults to LOG_TO_CONSOLE.

        Returns:
            None
        """
        self.logger = logger or configure_logging(
            module_name=__name__,
            log_file_name="chain_builder",
            log_level=DEBUG,
        )

        self.chat_prompt: ChatPromptTemplate = chat_prompt
        self.parser: Optional[ParserType] = parser
        self.fallback_parser: Optional[FallbackParserType] = fallback_parser
        self.llm: Union[ChatOpenAI, ChatAnthropic, ChatGoogleGenerativeAI] = llm
        self.chain: Runnable = self._build_chain()
        self.fallback_chain: Runnable = self._build_fallback_chain()

    def __str__(self) -> str:
        """
        Returns a string representation of the ChainBuilder object.

        The string includes the class names of the chat_prompt, llm, and parser attributes.

        Returns:
            str: A string representation of the ChainBuilder object.
        """
        return f"ChainBuilder(chat_prompt={type(self.chat_prompt).__name__}, llm={self.llm.__class__.__name__}, parser={type(self.parser).__name__ if self.parser else 'None'})"

    def __repr__(self) -> str:
        """
        Returns a string representation of the object for debugging purposes.

        This method calls the __str__() method to provide a human-readable
        representation of the object.

        Returns:
            str: A string representation of the object.
        """
        return self.__str__()

    def _run_pydantic_parser_logging(self) -> None:
        """
        Logs the required fields and their default values (if any) for a Pydantic model
        if the parser is an instance of PydanticOutputParser. Additionally, logs the
        JSON schema of the Pydantic model.

        This method performs the following steps:
        1. Checks if the parser is an instance of PydanticOutputParser.
        2. Retrieves the Pydantic model from the parser.
        3. Logs each field's name and whether it is required or optional.
        4. Logs the default value of each field if it is not None.
        5. Logs the JSON schema of the Pydantic model.

        Returns:
            None
        """
        if isinstance(self.parser, PydanticOutputParser):
            pydantic_model: BaseModel = self.parser.pydantic_object
            self.logger.info("Required fields in Pydantic model:")
            for field_name, field in pydantic_model.model_fields.items():
                self.logger.info(
                    f"  {field_name}: {'required' if field.is_required else 'optional'}"
                )
                if field.default is not None:
                    self.logger.info(f"    Default: {field.default}")

            self.logger.info("\nModel Schema:")
            self.logger.info(pydantic_model.model_json_schema())

    def _build_chain(self) -> Runnable:
        """
        Builds and returns a chain of Runnable objects.

        The chain is constructed by combining a RunnablePassthrough object with
        the chat_prompt and llm attributes. If a parser is provided, it is added
        to the chain and pydantic parser logging is executed.

        Returns:
            :class:`Runnable`: The constructed chain of Runnable objects.
        """
        # Build the chain
        chain: Runnable = RunnablePassthrough() | self.chat_prompt | self.llm

        if self.parser:
            self._run_pydantic_parser_logging()
            chain: Runnable = chain | self.parser

        return chain

    def _build_fallback_chain(self) -> Runnable:
        """
        Builds the fallback chain for the Runnable.

        This method constructs a fallback chain by combining a RunnablePassthrough
        instance with the chat prompt and language model (llm). If a fallback parser
        is provided, it adds the parser to the chain and logs the parser usage.

        Returns:
            :class:`Runnable`: The constructed fallback chain.
        """
        fallback_chain: Runnable = RunnablePassthrough() | self.chat_prompt | self.llm

        if self.fallback_parser:
            self._run_pydantic_parser_logging()
            fallback_chain: Runnable = fallback_chain | self.fallback_parser

        return fallback_chain

    def get_chain(self) -> Runnable:
        """
        Retrieves the current chain.

        Returns:
            :class:`Runnable`: The current chain instance.
        """
        return self.chain

    def get_fallback_chain(self) -> Runnable:
        """Retrieve the fallback chain.

        Returns:
            :class:`Runnable`: The fallback chain instance.
        """
        return self.fallback_chain


class ChainWrapper:
    """A wrapper class for managing and executing chains with optional fallback chains.

    This class is designed to handle the execution of primary chains and, if necessary,
    fallback chains in case of errors or unexpected outputs. It supports preprocessing
    and postprocessing of input and output data, as well as logging for debugging and
    monitoring purposes.

    Attributes:
        chain (Runnable): The primary chain to be executed.
        fallback_chain (Runnable): The fallback chain to be executed if the primary chain fails.
        parser (ParserType | None): The parser to be used for processing the output of the
            primary chain.
            Defaults to None.
        fallback_parser (FallbackParserType | None): The parser to be used for processing the output
            of the fallback chain.
            Defaults to None.
        preprocessor (:class:`~python:typing.Callable` | None): A function to preprocess input data before passing
            it to the chain.
            Defaults to None.
        postprocessor (:class:`~python:typing.Callable` | None): A function to postprocess the output data from the chain.
            Defaults to None.
        logger (:class:`~logging.Logger` | None): A logger instance for logging information and debugging.
            Defaults to None.

    Methods:
        __init__(chain, fallback_chain, parser=None, fallback_parser=None,
            preprocessor=None, postprocessor=None, logger=None):
                Initializes the ChainWrapper instance with the provided parameters.

        __str__() -> str:
            Returns a string representation of the ChainWrapper object, including the chain,
            parser type, and fallback parser type.

        __repr__() -> str:
            Returns a string representation of the ChainWrapper object for debugging purposes.

        run_chain(input_data=None, is_last_chain=False) -> Any:
            Executes the primary chain with the provided input data. If an error occurs,
            attempts to execute the fallback chain. Supports preprocessing and postprocessing.

            Args:
                input_data (dict | None): The input data for the chain
                    Type: Dict[str, Any]
                is_last_chain (bool): Whether this is the last chain in sequence
                    Defaults to False

        get_parser_type() -> str | None:
            Returns the type of the parser as a string if the parser exists, otherwise None.

        get_fallback_parser_type() -> str | None:
            Returns the type of the fallback parser as a string if it exists, otherwise None.
    """

    def __init__(
        self,
        *,
        chain: Runnable,
        fallback_chain: Runnable,
        parser: ParserType | None = None,
        fallback_parser: FallbackParserType | None = None,
        preprocessor: Callable[[Dict[str, Any]], Dict[str, Any]] | None = None,
        postprocessor: Callable[[Any], Any] | None = None,
        logger: logging.Logger | None = None,
    ) -> None:
        """Initialize the ChainBuilder.

        Args:
            chain (:class:`Runnable`): The primary chain to be executed.
            fallback_chain (:class:`Runnable`): The fallback chain to be executed if the primary chain fails.
            parser (ParserType | None): The parser to be used for the primary chain.
            fallback_parser (FallbackParserType | None): The parser to be used for the fallback chain.
            preprocessor (:class:`~python:typing.Callable` | None): A function to preprocess input data before passing it to the chain.
                Defaults to None.
            postprocessor (:class:`~python:typing.Callable` | None): A function to postprocess the output data from the chain.
                Defaults to None.
            logger (:class:`~logging.Logger` | None): A logger instance for logging information.
                Defaults to None.

        """
        self.logger = logger or configure_logging(
            module_name=__name__,
            log_file_name="chain_wrapper",
            log_level=DEBUG,
        )

        self.parser: Optional[ParserType] = parser
        self.fallback_parser: Optional[FallbackParserType] = fallback_parser
        self.preprocessor: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = (
            preprocessor
        )

        self.chain: Runnable = chain
        self.fallback_chain: Runnable = fallback_chain
        self.postprocessor: Optional[Callable[[Any], Any]] = postprocessor

    def __str__(self) -> str:
        """
        Returns a string representation of the ChainWrapper object.

        The string includes the chain, the type of the parser, and the type of the fallback parser.

        Returns:
            str: A string representation of the ChainWrapper object.
        """
        return f"ChainWrapper(chain={self.chain}, parser={type(self.parser).__name__ if self.parser else 'None'}, fallback_parser={type(self.fallback_parser).__name__ if self.fallback_parser else 'None'})"

    def __repr__(self) -> str:
        """
        Returns a string representation of the object for debugging purposes.

        This method calls the __str__() method to provide a user-friendly string
        representation of the object. It is intended to be used for debugging
        and development, rather than for end-user display.

        Returns:
            str: A string representation of the object.
        """
        return self.__str__()

    def run_chain(
        self,
        *,
        input_data: Dict[str, Any] | None = None,
        is_last_chain: bool | None = False,
    ) -> Any:
        """Executes the primary chain with the provided input data.

        If an error occurs in the primary chain, and a fallback chain is provided,
        it attempts to execute the fallback chain.

        Args:
            input_data (dict | None): The input data required to run the chain.
                Type: Dict[str, Any]
                Defaults to None.
            is_last_chain (bool | None): Indicates if this is the last chain in the sequence.
                Defaults to False.

        Returns:
            :class:`~python:typing.Any`: The output from the chain execution, potentially processed by a postprocessor.

        Raises:
            ValueError: If no input data is provided.
            json.JSONDecodeError: If a JSON decode error occurs in both the main and fallback chains.
            :class:`~pydantic.ValidationError`: If a validation error occurs in both the main and fallback chains.
            TypeError: If a type error occurs in both the main and fallback chains.

        Notes:
            - If the output is a Pydantic model and this is not the last chain,
              the model is converted to a dictionary.
            - If a postprocessor is defined, it processes the output before returning.
        """
        if cast(Any, input_data) is None:
            raise ValueError(
                "No input data provided to ChainWrapper.run_chain(). Input data is required to run the chain."
            )

        if self.preprocessor:
            input_data = self.preprocessor(input_data)
        output = None

        try:
            # Attempt to invoke the primary chain
            output: Any = self.chain.invoke(input_data)
        except (
            json.JSONDecodeError,
            ValidationError,
            ValueError,
            TypeError,
        ) as main_chain_exception:
            self.logger.info(f"Error in main chain: {main_chain_exception}")
            self.logger.info("Attempting to execute fallback chain")
            if self.fallback_chain and not is_last_chain:
                self.logger.info("Fallback chain provided, executing fallback chain")
                try:
                    # Attempt to invoke the fallback chain
                    output: Any = self.fallback_chain.invoke(input_data)
                except (
                    json.JSONDecodeError,
                    ValidationError,
                    ValueError,
                    TypeError,
                ) as fallback_exception:
                    if isinstance(fallback_exception, json.JSONDecodeError):
                        self.logger.debug(
                            f"Error in fallback chain: {fallback_exception.errors()}"
                        )
                        self.logger.debug(
                            "A json decode error occurred in the main chain. Executing fallback chain. If you didn't provide a fallback chain it will not be ran, since it was a json decode error you should check how you're telling the LLM to output the json and make sure it matches your pydantic model. Additionally, LLMs will sometimes output invalid characters to json such as Latex code, leading to invalide escape sequences or characters. If this is the case you should try StrOutputParser() if it isn't your last chain layer. If it is your last chain layer, try simplifying your pydantic model and enhancing your prompt to avoid this issue. If you are using a LLM such as 'gpt-4o-mini' or 'claude-3.5-haiku' you may see better results with a larger model such as 'gpt-4o' or 'claude-3.5-sonnet'."
                        )
                        raise fallback_exception
            else:
                if isinstance(main_chain_exception, json.JSONDecodeError):
                    self.logger.debug(
                        "A json decode error occurred in the main chain. Executing fallback chain. If you didn't provide a fallback chain it will not be ran, since it was a json decode error you should check how you're telling the LLM to output the json and make sure it matches your pydantic model. Additionally, LLMs will sometimes output invalid characters to json such as Latex code, leading to invalide escape sequences or characters. If this is the case you should try StrOutputParser() if it isn't your last chain layer. If it is your last chain layer, try simplifying your pydantic model and enhancing your prompt to avoid this issue. If you are using a LLM such as 'gpt-4o-mini' or 'claude-3.5-haiku' you may see better results with a larger model such as 'gpt-4o' or 'claude-3.5-sonnet'."
                    )
                    raise main_chain_exception

        # Only need to handle intermediate chain Pydantic outputs
        # Rest are handled by JsonOutputParser or PydanticOutputParser
        if not is_last_chain and isinstance(output, BaseModel):
            return output.model_dump()

        if self.postprocessor:
            output: Any = self.postprocessor(output)

        return output

    def get_parser_type(self) -> str | None:
        """Returns the type of the parser as a string if the parser exists, otherwise returns None.

        Returns:
            str | None: The type of the parser as a string, or None if the parser
                does not exist.
        """
        return type(self.parser).__name__ if self.parser else None

    def get_fallback_parser_type(self) -> str | None:
        """Returns the type name of the fallback parser if it exists, otherwise returns None.

        Returns:
            str | None: The type name of the fallback parser as a string if it exists,
                otherwise None.
        """
        return type(self.fallback_parser).__name__ if self.fallback_parser else None


class ChainComposer:
    """Manages a sequence of chain operations.

    ChainComposer is a class that manages a sequence of chain operations, allowing for
    the addition of chains and running them in sequence with provided data.

    Methods:
        __init__(logger: Logger | None):
            Initializes the ChainComposer instance with an optional logger.
            Type: :class:`~logging.Logger` | None
            Defaults to None.

        __str__() -> str:
            Returns a string representation of the ChainComposer instance.

        __repr__() -> str:
            Returns a string representation of the ChainComposer instance.

        add_chain(chain_wrapper, output_passthrough_key_name):
            Adds a chain to the chain sequence.

            Args:
                chain_wrapper (ChainWrapper): The chain wrapper to add
                output_passthrough_key_name (str | None): Optional output key name
                    Defaults to None.

        run(data_dict, data_dict_update_function):
            Runs the chain sequence with the provided data dictionary and optional update function.

            Args:
                data_dict (dict): The input data dictionary
                    Type: Dict[str, Any]
                data_dict_update_function (callable | None): Optional update function
                    Type: Callable[[Dict[str, Any]], None]
                    Defaults to None.
            Returns:
                dict: The updated data dictionary
                    Type: Dict[str, Any]
    """

    def __init__(self, logger: logging.Logger | None = None) -> None:
        """Initializes the ChainBuilder instance.

        Args:
            logger (Logger | None): A logger instance to be used for logging.
                Type: logging.Logger | None
                If not provided, a default logger will be created.
                Defaults to None.

        Attributes:
            logger (Logger): The logger instance used for logging.
                Type: logging.Logger
            chain_sequence (list): A list to store the chain sequence.
                Type: List[Tuple[:class:`~academic_metrics.ChainBuilder.ChainBuilder.ChainWrapper`, str | None]]
        """
        self.logger = logger or configure_logging(
            module_name=__name__,
            log_file_name="chain_composer",
            log_level=DEBUG,
        )

        self.chain_sequence: List[Tuple[ChainWrapper, Optional[str]]] = []

    def __str__(self) -> str:
        """Returns a string representation of the ChainComposer object.

        The string representation includes the index and the wrapper of each
        element in the chain_sequence.

        Returns:
            str: A string representation of the ChainComposer object.
        """
        chain_info = ", ".join(
            [
                f"{idx}: {wrapper}"
                for idx, (wrapper, _) in enumerate(self.chain_sequence)
            ]
        )
        return f"ChainComposer(chains=[{chain_info}])"

    def __repr__(self) -> str:
        """Returns a string representation of the object for debugging purposes.

        This method calls the __str__() method to provide a human-readable
        representation of the object.

        Returns:
            str: A string representation of the object.
        """
        return self.__str__()

    def add_chain(
        self,
        *,
        chain_wrapper: ChainWrapper,
        output_passthrough_key_name: str | None = None,
    ) -> None:
        """Adds a chain to the chain sequence.

        Args:
            chain_wrapper (ChainWrapper): The chain wrapper to be added.
            output_passthrough_key_name (str | None): The key name for output passthrough.
                Type: str | None
                Defaults to None.

        Returns:
            None
        """
        self.chain_sequence.append((chain_wrapper, output_passthrough_key_name))

    def run(
        self,
        *,
        data_dict: Dict[str, Any],
        data_dict_update_function: Callable[[Dict[str, Any]], None] | None = None,
    ) -> Dict[str, Any]:
        """Executes a sequence of chains, updating the provided data dictionary with the results of each chain.

        Args:
            data_dict (dict): The initial data dictionary containing input variables for the chains.
                Type: Dict[str, Any]
            data_dict_update_function (callable | None): An optional function to update the data
                dictionary after each chain execution.
                Type: Callable[[Dict[str, Any]], None]
                Defaults to None.

        Returns:
            dict: The updated data dictionary after all chains have been executed.
                Type: Dict[str, Any]

        Raises:
            UserWarning: If the provided data dictionary is empty.

        Notes:
            - The method iterates over a sequence of chains (`self.chain_sequence`), executing
              each chain and updating the `data_dict` with the output.
            - If an `output_name` is provided for a chain, the output is stored in `data_dict`
              under that name; otherwise, it is stored under the key `_last_output`.
            - If `data_dict_update_function` is provided, it is called with the updated
              `data_dict` after each chain execution.
        """
        if not data_dict:
            warnings.warn(
                "No variables provided for the chain. Please ensure you have provided the necessary variables. If you have variable placeholders and do not pass them in it will result in an error."
            )

        num_chains: int = len(self.chain_sequence)
        for index, (chain_wrapper, output_name) in enumerate(self.chain_sequence):
            is_last_chain: bool = index == num_chains - 1
            output: Union[BaseModel, Dict[str, Any], str] = chain_wrapper.run_chain(
                input_data=data_dict, is_last_chain=is_last_chain
            )

            # Update data with the output
            if output_name:
                data_dict[output_name] = output
            else:
                data_dict["_last_output"] = output

            if data_dict_update_function:
                data_dict_update_function(data_dict)

        return data_dict


class ChainManager:
    """Class responsible for managing and orchestrating a sequence of chain layers,

    ChainManager is a class responsible for managing and orchestrating a sequence of chain layers,
    each of which can process input data and produce output data. It supports various types of
    language models (LLMs) and parsers, and allows for pre-processing and post-processing of data.

    Attributes:
        api_key (str): The API key for accessing the LLM service.
        llm_model (str): The name of the LLM model to use.
        llm_model_type (str): The type of the LLM model (e.g., "openai", "anthropic", "google").
            Type: Literal["openai", "anthropic", "google"]
        llm_temperature (float): The temperature setting for the LLM model.
        llm_kwargs (dict): Additional keyword arguments for the LLM model.
            Type: Dict[str, Any]
        llm (Union[:class:`ChatOpenAI`, :class:`ChatAnthropic`, :class:`ChatGoogleGenerativeAI`]): The initialized LLM instance.
        chain_composer (ChainComposer): The composer for managing the sequence of chain layers.
        chain_variables (dict): A dictionary of variables used in the chain layers.
            Type: Dict[str, Any]
        chain_variables_update_overwrite_warning_counter (int): Counter for tracking variable overwrite warnings.
        preprocessor (:class:`~python:typing.Callable`, optional): Optional preprocessor function.
        postprocessor (:class:`~python:typing.Callable`, optional): Optional postprocessor function.
        logger (:class:`~python:logging.Logger`): Logger for logging information and debugging.

    Methods:
        __str__(): Returns a string representation of the ChainManager instance.
        __repr__(): Returns a string representation of the ChainManager instance.
        _get_llm_model_type(llm_model: str) -> str:
            Determines the type of the LLM model based on its name.
        _initialize_llm(api_key: str, llm_model_type: str, llm_model: str, llm_temperature: float, llm_kwargs: Dict[str, Any]) -> Union[:class:`ChatOpenAI`, :class:`ChatAnthropic`, :class:`ChatGoogleGenerativeAI`]:
            Initializes the LLM instance based on the model type.
        _create_openai_llm(api_key: str, llm_model: str, llm_temperature: float, llm_kwargs: Dict[str, Any]) -> :class:`ChatOpenAI`:
            Creates an OpenAI LLM instance.
        _create_anthropic_llm(api_key: str, llm_model: str, llm_temperature: float, llm_kwargs: Dict[str, Any]) -> :class:`ChatAnthropic`:
            Creates an Anthropic LLM instance.
        _create_google_llm(api_key: str, llm_model: str, llm_temperature: float, llm_kwargs: Dict[str, Any]) -> :class:`ChatGoogleGenerativeAI`:
            Creates a Google Generative AI LLM instance.
        _initialize_parser(parser_type: str, pydantic_output_model: Optional[BaseModel] = None) -> ParserUnion:
            Initializes a parser based on the specified type.
        _create_pydantic_parser(pydantic_output_model: :class:`pydantic.BaseModel`) -> :class:`~langchain_core.output_parsers.PydanticOutputParser`:
            Creates a Pydantic parser.
        _create_json_parser(pydantic_output_model: Optional[BaseModel]) -> :class:`~langchain_core.output_parsers.JsonOutputParser`:
        ) -> JsonOutputParser:
            Creates a JSON parser.
        _create_str_parser() -> :class:`~langchain_core.output_parsers.StrOutputParser`:
            Creates a string parser.
        _run_chain_validation_checks(output_passthrough_key_name: str | None, ignore_output_passthrough_key_name_error: bool, parser_type: Literal["pydantic", "json", "str"] | None, pydantic_output_model: :class:`~pydantic.BaseModel` | None, fallback_parser_type: Literal["pydantic", "json", "str"] | None, fallback_pydantic_output_model: :class:`~pydantic.BaseModel` | None) -> None:
            Runs validation checks for the chain configuration.
        _format_chain_sequence(chain_sequence: List[Tuple[ChainWrapper, Optional[str]]]) -> None:
            Formats and prints the chain sequence.
        _run_validation_checks(prompt_variables_dict: Union[FirstCallRequired, None]) -> None:
            Runs validation checks for the prompt variables.
        add_chain_layer(system_prompt: str, human_prompt: str, output_passthrough_key_name: str | None = None, ignore_output_passthrough_key_name_error: bool = False, preprocessor: Callable[[Dict[str, Any]], Dict[str, Any]] | None = None, postprocessor: Callable[[Any], Any] | None = None, parser_type: Literal["pydantic", "json", "str"] | None = None, fallback_parser_type: Literal["pydantic", "json", "str"] | None = None, pydantic_output_model: :class:`~pydantic.BaseModel` | None = None, fallback_pydantic_output_model: :class:`~pydantic.BaseModel` | None = None) -> None:
            Adds a chain layer to the chain composer.
        _format_overwrite_warning(overwrites: Dict[str, Dict[str, Any]]) -> None:
            Formats a warning message for variable overwrites.
        _check_first_time_overwrites(prompt_variables_dict: Dict[str, Any]) -> None:
            Checks for first-time overwrites of global variables and issues warnings.
        _update_chain_variables(prompt_variables_dict: Dict[str, Any]) -> None:
            Updates global variables with new values, warning on first-time overwrites.
        get_chain_sequence() -> List[Tuple[ChainWrapper, Optional[str]]]:
            Returns the current chain sequence.
        print_chain_sequence() -> None:
            Prints the current chain sequence.
        get_chain_variables() -> Dict[str, Any]:
            Returns the current chain variables.
        print_chain_variables() -> None:
            Prints the current chain variables.
        run(prompt_variables_dict: Union[FirstCallRequired, None] = None) -> None:
            Runs the chain composer with the provided prompt variables.
    """

    def __init__(
        self,
        llm_model: str,
        api_key: str,
        llm_temperature: float = 0.7,
        preprocessor: Callable[[Dict[str, Any]], Dict[str, Any]] | None = None,
        postprocessor: Callable[[Any], Any] | None = None,
        log_to_console: bool = False,
        verbose: bool | None = False,
        llm_kwargs: Dict[str, Any] | None = None,
        words_to_ban: List[str] | None = None,
    ) -> None:
        """Initializes the ChainBuilder class.

        Args:
            llm_model (str): The name of the language model to be used.
            api_key (str): The API key for accessing the language model.
            llm_temperature (float, optional): The temperature setting for the language model.
                Defaults to 0.7.
            preprocessor (:class:`~python:typing.Callable` | None): A function to preprocess input data.
                Defaults to None.
            postprocessor (:class:`~python:typing.Callable` | None): A function to postprocess output data.
                Defaults to None.
            log_to_console (bool | None): Flag to enable logging to console.
                Defaults to False.
            llm_kwargs (dict | None): Additional keyword arguments for the language model.
                Type: Dict[str, Any]
            words_to_ban (list): A list of words to ban from the language model's output.
                Type: List[str]
                Defaults to None.

        """
        self.log_to_console: bool = log_to_console

        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="chain_builder",
            log_level=DEBUG,
        )

        self.llm_kwargs: Dict[str, Any] = llm_kwargs if llm_kwargs is not None else {}
        self.api_key: str = api_key
        self.llm_model: str = llm_model
        self.llm_model_type: str = self._get_llm_model_type(llm_model=llm_model)
        self.words_to_ban: List[str] | None = words_to_ban
        self.logit_bias_dict: Dict[int, int] | None = None

        if self.words_to_ban is not None:
            self._validate_words_to_ban(words_to_ban=self.words_to_ban)
            self.logit_bias_dict: Dict[int, int] = self._get_logit_bias_dict(
                llm_model=self.llm_model
            )

        self.logger.info(f"Initializing LLM: {self.llm_model}")

        if verbose:
            self.llm_kwargs["verbose"] = True

        self.llm_temperature: float = llm_temperature
        self.llm: Union[ChatOpenAI, ChatAnthropic, ChatGoogleGenerativeAI] = (
            self._initialize_llm(
                api_key=self.api_key,
                llm_model_type=self.llm_model_type,
                llm_model=self.llm_model,
                llm_temperature=self.llm_temperature,
                llm_kwargs=self.llm_kwargs,
                logit_bias_dict=self.logit_bias_dict,
            )
        )
        self.logger.info(f"Initialized LLM: {self.llm}")

        self.logger.info("Initializing ChainComposer")
        self.chain_composer: ChainComposer = ChainComposer(logger=self.logger)
        self.logger.info(f"Initialized ChainComposer: {self.chain_composer}")

        self.logger.info("Initializing Chain Variables")
        self.chain_variables: Dict[str, Any] = {}
        self.logger.info(f"Initialized Chain Variables: {self.chain_variables}")

        self.chain_variables_update_overwrite_warning_counter: int = 0

        self.logger.info(f"Initializing Preprocessor {preprocessor}")
        self.preprocessor: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = (
            preprocessor
        )
        self.logger.info(f"Initialized Preprocessor: {self.preprocessor}")

        self.logger.info(f"Initializing Postprocessor {postprocessor}")
        self.postprocessor: Optional[Callable[[Any], Any]] = postprocessor
        self.logger.info(f"Initialized Postprocessor: {self.postprocessor}")

    def __str__(self) -> str:
        """
        Returns a string representation of the ChainManager object.

        The string includes the values of the :attr:`~academic_metrics.ChainBuilder.ChainManager.llm`, :attr:`~academic_metrics.ChainBuilder.ChainManager.chain_composer`, and :attr:`~academic_metrics.ChainBuilder.ChainManager.chain_variables` attributes.

        Returns:
            str: A string representation of the ChainManager object.
        """
        return (
            f"ChainManager(llm={self.llm}, chain_composer={self.chain_composer}, "
            f"global_variables={self.global_variables})"
        )

    def __repr__(self) -> str:
        """
        Returns a string representation of the object for debugging purposes.

        This method calls the __str__() method to provide a user-friendly string
        representation of the object.

        Returns:
            str: A string representation of the object.
        """
        return self.__str__()

    def _validate_words_to_ban(self, words_to_ban: List[str]) -> None:
        """
        Validate the words to ban list.

        Args:
            words_to_ban (list): The list of words to ban.
                Type: List[str]
        """
        if not isinstance(words_to_ban, list):
            raise ValueError("words_to_ban must be a list of strings")

        if not all(isinstance(word, str) for word in words_to_ban):
            raise ValueError("words_to_ban must be a list of strings")

        self.logger.info(f"Words to ban: {words_to_ban}")

        if self.llm_model_type != "openai":
            raise ValueError(
                "words_to_ban is currently only supported for OpenAI models"
            )

        if self.llm_model not in ["gpt-4o", "gpt-4o-mini"]:
            raise ValueError(
                "words_to_ban is currently only supported for gpt-4o and gpt-4o-mini"
            )

    def _get_logit_bias_dict(self, llm_model: str) -> Dict[int, int]:
        """
        Get the logit bias dictionary for the words to ban.
        """
        self.logger.info(
            f"Getting logit bias dict for words to ban: {self.words_to_ban}"
        )

        # Get the tokenizer for gpt-4o and gpt-4o-mini
        tokenizer = tiktoken.get_encoding("o200k_base")

        logit_bias: Dict[int, int] = {}

        for word in self.words_to_ban:
            token_ids: List[int] = tokenizer.encode(word)

            if len(token_ids) > 1:
                self.logger.warning(
                    f"Word to ban '{word}' has multiple tokens: {token_ids}, all tokens will be banned"
                )

            # Apply -100 bias to each token
            for token_id in token_ids:
                logit_bias[token_id] = -100

        return logit_bias

    def _get_llm_model_type(self, *, llm_model: str) -> str:
        """
        Determine the type of LLM (Large Language Model) based on the provided model name.

        Args:
            llm_model (str): The name of the LLM model.

        Returns:
            str: The type of the LLM model. Possible values are "openai", "anthropic", and "google".

        Raises:
            ValueError: If the provided LLM model name does not match any of the supported types.
        """
        if llm_model.lower().startswith("gpt"):
            return "openai"
        elif llm_model.lower().startswith("claude"):
            return "anthropic"
        elif llm_model.lower().startswith("gemini"):
            return "google"
        else:
            raise ValueError(
                f"Unsupported LLM model: {llm_model}. Supported types: gpt, claude, gemini."
            )

    def _initialize_llm(
        self,
        *,
        api_key: str | None = None,
        llm_model_type: str | None = None,
        llm_model: str | None = None,
        llm_temperature: float | None = None,
        llm_kwargs: Dict[str, Any] | None = None,
        logit_bias_dict: Dict[int, int] | None = None,
    ) -> Union[ChatOpenAI, ChatAnthropic, ChatGoogleGenerativeAI]:
        """Initializes a language model based on the specified type.

        Args:
            api_key (str | None): The API key for accessing the language model service.
                Defaults to None.
            llm_model_type (str | None): The type of the language model (e.g., "openai", "anthropic", "google").
                Defaults to None.
            llm_model (str | None): The specific model to use within the chosen type.
                Defaults to None.
            llm_temperature (float | None): The temperature setting for the language model, affecting randomness.
                Defaults to None.
            llm_kwargs (dict | None): Additional keyword arguments specific to the language model.
                Type: Dict[str, Any]
                Defaults to None.
            logit_bias_dict (dict | None): A dictionary mapping token IDs to logit bias values.
                Type: Dict[int, int]
                Defaults to None.

        Returns:
            Union[:class:`ChatOpenAI`, :class:`ChatAnthropic`, :class:`ChatGoogleGenerativeAI`]: An instance of the initialized language model.

        Raises:
            ValueError: If the specified `llm_model_type` is not supported.
        """
        if api_key is None:
            api_key = self.api_key

        if llm_model_type is None:
            llm_model_type = self.llm_model_type

        if llm_model is None:
            llm_model = self.llm_model

        if llm_temperature is None:
            llm_temperature = self.llm_temperature

        if llm_kwargs is None:
            llm_kwargs = self.llm_kwargs

        if llm_model_type == "openai":
            return self._create_openai_llm(
                api_key, llm_model, llm_temperature, llm_kwargs, logit_bias_dict
            )
        elif llm_model_type == "anthropic":
            return self._create_anthropic_llm(
                api_key, llm_model, llm_temperature, **llm_kwargs
            )
        elif llm_model_type == "google":
            return self._create_google_llm(
                api_key, llm_model, llm_temperature, **llm_kwargs
            )
        else:
            raise ValueError(
                f"Unsupported LLM model type: {llm_model_type}. Supported types: openai, anthropic, google."
            )

    def _recreate_llm(self) -> None:
        """Recreates the LLM with the current parameters."""
        self.llm = self._initialize_llm(
            api_key=self.api_key,
            llm_model_type=self.llm_model_type,
            llm_model=self.llm_model,
            llm_temperature=self.llm_temperature,
            llm_kwargs=self.llm_kwargs,
            logit_bias_dict=self.logit_bias_dict,
        )

    def _create_openai_llm(
        self,
        api_key: str,
        llm_model: str,
        llm_temperature: float,
        llm_kwargs: Dict[str, Any],
        logit_bias_dict: Dict[int, int] | None = None,
    ) -> ChatOpenAI:
        """Creates an instance of the ChatOpenAI language model.

        Args:
            api_key (str): The API key for authenticating with the OpenAI service.
            llm_model (str): The identifier of the language model to use.
            llm_temperature (float): The temperature setting for the language model,
                which controls the randomness of the output.
            llm_kwargs (dict): Additional keyword arguments to pass to the ChatOpenAI constructor.
                Type: Dict[str, Any]
            logit_bias_dict (dict | None): A dictionary mapping token IDs to logit bias values.
                Type: Dict[int, int] | None
                Defaults to None.

        Returns:
            :class:`ChatOpenAI`: An instance of the ChatOpenAI language model configured with
                the specified parameters.
        """
        request_timeout: float | None = llm_kwargs.get("request_timeout", None)
        max_retries: int = llm_kwargs.get("max_retries", 3)

        return ChatOpenAI(
            model=llm_model,
            api_key=api_key,
            temperature=llm_temperature,
            request_timeout=request_timeout,
            max_retries=max_retries,
            logit_bias=logit_bias_dict,
        )

    def _create_anthropic_llm(
        self, api_key: str, llm_model: str, llm_temperature: float, **llm_kwargs
    ) -> ChatAnthropic:
        """Creates an instance of the ChatAnthropic language model.

        Args:
            api_key (str): The API key for authenticating with the Anthropic service.
            llm_model (str): The identifier of the language model to use.
            llm_temperature (float): The temperature setting for the language model,
                controlling the randomness of the output.
            llm_kwargs (dict): Additional keyword arguments to pass to the ChatAnthropic constructor.
                Type: Dict[str, Any]

        Returns:
            :class:`ChatAnthropic`: An instance of the ChatAnthropic language model.
        """
        return ChatAnthropic(
            model=llm_model, api_key=api_key, temperature=llm_temperature, **llm_kwargs
        )

    def _create_google_llm(
        self, api_key: str, llm_model: str, llm_temperature: float, **llm_kwargs
    ) -> ChatGoogleGenerativeAI:
        """Creates an instance of ChatGoogleGenerativeAI with the specified parameters.

        Args:
            api_key (str): The API key for authenticating with the Google LLM service.
            llm_model (str): The model identifier for the Google LLM.
            llm_temperature (float): The temperature setting for the LLM,
                which controls the randomness of the output.
            llm_kwargs (dict): Additional keyword arguments to pass to the ChatGoogleGenerativeAI constructor.
                Type: Dict[str, Any]

        Returns:
            :class:`ChatGoogleGenerativeAI`: An instance of the ChatGoogleGenerativeAI class configured
                with the provided parameters.
        """
        return ChatGoogleGenerativeAI(
            model=llm_model, api_key=api_key, temperature=llm_temperature, **llm_kwargs
        )

    def _initialize_parser(
        self,
        parser_type: Literal["pydantic", "json", "str"],
        pydantic_output_model: BaseModel | None = None,
    ) -> ParserType:
        """Initializes and returns a parser based on the specified parser type.

        Args:
            parser_type (str): The type of parser to initialize.
                Must be one of "pydantic", "json", or "str".
                Type: Literal["pydantic", "json", "str"]
            pydantic_output_model (:class:`~pydantic.BaseModel` | None): The Pydantic model to use for the parser. Required if parser_type is "pydantic".
                Defaults to None.

        Returns:
            ParserUnion: An instance of the specified parser type.

        Raises:
            ValueError: If an invalid parser_type is provided.
        """
        if parser_type == "pydantic":
            parser: PydanticOutputParser = self._create_pydantic_parser(
                pydantic_output_model=pydantic_output_model
            )
            self.logger.debug(f"Created Pydantic parser: {parser}")
            return parser
        elif parser_type == "json":
            parser: JsonOutputParser = self._create_json_parser(
                pydantic_output_model=pydantic_output_model
            )
            self.logger.debug(f"Created JSON parser: {parser}")
            return parser
        elif parser_type == "str":
            parser: StrOutputParser = self._create_str_parser()
            self.logger.debug(f"Created Str parser: {parser}")
            return parser
        else:
            raise ValueError(f"Invalid parser_type: {parser_type}")

    def _create_pydantic_parser(
        self, *, pydantic_output_model: BaseModel | None
    ) -> PydanticOutputParser:
        """Creates a Pydantic output parser.

        Args:
            pydantic_output_model (:class:`~pydantic.BaseModel` | None): The Pydantic model to be used for parsing output.

        Returns:
            :class:`PydanticOutputParser`: An instance of PydanticOutputParser initialized with
                the provided Pydantic model.

        Raises:
            ValueError: If pydantic_output_model is not provided.

        Notes:
            - `pydantic_output_model` must be provided for `parser_type` 'pydantic'.
        """
        if not pydantic_output_model:
            raise ValueError(
                "pydantic_output_model must be provided for 'pydantic' parser_type."
            )
        return PydanticOutputParser(pydantic_object=pydantic_output_model)

    def _create_json_parser(
        self, *, pydantic_output_model: BaseModel | None
    ) -> JsonOutputParser:
        """Creates a JSON parser for the chain layer output.

        Args:
            pydantic_output_model (BaseModel | None): An optional Pydantic model to enforce
                typing on the JSON output.
                Type: :class:`pydantic.BaseModel` | None
                Defaults to None.

        Returns:
            :class:`JsonOutputParser`: An instance of JsonOutputParser. If pydantic_output_model is provided, the parser will enforce the model's schema on the output.

        Raises:
            UserWarning: If `pydantic_output_model` is not provided, a warning is issued
                recommending its use for proper typing of the output.
        """
        json_parser: JsonOutputParser | None = None
        if not pydantic_output_model:
            warnings.warn(
                "It is highly recommended to provide a pydantic_output_model when parser_type is 'json'. "
                "This will ensure that the output of the chain layer is properly typed and can be used in downstream chain layers."
            )
            self.logger.debug("Creating JSON parser without pydantic_output_model. ")
            json_parser = JsonOutputParser()
        else:
            self.logger.debug(
                f"Creating JSON parser with pydantic_output_model: {pydantic_output_model}"
            )
        return JsonOutputParser(pydantic_object=pydantic_output_model)

    def _create_str_parser(self) -> StrOutputParser:
        """
        Creates an instance of StrOutputParser.

        Returns:
            :class:`StrOutputParser`: An instance of the StrOutputParser class.
        """
        return StrOutputParser()

    def _run_chain_validation_checks(
        self,
        *,
        output_passthrough_key_name: str | None,
        ignore_output_passthrough_key_name_error: bool,
        parser_type: Literal["pydantic", "json", "str"] | None,
        pydantic_output_model: BaseModel | None,
        fallback_parser_type: Literal["pydantic", "json", "str"] | None,
        fallback_pydantic_output_model: BaseModel | None,
    ) -> None:
        """Validates chain configuration parameters before execution.

        Performs validation checks on chain configuration parameters to ensure proper setup
        and compatibility between different components.

        Args:
            output_passthrough_key_name (str | None): Optional key name for passing chain
                output to next layer.
                Defaults to None.
            ignore_output_passthrough_key_name_error (bool): Whether to ignore missing output key name.
                Defaults to False.
            parser_type (str | None): Type of parser to use.
                Type: Literal["pydantic", "json", "str"] | None
                Defaults to None.
            pydantic_output_model (:class:`~pydantic.BaseModel` | None): Pydantic model for output validation.
                Defaults to None.
            fallback_parser_type (str | None): Type of fallback parser.
                Type: Literal["pydantic", "json", "str"] | None
                Defaults to None.
            fallback_pydantic_output_model (:class:`~pydantic.BaseModel` | None): Pydantic model for fallback parser.
                Defaults to None.

        Raises:
            ValueError: If validation fails for:
                - Missing output key name when required
                - Invalid parser type combinations
                - Missing required models
                - Duplicate parser types
                - Same models used for main and fallback

        Warnings:
            UserWarning: For non-critical issues like:
                - Missing output key name when ignored
                - Missing recommended Pydantic models
                - Unused provided models
        """
        if (
            len(self.chain_composer.chain_sequence) > 0
            and not output_passthrough_key_name
        ):
            if not ignore_output_passthrough_key_name_error:
                raise ValueError(
                    "output_passthrough_key_name not provided and ignore_output_passthrough_key_name_error is False. output_passthrough_key_name is required to identify the output of the chain layer in order to pass the output to the next chain layer. If you do not specify output_passthrough_key_name, the output of the chain layer will not be assigned to a variable and thus will not be available to the next chain layer. If you do not need the output of the chain layer to be passed to the next chain layer, you can set ignore_output_passthrough_key_name_error to True."
                )
            else:
                warnings.warn(
                    "output_passthrough_key_name not provided when adding a chain layer after another. Output of the chain layer will not be assigned to a variable."
                )

        if parser_type is None and fallback_parser_type is not None:
            raise ValueError(
                "parser_type is None when fallback_parser_type is not None. This is not allowed."
            )

        if (parser_type is not None and pydantic_output_model is not None) and (
            fallback_parser_type is not None
            and fallback_pydantic_output_model is not None
        ):
            if pydantic_output_model == fallback_pydantic_output_model:
                raise ValueError(
                    "pydantic_output_model and fallback_pydantic_output_model are the same. This is not allowed."
                )

        if parser_type is not None:
            if parser_type not in ["pydantic", "json", "str"]:
                raise ValueError(
                    f"Unsupported parser type: {parser_type}. Supported types:\n"
                    f"\t'{PydanticOutputParser.__name__}'\n"
                    f"\t'{JsonOutputParser.__name__}'\n"
                    f"\t'{StrOutputParser.__name__}'"
                )
            if parser_type == "pydantic":
                if not pydantic_output_model:
                    raise ValueError(
                        "pydantic_output_model must be specified when parser_type is 'pydantic'."
                    )
            elif parser_type == "json":
                if not pydantic_output_model:
                    warnings.warn(
                        "It is highly recommended to provide a pydantic_output_model when parser_type is 'json'. "
                        "This will ensure that the output of the chain layer is properly typed and can be used in downstream chain layers."
                    )
        else:
            if pydantic_output_model:
                warnings.warn(
                    "pydantic_output_model is provided but parser_type is None. The pydantic_output_model will not be used."
                )

        if fallback_parser_type is not None:
            if parser_type is not None and fallback_parser_type == parser_type:
                raise ValueError(
                    "parser_type and fallback_parser_type are the same. This is not allowed."
                )

            if fallback_parser_type not in ["pydantic", "json", "str"]:
                raise ValueError(
                    f"Unsupported fallback_parser_type: {fallback_parser_type}"
                )
            if fallback_parser_type == "pydantic":
                if not fallback_pydantic_output_model:
                    raise ValueError(
                        "fallback_pydantic_output_model must be specified when fallback_parser_type is 'pydantic'."
                    )
            elif fallback_parser_type == "json":
                if not fallback_pydantic_output_model:
                    warnings.warn(
                        "It is highly recommended to provide a fallback_pydantic_output_model when fallback_parser_type is 'json'. "
                        "This will ensure that the output of the fallback chain layer is properly typed and can be used in downstream chain layers."
                    )
        else:
            if fallback_pydantic_output_model:
                warnings.warn(
                    "fallback_pydantic_output_model is provided but fallback_parser_type is None. The fallback_pydantic_output_model will not be used."
                )

    def _format_chain_sequence(
        self, chain_sequence: List[Tuple[ChainWrapper, str | None]]
    ) -> None:
        """Formats and prints the details of each chain in the given chain sequence.

        Args:
            chain_sequence (list): A list of tuples where each tuple contains a ChainWrapper
                object and an optional output name.
                Type: List[Tuple[ChainWrapper, str | None]]

        Returns:
            None
        """
        for index, (chain_wrapper, output_name) in enumerate(chain_sequence):
            print(f"Chain {index + 1}:")
            print(f"\tOutput Name: {output_name}")
            print(f"\tParser Type: {chain_wrapper.get_parser_type()}")
            print(f"\tFallback Parser Type: {chain_wrapper.get_fallback_parser_type()}")
            print(f"\tPreprocessor: {chain_wrapper.preprocessor}")
            print(f"\tPostprocessor: {chain_wrapper.postprocessor}")

    def _run_validation_checks(
        self,
        *,
        prompt_variables_dict: Union[FirstCallRequired, None],
    ) -> None:
        """Validates the input parameters for the chain execution.

        Args:
            prompt_variables_dict (dict | None): A dictionary containing the variables
                to be passed to the chain layers.
                Type: Union[FirstCallRequired, None]
                - On the first call to `run()`, this parameter must be provided.
                - On subsequent calls, it can be omitted if there are no new variables to pass.

        Raises:
            ValueError: If `prompt_variables_dict` is None on the first call to `run()`.
            TypeError: If `prompt_variables_dict` is not a dictionary when provided.

        Notes:
            - The `prompt_variables_dict` should contain keys that match the variable names
              used in the chain layers.
            - The `output_passthrough_key_name` parameter in the `add_chain_layer` method is
              used to identify the output of the chain layer and assign it to a variable.
            - If `output_passthrough_key_name` is not specified, the output of the chain layer
              will not be assigned to a variable and will not be available to the next chain layer.
            - The `ignore_output_passthrough_key_name_error` parameter can be set to True if
              the output of the chain layer is not needed for the next chain layer, such as
              when running a chain layer solely for its side effects or if it is the last
              chain layer in a multi-layer chain.
            - Ensure that the placeholder variable names in your prompt strings match the keys
              in `prompt_variables_dict` passed into the `ChainManager.run()` method.
        """
        if (
            self.chain_variables_update_overwrite_warning_counter == 0
            and prompt_variables_dict is None
        ):
            raise ValueError(
                "First call to run() must provide a prompt_variables_dict, otherwise there are no variables to pass to the chain layers. "
                "If you do not need to pass variables to the chain layers, you can set prompt_variables_dict to an empty dictionary. "
                "Subsequent calls to run() can omit prompt_variables_dict if you have no new variables to pass to the chain layers. "
                "This is because a global_variables dictionary is maintained in the ChainManager instance and is updated with each call to the class instance method _update_global_variables(), which is automatically called by run() when a new prompt_variables_dict is provided. "
            )

        if prompt_variables_dict is not None and not isinstance(
            cast(Any, prompt_variables_dict), dict
        ):
            raise TypeError(
                "prompt_variables_dict must be a dictionary. "
                "Each key should match the variable names used in your chain layers. "
                "output_passthrough_key_name parameter in add_chain_layer method is used to identify the output of the chain layer "
                "and assign it to a variable. If you do not specify output_passthrough_key_name, the output of the chain layer will not be assigned to a variable and thus will not be available to the next chain layer. "
                "If you do not need the output of the chain layer to be passed to the next chain layer, you can set ignore_output_passthrough_key_name_error to True. "
                "A time to set ignore_output_passthrough_key_name_error to True is when you are running a chain layer solely for its side effects (e.g. printing, saving to a database, etc.) without needing the output of the chain layer to be passed to the next chain layer. "
                "Another reason to set ignore_output_passthrough_key_name_error to True is if you have a multi-layer chain and this is your last chain layer. "
                "Check your prompt strings for your placeholder variables, these names should match the keys in prompt_variables_dict passed into the ChainManager.run() method."
            )

    def add_chain_layer(
        self,
        *,
        system_prompt: str,
        human_prompt: str,
        output_passthrough_key_name: str | None = None,
        ignore_output_passthrough_key_name_error: bool = False,
        preprocessor: Callable[[Dict[str, Any]], Dict[str, Any]] | None = None,
        postprocessor: Callable[[Any], Any] | None = None,
        parser_type: Literal["pydantic", "json", "str"] | None = None,
        fallback_parser_type: Literal["pydantic", "json", "str"] | None = None,
        pydantic_output_model: BaseModel | None = None,
        fallback_pydantic_output_model: BaseModel | None = None,
    ) -> None:
        """Adds a chain layer to the chain composer.

        This method configures and adds a new chain layer to the chain composer,
        allowing for the processing of input data through specified prompts and parsers.

        Args:
            system_prompt (str): The system prompt template for the chain layer.
            human_prompt (str): The human prompt template for the chain layer.
            output_passthrough_key_name (str | None): Key name for passing chain output
                to the next layer.
                Defaults to None.
            ignore_output_passthrough_key_name_error (bool): Flag to ignore missing output
                key name errors.
                Defaults to False.
            preprocessor (:class:`python:typing.Callable` | None): Function to preprocess input data.
                Defaults to None.
            postprocessor (:class:`python:typing.Callable` | None): Function to postprocess output data.
                Defaults to None.
            parser_type (str | None): Type of parser to use.
                Type: Literal["pydantic", "json", "str"] | None
                Defaults to None.
            fallback_parser_type (str | None): Type of fallback parser.
                Type: Literal["pydantic", "json", "str"] | None
                Defaults to None.
            pydantic_output_model (:class:`~pydantic.BaseModel` | None): Pydantic model for output validation.
                Defaults to None
            fallback_pydantic_output_model (:class:`~pydantic.BaseModel` | None): Pydantic model for fallback parser.
                Defaults to None.

        Returns:
            None
        """
        self.logger.info(
            f"Adding chain layer with output_passthrough_key_name: {output_passthrough_key_name}"
        )
        self.logger.info(
            f"ignore_output_passthrough_key_name_error: {ignore_output_passthrough_key_name_error}"
        )
        self.logger.info(f"parser_type: {parser_type}")
        self.logger.info(f"pydantic_output_model: {pydantic_output_model}")
        self.logger.info(f"preprocessor: {preprocessor}")
        self.logger.info(f"postprocessor: {postprocessor}")
        self.logger.info("--------------------------------")
        self.logger.info(f"system_prompt: {system_prompt}")
        self.logger.info(f"human_prompt: {human_prompt}")
        self.logger.info("--------------------------------")

        self._run_chain_validation_checks(
            output_passthrough_key_name=output_passthrough_key_name,
            ignore_output_passthrough_key_name_error=ignore_output_passthrough_key_name_error,
            parser_type=parser_type,
            pydantic_output_model=pydantic_output_model,
            fallback_parser_type=fallback_parser_type,
            fallback_pydantic_output_model=fallback_pydantic_output_model,
        )

        parser: Optional[ParserType] = None
        fallback_parser: Optional[ParserType] = None
        if parser_type:
            parser = self._initialize_parser(
                parser_type=parser_type, pydantic_output_model=pydantic_output_model
            )
        if fallback_parser_type:
            fallback_parser = self._initialize_parser(
                parser_type=fallback_parser_type,
                pydantic_output_model=fallback_pydantic_output_model,
            )
        # Create prompt templates without specifying input_variables
        system_prompt_template: PromptTemplate = PromptTemplate(template=system_prompt)
        human_prompt_template: PromptTemplate = PromptTemplate(template=human_prompt)
        system_message_prompt_template: SystemMessagePromptTemplate = (
            SystemMessagePromptTemplate.from_template(system_prompt_template.template)
        )
        human_message_prompt_template: HumanMessagePromptTemplate = (
            HumanMessagePromptTemplate.from_template(human_prompt_template.template)
        )

        chat_prompt_template: ChatPromptTemplate = ChatPromptTemplate.from_messages(
            [system_message_prompt_template, human_message_prompt_template]
        )
        # Build the chain using ChainBuilder
        chain_builder: ChainBuilder = ChainBuilder(
            chat_prompt=chat_prompt_template,
            llm=self.llm,
            parser=parser,
            fallback_parser=fallback_parser,
            logger=self.logger,
        )
        chain: Runnable = chain_builder.get_chain()
        fallback_chain: Runnable = chain_builder.get_fallback_chain()

        # Wrap the chain
        chain_wrapper: ChainWrapper = ChainWrapper(
            chain=chain,
            fallback_chain=fallback_chain,
            parser=parser,
            fallback_parser=fallback_parser,
            preprocessor=preprocessor or self.preprocessor,
            postprocessor=postprocessor or self.postprocessor,
            logger=self.logger,
        )

        # Add the chain to the composer
        self.chain_composer.add_chain(
            chain_wrapper=chain_wrapper,
            output_passthrough_key_name=output_passthrough_key_name,
        )

        # Return the ChainManager instance to allow for method chaining
        return self

    def _format_overwrite_warning(self, overwrites: Dict[str, Dict[str, Any]]) -> str:
        """Formats a warning message for overwritten values.

        Args:
            overwrites (dict): A dictionary where the key is the name of the overwritten item,
                and the value is another dictionary with 'old' and 'new' keys representing
                the old and new values respectively.
                Type: Dict[str, Dict[str, Any]]

        Returns:
            str: A formatted string that lists each overwritten item with its old and
                new values.
        """
        return "\n".join(
            f"  {key}:\n    - {details['old']}\n    + {details['new']}"
            for key, details in overwrites.items()
        )

    def _check_first_time_overwrites(
        self, prompt_variables_dict: Dict[str, Any]
    ) -> None:
        """Checks and warns if any global chain variables are being overwritten for the first time.

        This method compares the keys in the provided `prompt_variables_dict` with the existing
        `chain_variables`. If any keys match, it indicates that an overwrite is occurring. A warning
        is issued the first time this happens, detailing the old and new values of the overwritten
        variables. Subsequent overwrites will not trigger warnings.

        Args:
            prompt_variables_dict (dict): A dictionary containing the new values for the chain
                variables that may overwrite existing ones.
                Type: Dict[str, Any]

        Returns:
            None
        """
        if self.chain_variables_update_overwrite_warning_counter == 0:
            overwrites = {
                key: {
                    "old": self.chain_variables[key],
                    "new": prompt_variables_dict[key],
                }
                for key in prompt_variables_dict
                if key in self.chain_variables
            }
            if overwrites:
                warnings.warn(
                    f"Overwriting existing global variables:\n"
                    f"{self._format_overwrite_warning(overwrites)}\n"
                    "Subsequent overwrites will not trigger warnings."
                )
            self.chain_variables_update_overwrite_warning_counter += 1

    def _update_chain_variables(self, prompt_variables_dict: Dict[str, Any]) -> None:
        """Update global variables with new values, warning on first-time overwrites.

        Args:
            prompt_variables_dict (dict): A dictionary containing the new values for
                the global variables.
                Type: Dict[str, Any]

        Returns:
            None
        """
        # Update global variables with new values, warning on first-time overwrites.
        self._check_first_time_overwrites(prompt_variables_dict)
        self.chain_variables.update(prompt_variables_dict)

    def get_chain_sequence(self) -> List[Tuple[ChainWrapper, str | None]]:
        """Retrieves the chain sequence from the chain composer.

        Returns:
            list: A list of tuples where each tuple contains a ChainWrapper object and
                the output key name if output_passthrough_key_name was provided to add_chain_layer.
                Type: List[Tuple[:class:`academic_metrics.ChainBuilder.ChainBuilder.ChainWrapper`, str | None]]
        """
        return self.chain_composer.chain_sequence

    def print_chain_sequence(self) -> None:
        """Prints the chain sequence by formatting it.

        This method retrieves the chain sequence from the chain composer and
        formats it using the _format_chain_sequence method.

        Returns:
            None
        """
        chain_sequence: List[Tuple[ChainWrapper, str | None]] = (
            self.chain_composer.chain_sequence
        )
        self._format_chain_sequence(chain_sequence)

    def get_chain_variables(self) -> Dict[str, Any]:
        """Retrieve the chain variables.

        Returns:
            dict: A dictionary containing the chain variables.
                Type: Dict[str, Any]
        """
        return self.chain_variables

    def print_chain_variables(self) -> None:
        """Prints the chain variables in a formatted manner.

        This method prints the chain variables stored in the `chain_variables`
        attribute of the class. The output is formatted with a header and
        footer consisting of dashes, and each key-value pair is printed on
        a new line.

        Returns:
            None
        """
        print(f"Chain Variables:\n{'-' * 10}")
        for key, value in self.chain_variables.items():
            print(f"{key}: {value}")
        print(f"{'-' * 10}\n")

    def set_words_to_ban(self, words_to_ban: List[str]) -> None:
        """Updates the list of words to ban and recreates the OpenAI LLM with new logit bias.

        Args:
            words_to_ban (list): List of words to ban from LLM output
                Type: List[str]

        Returns:
            None
        """
        self.words_to_ban = words_to_ban
        self._validate_words_to_ban(words_to_ban)
        self.logit_bias_dict = self._get_logit_bias_dict(llm_model=self.llm_model)
        self._recreate_llm()

    def run(
        self,
        *,
        prompt_variables_dict: Union[FirstCallRequired, None] = None,
    ) -> str:
        """Executes the chain builder process.

        This method performs validation checks, updates chain variables if provided,
        and runs the chain composer with the current chain variables.

        Args:
            prompt_variables_dict (dict | None): A dictionary containing prompt variables.
                If provided, it will be used to update the chain variables.
                Type: Union[:data:`~academic_metrics.ChainBuilder.ChainBuilder.FirstCallRequired`, None]
                Defaults to None.

        Returns:
            str: The result of running the chain composer.
        """
        self._run_validation_checks(
            prompt_variables_dict=prompt_variables_dict,
        )

        if prompt_variables_dict is not None:
            self._update_chain_variables(prompt_variables_dict)

        return self.chain_composer.run(
            data_dict=self.chain_variables,
            data_dict_update_function=self._update_chain_variables,
        )

            ```

            src/academic_metrics/ChainBuilder/__init__.py:
            ```
from .ChainBuilder import (
    ChainManager,
    ChainWrapper,
    ChainComposer,
    ChainBuilder,
    FirstCallRequired,
    ParserUnion,
    ParserType,
)

            ```

            src/academic_metrics/DB/DatabaseSetup.py:
            ```
import atexit
import json
import logging
import os
from typing import Any, Dict, List, Tuple, TypeAlias

from dotenv import load_dotenv
from pymongo.collection import Collection
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)

CollectionData: TypeAlias = List[Dict[str, Any]]
"""Type alias representing a collection of documents from MongoDB.

Each document is represented as a dictionary with string keys and arbitrary values.

Type:
    List[Dict[str, Any]]: A list of dictionaries where each dictionary represents a MongoDB document.
"""

DatabaseSnapshot: TypeAlias = Tuple[CollectionData, CollectionData, CollectionData]
"""Type alias representing a snapshot of all collections in the database.

Contains data from articles, categories, and faculty collections in that order.

Type:
    Tuple[CollectionData, CollectionData, CollectionData]: A tuple containing:
        - article_data (CollectionData): Documents from the articles collection
        - category_data (CollectionData): Documents from the categories collection
        - faculty_data (CollectionData): Documents from the faculty collection
"""


class DatabaseWrapper:
    """A wrapper class for MongoDB operations.

    Attributes:
        logger (logging.Logger): Logger for logging messages.
        client (MongoClient): MongoDB client.
        db (Database): MongoDB database.
        article_collection (Collection): MongoDB collection for article data.
        category_collection (Collection): MongoDB collection for category data.
        faculty_collection (Collection): MongoDB collection for faculty data.

    Methods:
        _test_connection: Test the connection to the MongoDB server.
        get_dois: Get all DOIs from the article collection.
        get_all_data: Get all data from the article, category, and faculty collections.
        insert_categories: Insert multiple categories into the collection.
        update_category: Update an existing category.
        insert_articles: Insert multiple articles into the collection.
        insert_faculty: Insert multiple faculty entries into the collection.
        update_faculty: Update an existing faculty member.
        process: Process data and insert it into the appropriate collection.
        run_all_process: Run the process method for all collections.
        clear_collection: Clear the entire collection.
        close_connection: Close the connection to the MongoDB server.
    """

    def __init__(self, *, db_name: str, mongo_uri: str):
        """Initialize the DatabaseWrapper with database name, collection name, and MongoDB URL.

        Args:
            db_name (str): Name of the database.
            mongo_uri (str): MongoDB URI.
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="database_setup",
            log_level=DEBUG,
        )

        if not mongo_uri:
            print("Url error")
            return

        self.mongo_uri = mongo_uri
        self.client = MongoClient(self.mongo_uri, server_api=ServerApi("1"))
        self.db = self.client[db_name]
        self.article_collection: Collection = self.db["article_data"]
        self.category_collection: Collection = self.db["category_data"]
        self.faculty_collection: Collection = self.db["faculty_data"]
        self._test_connection()
        atexit.register(self.close_connection)

    def _test_connection(self):
        """Test the connection to the MongoDB server."""
        try:
            self.client.admin.command("ping")
            self.logger.info(
                "Pinged your deployment. You successfully connected to MongoDB!"
            )
        except Exception as e:
            self.logger.error(f"Connection error: {e}")

    def get_dois(self) -> List[str]:
        """Get all DOIs from the article collection.

        Returns:
            doi_list (List[str]): List of DOIs.
        """
        articles = self.article_collection.find({})
        doi_list = []
        for article in articles:
            doi_list.append(article["_id"])
        self.logger.info(f"Retrieved DOIs: {doi_list}")
        return doi_list

    def get_all_data(self) -> DatabaseSnapshot:
        """Get all data from the article, category, and faculty collections.

        Returns:
            Tuple[CollectionData, CollectionData, CollectionData]: A tuple containing:
            - articles (CollectionData): Documents from the articles collection

            - categories (CollectionData): Documents from the categories collection

            - faculty (CollectionData): Documents from the faculty collection
        """
        articles: CollectionData = list(self.article_collection.find({}))
        categories: CollectionData = list(self.category_collection.find({}))
        faculty: CollectionData = list(self.faculty_collection.find({}))

        self.logger.info("Retrieved all data from collections.")
        return (articles, categories, faculty)

    def insert_categories(self, category_data: List[Dict[str, Any]]):
        """Insert multiple categories into the collection.

        If a category already exists, add the numbers and extend the lists.

        Args:
            category_data (List[Dict[str, Any]]): List of category data.
        """
        if not category_data:
            self.logger.error("Category data is empty or None")
        for item in category_data:
            existing_data = self.category_collection.find_one({"_id": item["_id"]})
            if existing_data:
                new_item = self.update_category(existing_data, item)
                self.category_collection.update_one(
                    {"_id": item["_id"]}, {"$set": new_item}
                )
                self.logger.info(f"Updated category: {item['_id']}")
            else:
                self.category_collection.insert_one(item)
                self.logger.info(f"Inserted new category: {item['_id']}")

    def update_category(
        self, existing_data: Dict[str, Any], new_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Update existing category data with new data, handling None values and logging state.

        Args:
            existing_data (dict[str, Any]): Existing category data.
            new_data (dict[str, Any]): New category data.

        Returns:
            existing_data (Dict[str, Any]): Updated category data.
        """
        # ! THESE HAVE TO BE UNION NOT UPDATE
        # ! ALSO USING .GET() SO IT DOESN'T THROW AN ERROR IF THE KEY DOESN'T EXIST
        # ! NOW DOING LIST(SET()) TO CONVERT TO LIST
        # Get DOI lists with None protection
        existing_dois = existing_data.get("doi_list", []) or []
        new_dois = new_data.get("doi_list", []) or []
        self.logger.debug(f"DOIs - Existing: {existing_dois}, New: {new_dois}")

        # Only update if there's no intersection between DOI lists
        if not set(existing_dois).intersection(set(new_dois)):
            self.logger.info(
                f"No DOI intersection found for category {existing_data.get('_id')} - updating data"
            )

            # Calculate new citation average
            scaled_averages = len(existing_dois) * existing_data.get(
                "citation_average", 0
            ) + len(new_dois) * new_data.get("citation_average", 0)
            new_average = scaled_averages / (len(existing_dois) + len(new_dois))
            existing_data["citation_average"] = new_average
            self.logger.debug(f"Updated citation average to: {new_average}")

            # Update lists using set operations with None protection
            existing_data["doi_list"] = list(set(existing_dois).union(new_dois))

            existing_data["themes"] = list(
                set(existing_data.get("themes", []) or []).union(
                    new_data.get("themes", []) or []
                )
            )

            existing_data["faculty"] = list(
                set(existing_data.get("faculty", []) or []).union(
                    new_data.get("faculty", []) or []
                )
            )

            existing_data["departments"] = list(
                set(existing_data.get("departments", []) or []).union(
                    new_data.get("departments", []) or []
                )
            )

            existing_data["titles"] = list(
                set(existing_data.get("titles", []) or []).union(
                    new_data.get("titles", []) or []
                )
            )

            # Update numeric counts
            existing_data["faculty_count"] = len(existing_data["faculty"])
            existing_data["department_count"] = len(existing_data["departments"])
            existing_data["article_count"] = len(existing_data["titles"])
            existing_data["tc_count"] = existing_data.get("tc_count", 0) + new_data.get(
                "tc_count", 0
            )

            self.logger.debug(
                f"Updated counts - Faculty: {existing_data['faculty_count']}, "
                f"Departments: {existing_data['department_count']}, "
                f"Articles: {existing_data['article_count']}, "
                f"TC: {existing_data['tc_count']}"
            )
        else:
            self.logger.info(
                f"DOI intersection found for category {existing_data.get('_id')} - skipping update"
            )

        return existing_data

    def insert_articles(self, article_data: List[Dict[str, Any]]):
        """Insert multiple articles into the collection.

        If an article already exists, merge the new data with the existing data.

        Args:
            article_data (List[Dict[str, Any]]): List of article data.
        """
        for item in article_data:
            try:
                self.article_collection.insert_one(item)
                self.logger.info(f"Inserted new articles: {item['_id']}")
            except Exception as e:
                self.logger.info(f"Duplicate content not adding {e}")

    def insert_faculty(self, faculty_data: List[Dict[str, Any]]):
        """Insert multiple faculty entries into the collection.

        If a faculty member already exists, update the data accordingly.

        Args:
            faculty_data (List[Dict[str, Any]]): List of faculty data.
        """
        for item in faculty_data:
            existing_data = self.faculty_collection.find_one({"_id": item["_id"]})
            if existing_data:
                new_item = self.update_faculty(existing_data, item)
                self.faculty_collection.update_one(
                    {"_id": item["_id"]}, {"$set": new_item}
                )
                self.logger.info(f"Updated faculty: {item['_id']}")
            else:
                self.faculty_collection.insert_one(item)
                self.logger.info(f"Inserted new faculty: {item['_id']}")

    def update_faculty(
        self, existing_data: Dict[str, Any], new_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Update existing faculty data with new data, handling None values and logging state.

        Args:
            existing_data (Dict[str, Any]): Existing faculty data.
            new_data (Dict[str, Any]): New faculty data.

        Returns:
            existing_data (Dict[str, Any]): Updated faculty data.
        """
        # Get DOI lists with None protection
        existing_dois = existing_data.get("dois", []) or []
        new_dois = new_data.get("dois", []) or []
        self.logger.debug(f"DOIs - Existing: {existing_dois}, New: {new_dois}")

        # Only update if there's no intersection between DOI lists
        if not set(existing_dois).intersection(set(new_dois)):
            self.logger.info(
                f"No DOI intersection found for faculty {existing_data.get('_id')} - updating data"
            )

            # Update numeric values
            existing_data["total_citations"] = existing_data.get(
                "total_citations", 0
            ) + new_data.get("total_citations", 0)

            # Update lists using set operations with None protection
            existing_data["dois"] = list(set(existing_dois).union(new_dois))

            # Handle department affiliations as list
            existing_affiliations = (
                existing_data.get("department_affiliations", []) or []
            )
            new_affiliations = new_data.get("department_affiliations", []) or []
            existing_data["department_affiliations"] = list(
                set(existing_affiliations).union(new_affiliations)
            )

            # Update all set-based fields with None protection
            existing_data["titles"] = list(
                set(existing_data.get("titles", []) or []).union(
                    new_data.get("titles", []) or []
                )
            )

            existing_data["categories"] = list(
                set(existing_data.get("categories", []) or []).union(
                    new_data.get("categories", []) or []
                )
            )

            existing_data["top_level_categories"] = list(
                set(existing_data.get("top_level_categories", []) or []).union(
                    new_data.get("top_level_categories", []) or []
                )
            )

            existing_data["mid_level_categories"] = list(
                set(existing_data.get("mid_level_categories", []) or []).union(
                    new_data.get("mid_level_categories", []) or []
                )
            )

            existing_data["low_level_categories"] = list(
                set(existing_data.get("low_level_categories", []) or []).union(
                    new_data.get("low_level_categories", []) or []
                )
            )

            existing_data["category_urls"] = list(
                set(existing_data.get("category_urls", []) or []).union(
                    new_data.get("category_urls", []) or []
                )
            )

            existing_data["top_category_urls"] = list(
                set(existing_data.get("top_category_urls", []) or []).union(
                    new_data.get("top_category_urls", []) or []
                )
            )

            existing_data["mid_category_urls"] = list(
                set(existing_data.get("mid_category_urls", []) or []).union(
                    new_data.get("mid_category_urls", []) or []
                )
            )

            existing_data["low_category_urls"] = list(
                set(existing_data.get("low_category_urls", []) or []).union(
                    new_data.get("low_category_urls", []) or []
                )
            )

            existing_data["themes"] = list(
                set(existing_data.get("themes", []) or []).union(
                    new_data.get("themes", []) or []
                )
            )

            existing_data["journals"] = list(
                set(existing_data.get("journals", []) or []).union(
                    new_data.get("journals", []) or []
                )
            )

            self.logger.debug(
                f"Updated counts - Citations: {existing_data['total_citations']}, "
                f"DOIs: {len(existing_data['dois'])}"
            )
        else:
            self.logger.info(
                f"DOI intersection found for faculty {existing_data.get('_id')} - skipping update"
            )

        return existing_data

    def process(self, data: List[Dict[str, Any]], collection: str):
        """Process data and insert it into the appropriate collection.

        Args:
            data (List[Dict[str, Any]]): Data to be inserted.
            collection (str): Name of the collection to insert the data into.
        """
        if collection == "article_data":
            self.insert_articles(data)
        elif collection == "category_data":
            self.insert_categories(data)
        elif collection == "faculty_data":
            self.insert_faculty(data)

    def run_all_process(
        self,
        category_data: List[Dict[str, Any]],
        article_data: List[Dict[str, Any]],
        faculty_data: List[Dict[str, Any]],
    ):
        """Process all data and insert it into the appropriate collections.

        Args:
            category_data (List[Dict[str, Any]]): Category data.
            article_data (List[Dict[str, Any]]): Article data.
            faculty_data (List[Dict[str, Any]]): Faculty data.
        """
        self.process(category_data, "category_data")
        self.process(article_data, "article_data")
        self.process(faculty_data, "faculty_data")

    def fix_counts(self):
        existing_data = list(self.category_collection.find({}))
        for data in existing_data:
            data["faculty_count"] = len(data["faculty"])
            data["department_count"] = len(data["departments"])
            self.category_collection.update_one({"_id": data["_id"]}, {"$set": data})

    def clear_collection(self):
        """Clear the entire collection."""
        self.category_collection.delete_many({})
        self.article_collection.delete_many({})
        self.faculty_collection.delete_many({})
        self.logger.info("Cleared the entire collection")

    def close_connection(self):
        """Close the connection to the MongoDB server."""
        self.client.close()
        self.logger.info("Connection closed")


if __name__ == "__main__":
    # Load environment variables
    load_dotenv()
    mongo_uri = os.getenv("MONGODB_URI")

    # # Handle article data
    # with open(
    #     "../../data/core/output_files/test_processed_article_stats_obj_data.json", "r"
    # ) as f:
    #     article_data = json.load(f)

    # # Handle category data
    # with open(
    #     "../../data/core/output_files/test_processed_category_data.json", "r"
    # ) as f:
    #     category_data = json.load(f)

    # # Handle faculty data
    # with open(
    #     "../../data/core/output_files/test_processed_global_faculty_stats_data.json",
    #     "r",
    # ) as f:
    #     faculty_data = json.load(f)

    database = DatabaseWrapper(db_name="Site_Data", mongo_uri=mongo_uri)
    # database.clear_collection()

    # database.process(article_data, "article_data")
    # database.process(category_data, "category_data")
    # database.process(faculty_data, "faculty_data")
    database.fix_counts()

            ```

            src/academic_metrics/DB/__init__.py:
            ```
from .DatabaseSetup import DatabaseWrapper

            ```

            src/academic_metrics/DB/article_to_excel.py:
            ```
import pandas as pd

# pd.set_option('display.max_columns', None)  # Show all columns
# pd.set_option('display.width', None)       # Show all columns

df = pd.read_json("category_data.json")

df.to_excel("category_data.xlsx", index=False)

df = pd.read_json("article_data.json")

df.to_excel("article_data.xlsx", index=False)

df = pd.read_json("faculty_data.json")

df.to_excel("faculty_data.xlsx", index=False)

            ```

            src/academic_metrics/DB/clear_db.py:
            ```
from academic_metrics.DB.DatabaseSetup import DatabaseWrapper

from dotenv import load_dotenv
import os

load_dotenv()

mongodb_url = os.getenv("LOCAL_MONGODB_URL")
db = DatabaseWrapper(db_name="Site_Data", mongo_url=mongodb_url)

db.clear_collection()
db.close_connection()

            ```

            src/academic_metrics/DB/get_all_data.py:
            ```
from academic_metrics.DB.DatabaseSetup import DatabaseWrapper, DatabaseSnapshot
import os
from dotenv import load_dotenv
import json

load_dotenv()


def get_all_data(db_setup: DatabaseWrapper) -> DatabaseSnapshot:
    return db_setup.get_all_data()


if __name__ == "__main__":
    MONGODB_URL = os.getenv("MONGODB_URL")
    db = DatabaseWrapper(db_name="Site_Data", mongo_url=MONGODB_URL)

    (
        articles,
        categories,
        faculty,
    ) = db.get_all_data()

    with open("article_data.json", "w") as f:
        json.dump(articles, f, indent=4)
    with open("category_data.json", "w") as f:
        json.dump(categories, f, indent=4)
    with open("faculty_data.json", "w") as f:
        json.dump(faculty, f, indent=4)

            ```

        src/academic_metrics/__init__.py:
        ```
from . import (
    DB,
    ai_data_models,
    constants,
    core,
    data_collection,
    enums,
    mapping,
    orchestrators,
    strategies,
    utils,
)
from .core import CategoryProcessor
from .data_collection.CrossrefWrapper import CrossrefWrapper
from .data_collection.scraper import Scraper
from .dataclass_models.concrete_dataclasses import (
    CategoryInfo,
    CrossrefArticleDetails,
    CrossrefArticleStats,
    FacultyInfo,
    FacultyStats,
    GlobalFacultyStats,
)
from .enums import AttributeTypes
from .factories import (
    DataClassFactory,
    ClassifierFactory,
    StrategyFactory,
)

# Expose commonly used classes/functions at the package level
from .utils import Taxonomy, Utilities, WarningManager
from .other import TAXONOMY_AS_STRING

        ```

            src/academic_metrics/ai_data_models/__init__.py:
            ```
from .ai_pydantic_models import (
    AbstractSentenceAnalysis,
    AbstractSummary,
    ClassificationOutput,
    MethodExtractionOutput,
    ThemeAnalysis,
)

            ```

            src/academic_metrics/ai_data_models/ai_pydantic_models.py:
            ```
from typing import List

from pydantic import BaseModel, Field


class Feedback(BaseModel):
    """Model for storing feedback about AI assistant interactions.

    Attributes:
        assistant_name (str): Name of the AI assistant providing feedback
        feedback (str): The actual feedback content
    """

    assistant_name: str
    feedback: str


class Classification(BaseModel):
    """Model for storing category classifications.

    Attributes:
        categories (List[str]): List of assigned category names
            Type: List[str]
    """

    categories: List[str]


class MethodDetail(BaseModel):
    """Model for storing detailed information about extracted methods.

    Attributes:
        reasoning (str): Explanation for why this method was identified
        passages (List[str]): Relevant text passages supporting the method identification
            Type: List[str]
        confidence_score (float): Confidence level in the method identification
            Type: float
    """

    reasoning: str
    passages: List[str]
    confidence_score: float


class MethodExtractionOutput(BaseModel):
    """Model for storing the final output of method extraction.

    Attributes:
        methods (List[str]): List of extracted research methods
            Type: List[str]
    """

    methods: List[str]


class SentenceDetails(BaseModel):
    """Model for storing detailed analysis of individual sentences.

    Attributes:
        sentence (str): The original sentence text
        meaning (str): Interpreted meaning of the sentence
        reasoning (str): Explanation for the interpretation
        confidence_score (float): Confidence level in the analysis
            Type: float
    """

    sentence: str
    meaning: str
    reasoning: str
    confidence_score: float


class AbstractSentenceAnalysis(BaseModel):
    """Model for storing complete sentence-by-sentence analysis of an abstract.

    Attributes:
        sentence_details (List[SentenceDetails]): Detailed analysis of each sentence
            Type: List[:class:`academic_metrics.ai_data_models.ai_pydantic_models.SentenceDetails`]
        overall_theme (str): Main theme identified from all sentences
        summary (str): Brief summary of the entire analysis
    """

    sentence_details: List[SentenceDetails]
    overall_theme: str
    summary: str


class AbstractSummary(BaseModel):
    """Model for storing the condensed summary of an abstract.

    Attributes:
        summary (str): Condensed version of the abstract maintaining key points
    """

    summary: str


class ClassificationOutput(BaseModel):
    """Model for storing the output of taxonomy classification.

    Attributes:
        classifications (List[Classification]): List of category classifications
            Type: List[:class:`academic_metrics.ai_data_models.ai_pydantic_models.Classification`]
    """

    classifications: List[Classification]


class IndividualThemeAnalysis(BaseModel):
    """Model for storing detailed analysis of a single theme.

    Attributes:
        theme (str): The theme being analyzed
        reasoning (str): Detailed reasoning for theme identification
        confidence_score (float): Confidence level in theme identification
            Type: float
        supporting_passages (List[str]): Text passages supporting theme identification
            Type: List[str]
        abstract_summary_alignment (str): Theme alignment with abstract summary
        methodologies_justification (str): Justification based on abstract methodologies
    """

    theme: str = (Field(..., description="The theme you are analyzing"),)
    reasoning: str = (
        Field(
            ...,
            description="Detailed reasoning for why this theme is present in the abstract",
        ),
    )
    confidence_score: float = (
        Field(..., description="Confidence score for the identified theme"),
    )
    supporting_passages: List[str] = (
        Field(
            ...,
            description="List of passages from the abstract which support the identification of this theme",
        ),
    )
    abstract_summary_alignment: str = (
        Field(..., description="How this theme aligns with the abstract summary"),
    )
    methodologies_justification: str = Field(
        ...,
        description="A justification for why this identified theme was not selected due to the methodologies present in the abstract",
    )


class ThemeAnalysis(BaseModel):
    """Model for storing the complete theme analysis of an abstract.

    Attributes:
        themes (List[str]): List of all identified themes
            Type: List[str]
    """

    themes: List[str] = (
        Field(..., description="List of all themes identified in the abstract"),
    )

            ```

            src/academic_metrics/ai_prompts/__init__.py:
            ```
from .abstract_summary_prompts import (
    ABSTRACT_SUMMARY_SYSTEM_MESSAGE,
    SUMMARY_JSON_STRUCTURE,
)
from .classification_prompts import (
    CLASSIFICATION_SYSTEM_MESSAGE,
    TAXONOMY_EXAMPLE,
    CLASSIFICATION_JSON_FORMAT,
)
from .human_prompt import HUMAN_MESSAGE_PROMPT
from .method_prompts import (
    METHOD_EXTRACTION_SYSTEM_MESSAGE,
    METHOD_JSON_FORMAT,
    METHOD_EXTRACTION_CORRECT_EXAMPLE_JSON,
    METHOD_EXTRACTION_INCORRECT_EXAMPLE_JSON,
)
from .sentence_analysis_prompts import (
    ABSTRACT_SENTENCE_ANALYSIS_SYSTEM_MESSAGE,
    SENTENCE_ANALYSIS_JSON_EXAMPLE,
)
from .theme_prompts import (
    THEME_RECOGNITION_SYSTEM_MESSAGE,
    THEME_RECOGNITION_JSON_FORMAT,
)

            ```

            src/academic_metrics/ai_prompts/abstract_summary_prompts.py:
            ```
ABSTRACT_SUMMARY_SYSTEM_MESSAGE: str = """
You are an expert AI researcher tasked with summarizing academic research abstracts. Your task is to analyze the abstract and extract the main ideas and themes. The summary should focus on what the research is doing rather than how it is doing it; do not include specific methods used to conduct the research.

To assist you, the following data is provided:

1. **Methodologies:**

   - A previous AI assistant has extracted methodologies from the abstract.
   - For each methodology, it provides:
     - Reasoning for why it identified it as a methodology.
     - The passage(s) from the abstract supporting its identification.
     - A confidence score for its identification.
   - Here is the format of the methodologies assistant's output:
     {METHOD_JSON_FORMAT}

2. **Abstract Sentence Level Analysis:**

   - Another assistant has analyzed each sentence in the abstract.
   - For each sentence, it provides:
     - The identified meaning.
     - The reasoning for the identified meaning.
     - A confidence score.
   - It also provides:
     - An overall theme of the abstract.
     - A detailed summary of the abstract.
   - Here is the format of the abstract sentence level analysis assistant's output:
     {SENTENCE_ANALYSIS_JSON_EXAMPLE}

**Outputs from Previous Assistants:**

- **Methodologies Assistant Output:**
  {method_json_output}

- **Abstract Sentence Level Analysis Assistant Output:**
  {sentence_analysis_output}

### Your Output Should Contain:

- **summary:** A detailed summary of the abstract that captures the main idea of the research without focusing on the specific methods used.
- **reasoning:** A detailed explanation for the summary you have provided.
- **feedback_for_methodologies_assistant:** Specific feedback on any issues that affected your ability to accurately summarize the abstract, and any requests for improvements in their analysis. If you have no feedback, simply provide "The analysis is correct and sufficient, I have no feedback at this time."
- **feedback_for_abstract_sentence_level_analysis_assistant:** Specific feedback on any issues that affected your ability to accurately summarize the abstract, and any requests for improvements in their analysis. If you have no feedback, simply provide "The analysis is correct and sufficient, I have no feedback at this time."

### Steps to Complete Your Task:

1. Carefully read and understand the methodologies identified by the previous assistant.
2. Carefully read and understand the sentence-level analysis of the abstract provided by the previous assistant.
3. Carefully read and understand the abstract as a whole.
4. Form a detailed summary of the abstract that captures the main idea of the research without focusing on specific methods.

### Output Format:

Your output should be a JSON object with the following structure:

{SUMMARY_JSON_STRUCTURE}

**Important Notes:**

- **Focus on the Main Idea:**

  - Your summary should focus on the main idea of the research without including specific methods.
  - If your summary mentions methodologies used, you are not following the instructions.

- **Provide Specific Feedback:**

  - Ensure that your feedback is specific and helpful to the methodologies assistant and the abstract sentence-level analysis assistant.
  - Do not provide feedback for the sake of it; only include feedback that will help them improve their analysis.

- **JSON Output Only:**

  - Do not include the markdown JSON code block notation in your response.
  - Simply return the JSON object without surrounding it with ```json and ```.

- **Properly Escape Special Notations:**

  - If you use any special notation (e.g., LaTeX) within the JSON values, ensure it is properly escaped.
  - Failure to do so may result in a JSON parsing error, which is considered a critical failure.

- **Compliance is Mandatory:**

  - You must return the output in the specified JSON format.
  - Failure to do so will be considered a failure to complete the task.

"""

SUMMARY_JSON_STRUCTURE: str = """
{
    "summary": "<Detailed summary of the abstract>",
}
"""

            ```

            src/academic_metrics/ai_prompts/classification_prompts.py:
            ```
CLASSIFICATION_SYSTEM_MESSAGE: str = """
You are an expert in topic classification of research paper abstracts. Your task is to classify the provided abstract into one or more of the specified categories. Use only the categories provided; do not create new ones. Focus on capturing the main idea of the research, not the specific methods used, unless the methods are central to the research or provide essential context.

## Categories You Can Classify the Abstract Into:

{categories}

### Taxonomy Item Example:

Use this example to understand the academic nature of the categories.

{TAXONOMY_EXAMPLE}

### Additional Information:

Previous AI assistants have processed the abstract to provide extra context. Here is their output:

### Methodologies:

Extracted methodologies from the abstract.

Methodologies Format Example:
{METHOD_JSON_FORMAT}

Output:
{method_json_output}

### Abstract Summary:

An overall in-depth summary of the abstract.

Includes:
- Summary.

Abstract Summary Format Example:
{SUMMARY_JSON_STRUCTURE}

Output:
{abstract_summary_output}

## Steps to Follow:

1. Understand the Categories:
- Carefully read and comprehend the provided categories.
- Remember, these are ACADEMIC RESEARCH CATEGORIES (e.g., â€œeducationâ€ involves research around education).

2. Review Additional Information:
- Examine the outputs from previous assistants.
- Use this information to gain a deeper understanding of the abstract.

3. Classify the Abstract:
- Assign the abstract to one or more of the provided categories.

Output Format:

Your output must be a JSON object with the following structure:

{CLASSIFICATION_JSON_FORMAT}

Important Notes:
- Use Only Provided Categories:
- Do not create new categories.
- Label categories exactly as they appear in the list.
- Focus on Research Themes:
- Base your classification on the themes of the research described in the abstract.
- Do not focus on specific methods unless they are central to the research.
- Your response should be only the JSON object following the provided structure.
- Do not include markdown code block notation or additional text.
- Properly Escape Special Notations:
  - If using special notations (e.g., LaTeX) within JSON values, ensure they are properly escaped to prevent parsing errors.

## Compliance is Critical:

Failure to follow the instructions and output format is considered a critical failure.
"""

TAXONOMY_EXAMPLE: str = """
'Education': {
    'Education leadership and administration': [
        'Educational leadership and administration, general',
        "Higher education and community college administration",
        "Education leadership and administration nec"
    ],
    'Education research': [
        'Curriculum and instruction',
        'Educational assessment, evaluation, and research methods',
        'Educational/ instructional technology and media design',
        'Higher education evaluation and research',
        'Student counseling and personnel services',
        'Education research nec'
    ],
    "Teacher education": [
        "Adult, continuing, and workforce education and development",
        "Bilingual, multilingual, and multicultural education",
        "Mathematics teacher education",
        "Music teacher education",
        "Special education and teaching",
        "STEM educational methods",
        "Teacher education, science and engineering",
        "Teacher education, specific levels and methods",
        "Teacher education, specific subject areas"
    ],
    "Education, other": [
        "Education, general",
        "Education nec"
    ]
}
"""

CLASSIFICATION_JSON_FORMAT: str = """
{
    "classifications": [
        {
            "categories": [
                "<first category you decided to classify the abstract into>",
                "<second category you decided to classify the abstract into>",
                "<third category you decided to classify the abstract into>"
            ]
        }
    ]
}
"""

            ```

            src/academic_metrics/ai_prompts/human_prompt.py:
            ```
HUMAN_MESSAGE_PROMPT: str = """
## Abstract:
{abstract}
## Extra Context:
{extra_context}
"""

            ```

            src/academic_metrics/ai_prompts/method_prompts.py:
            ```
METHOD_EXTRACTION_SYSTEM_MESSAGE: str = """
You are a method extraction AI whose purpose is to identify and extract method keywords from an academic abstract. Your role is to locate the specific methodologies, techniques, or approaches mentioned in the abstract and provide them in the JSON format specified.

### Definition of Methods:

- "Methods" refers to the **specific processes**, **techniques**, **procedures**, or **approaches** used in conducting the research. This includes techniques for data collection, data analysis, algorithms, experimental procedures, or any other specific methodology employed by the researchers. Methods should **not** include general descriptions, conclusions, or research themes.

### What You Should Do:

1. **Ponder** the meaning of methods and what they refer to in the context of a research paper.
2. **Extract** keywords that refer to the **methods** used in the abstract.
3. **Present** the results in the required **JSON format** with a list of methods identified.

### JSON Output Requirements:

- **Response Format**: You must return your output as a JSON object.
- The JSON object must contain:
  - A key `"methods"` whose value is a list of extracted **method keywords**.

### JSON Structure you must follow:

{METHOD_JSON_FORMAT}

### Examples:

**Abstract:**

â€œDrawing on expectation states theory and expertise utilization literature, we examine the effects of team membersâ€™ actual expertise and social status on the degree of influence they exert over team processes via perceived expertise. We also explore the conditions under which teams rely on perceived expertise versus social status in determining influence relationships in teams. To do so, we present a contingency model in which the salience of expertise and social status depends on the types of intragroup conflicts. **Using multiwave survey data from 50 student project teams with 320 members** at a large national research institute located in South Korea, we found that both actual expertise and social status had direct and indirect effects on member influence through perceived expertise. Furthermore, perceived expertise at the early stage of team projects is driven by social status, whereas perceived expertise at the later stage of a team project is mainly driven by actual expertise. Finally, we found that members who are being perceived as experts are more influential when task conflict is high or when relationship conflict is low. We discuss the implications of these findings for research and practice.â€

#### Example 1: Correct Extraction for the Abstract Above

**Output:**

{METHOD_EXTRACTION_CORRECT_EXAMPLE_JSON}

**Explanation for Correct Extraction:**

- **"Multiwave survey data collection"**:
  - **Why this is a method**: This is a method because it refers to how data was gathered from research subjects over multiple time points.

- **"Contingency modeling"**:
  - **Why this is a method**: This is a method because it describes the analytical process used to explore relationships between variables like expertise and social status.
  
#### Example 2: Incorrect Extraction for the Abstract Above

**Incorrect Output:**

{METHOD_EXTRACTION_INCORRECT_EXAMPLE_JSON}

**Explanation for Incorrect Extraction:**

- **"Intragroup conflict"**:
  - This is incorrect because it is a variable or condition examined in the research, not a method.
- **"Perceived expertise"**:
  - This is incorrect because it is a measured variable, not a method.
- **"Social status"**:
  - This is incorrect because it is a variable the study investigates, not a methodological process.

**Important Notes:**

- **JSON Output Only**:
  - Do not include the markdown JSON code block notation in your response.
  - Simply return the JSON object, do not surround it with ```json and ```.

- **Properly Escape Special Notations**:
  - If you use any special notation (e.g., LaTeX) within the JSON values, ensure it is properly escaped.
  - Failure to do so may result in a JSON parsing error, which is considered a critical failure.

- **Compliance is Mandatory**:
  - You must return the output in the specified JSON format.
  - Failure to do so will be considered a failure to complete the task.

"""

METHOD_JSON_FORMAT: str = """
{
    "methods": [
        "<method_keyword_1>",
        "<method_keyword_2>"
    ]
}
"""


METHOD_EXTRACTION_CORRECT_EXAMPLE_JSON: str = """
{
    "methods": [
        "multiwave survey data collection",
        "contingency modeling"
    ]
}
"""

METHOD_EXTRACTION_INCORRECT_EXAMPLE_JSON: str = """    
{
    "methods": [
        "intragroup conflict",
        "perceived expertise",
        "social status",
        "multiwave survey data collection"
    ]
}
"""

            ```

            src/academic_metrics/ai_prompts/sentence_analysis_prompts.py:
            ```
ABSTRACT_SENTENCE_ANALYSIS_SYSTEM_MESSAGE: str = """
You are tasked with analyzing an abstract of a research paper. Your task involves the following steps:

Steps to follow:
1. **Record each sentence in the abstract**: 
2. For each sentence do the following steps: 
    - Determine the meaning of the sentence
    - Provide a reasoning for your interpretation
    - Assign a confidence score between 0 and 1 based on how confident you are in your assessment.
3. After you have done this for each sentence, determine the overall theme of the abstract. This should be a high-level overview of the main idea of the research.
4. Provide a detailed summary of the abstract. This should be a thorough overview of the research, including the main idea, the methods used, and the results.
       
Your output should follow this structure:

{SENTENCE_ANALYSIS_JSON_EXAMPLE}

IMPORTANT: Be concise but clear in your meanings and reasonings.
IMPORTANT: Ensure that the confidence score reflects how certain you are about the meaning of the sentence in context.
IMPORTANT: Do not include the markdown json code block notation in your response. Simply return the JSON object. The markdown json code block notation is: ```json\n<your json here>\n```, do not include the ```json\n``` in your response.
IMPORTANT: If within the values to the keys in the json, you use any other notation such as **Latex** ensure you properly escape. If you do not then the JSON will not be able to be parsed, which is a **critical failure**.
IMPORTANT: You must return the output in the specified JSON format. If you do not return the output in the specified JSON format, you have failed.
"""

SENTENCE_ANALYSIS_JSON_EXAMPLE: str = """
    {
      "sentence_details": [
        {
          "sentence": "Original sentence 1",
          "meaning": "Meaning of the sentence.",
          "reasoning": "Why this is the meaning of the sentence.",
          "confidence_score": Confidence score (0.0 - 1.0)
        },
        {
          "sentence": "Original sentence 2",
          "meaning": "Meaning of the sentence.",
          "reasoning": "Why this is the meaning of the sentence.",
          "confidence_score": Confidence score (0.0 - 1.0)
        }
      ],
      "overall_theme": "Overall theme of the abstract",
      "summary": "Detailed summary of the abstract"
    }
"""

            ```

            src/academic_metrics/ai_prompts/theme_prompts.py:
            ```
THEME_RECOGNITION_SYSTEM_MESSAGE: str = """
You are an AI assistant who is an expert in recognizing themes present in research paper abstracts. Your task is to identify the main themes present in the abstract. A theme is a main idea or central concept that the research is exploring; it should not be driven by the specific methods used to conduct the research.

Previous AI assistants have processed the abstract in the following ways:

- **Identifying and Extracting Methodologies Used in the Research**
- **Creating a Summary of the Abstract**
- **Classifying the Abstract into a Hierarchical Taxonomy**

You will be provided with the outputs from these previous AI assistants.

### How to Use the Provided Information:

- **Methodologies:**

  - Use the extracted methodologies to be aware of the methods present in the abstract.
  - This helps ensure your focus is on the themes related to the overall purpose of the research rather than the specific methods.

- **Abstract Summary:**

  - Use the summary to understand the main points of the abstract.

- **Categories (Hierarchical Taxonomy):**

  - Use the categories and their hierarchical components to understand where this abstract fits into the academic landscape.

### Your Task:

- Identify the main themes present in the abstract.
- First, determine if the abstract aligns with any of the provided themes (categories).
- If you identify additional themes not covered by the current categories, add them to your output.

### Provided Outputs:

#### Methodologies:

- **Format of the Methodologies Assistant's Output:**

  {METHOD_JSON_FORMAT}

- **Output from the Methodologies Assistant:**

  {method_json_output}

#### Abstract Summary:

- **Format of the Abstract Summary Assistant's Output:**

  {SUMMARY_JSON_STRUCTURE}

- **Output from the Abstract Summary Assistant:**

  {abstract_summary_output}

#### Categories the Abstract Has Been Classified Into:

**Note**: Top means top-level category, Mid means mid-level category, and Low means low-level category. The levels refer to the hierarchy of the categories, it does not imply any ranking of relevance or importance, all are equally important.

{categories}

### Output Format:

Your output should be a JSON object following the provided structure:

{THEME_RECOGNITION_JSON_FORMAT}

**Important Notes:**

- **Focus on Identifying Main Themes:**

  - Concentrate on the central ideas of the research, not the specific methods.
  - Use keywords as a guide but do not rely solely on them.

- **Use of Categories:**

  - Start by identifying any current themes the abstract aligns with.
  - If additional themes are identified, include them in your output.

- **JSON Output Requirements:**

  - Your output must be a JSON object following the provided structure exactly.
  - Do not change any keys or create your own keys.
  - Fill in all the values for each key, even if some are empty strings.

- **Formatting:**

  - Do not include the markdown JSON code block notation in your response.
  - Simply return the JSON object.

- **Special Notations:**

  - If you use any special notation (e.g., LaTeX) within the JSON values, ensure it is properly escaped to avoid parsing errors, which are considered a critical failure.

"""

THEME_RECOGNITION_JSON_FORMAT: str = """
{
    "themes": ["<list of all the themes you identified to be present in the abstract>"]
}
"""

            ```

            src/academic_metrics/configs/__init__.py:
            ```
from .global_config import (
    # Main functions
    configure_logging,
    set_log_to_console,
    # Configuration flags
    LOG_TO_CONSOLE,
    LOG_LEVEL,
    # Log levels for use in other modules
    DEBUG,
    INFO,
    WARNING,
    ERROR,
    CRITICAL,
)

            ```

            src/academic_metrics/configs/global_config.py:
            ```
import logging
from typing import Dict, Any, cast
import warnings
import os

from academic_metrics.constants import LOG_DIR_PATH, RELEASE_MODE

LOG_TO_CONSOLE = False
LOG_LEVEL = logging.INFO

if RELEASE_MODE:
    LOG_TO_CONSOLE = True
    LOG_LEVEL = logging.WARNING
else:
    LOG_TO_CONSOLE = True
    LOG_LEVEL = logging.DEBUG


class ColorFormatter(logging.Formatter):
    """Custom formatter that adds colors to log levels

    Attributes:
        COLOR_MAP (Dict[str, str]): A dictionary mapping log levels to their corresponding colors.

    Methods:
        Public Methods:
            format: Format the log record with colors.
    """

    COLOR_MAP: Dict[str, str] = {
        "DEBUG": "\033[36m",  # Cyan
        "INFO": "\033[32m",  # Green
        "WARNING": "\033[33m",  # Yellow
        "ERROR": "\033[31m",  # Red
        "CRITICAL": "\033[41m",  # Red background
        "RESET": "\033[0m",  # Reset color
    }

    def format(self, record: logging.LogRecord) -> str:
        """Format the log record with colors

        Args:
            record (logging.LogRecord): The log record to format.

        Returns:
            str: The formatted log record with colors.
        """
        # Color only for console output (StreamHandler)
        for handler in logging.getLogger(record.name).handlers:
            if type(handler) is logging.StreamHandler:
                levelname = record.levelname
                if levelname in self.COLOR_MAP:
                    record.levelname = f"{self.COLOR_MAP[levelname]}{levelname}{self.COLOR_MAP['RESET']}"
                break
        return super().format(record)


# Keep track of configured loggers to
# avoid re-configuring the same logger multiple times
# for seperate class instances.
#
# Implements a singleton pattern for loggers of each unique name.
#
# Results in only one logger instance per unique name (i.e. a class object)
# per runtime instance of the program.
_configured_loggers: Dict[str, logging.Logger] = {}

# Log levels for export across the package
# so that other modules can use them without
# needing to import the logging module.
DEBUG = logging.DEBUG
INFO = logging.INFO
WARNING = logging.WARNING
ERROR = logging.ERROR
CRITICAL = logging.CRITICAL

# Used to validate the log_level argument is
# a valid python logging log level
# in the configure_logging() function.
VALID_LOG_LEVELS = {DEBUG, INFO, WARNING, ERROR, CRITICAL}

# Configure the config module's logger prior to
# any calls to configure_logging() or set_log_to_console()
_config_logger = logging.getLogger(__name__)
_config_logger.setLevel(LOG_LEVEL)

if os.environ.get("READTHEDOCS") != "True":
    if not RELEASE_MODE:
        _config_log_file_path = LOG_DIR_PATH / "config.log"
        _file_handler = logging.FileHandler(_config_log_file_path)
        _file_handler.setLevel(LOG_LEVEL)
        _config_file_formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        _file_handler.setFormatter(_config_file_formatter)
        _config_logger.addHandler(_file_handler)

    _config_color_formatter = ColorFormatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    if LOG_TO_CONSOLE:
        _console_handler = logging.StreamHandler()
        _console_handler.setLevel(LOG_LEVEL)
        _console_handler.setFormatter(_config_color_formatter)
        _config_logger.addHandler(_console_handler)


def set_log_to_console(value: bool) -> None:
    """Set the global LOG_TO_CONSOLE variable.

    Void function which sets the global LOG_TO_CONSOLE to the provided boolean value
    or issues a warning and leaves the current value unchanged if the provided value is not a boolean.

    Args:
        value (bool): The new boolean True/False value for LOG_TO_CONSOLE.

    Warning:
        If the value is not a boolean, a warning is issued and the current value remains unchanged.
    """
    global LOG_TO_CONSOLE
    if not isinstance(cast(Any, value), bool):
        # The stacklevel=2 argument specifies the level in the stack trace where the warning originates.
        # By setting stacklevel to 2, the warning will point to the caller of the function that issued the warning,
        # rather than the line inside the function itself. This makes it easier for the individual who called the function to locate the source of the issue in their code.
        warnings.warn(
            "LOG_TO_CONSOLE must be a boolean value",
            f"Current `LOG_TO_CONSOLE` value of: {LOG_TO_CONSOLE} will remain unchanged",
            stacklevel=2,
        )
        return

    LOG_TO_CONSOLE = value

    # Update this config logger's console handler
    if LOG_TO_CONSOLE == False:
        # Find and remove any console handlers
        for handler in _config_logger.handlers[
            :
        ]:  # Copy list to avoid modification during iteration
            # If the handler is a StreamHandler (log to console handler)
            # and not a FileHandler (log to file handler)
            if isinstance(handler, logging.StreamHandler) and not isinstance(
                handler, logging.FileHandler
            ):
                _config_logger.removeHandler(handler)
                _config_logger.info(f"Removed console handler: {handler}")
                _config_logger.debug(f"Removed handler is of type: {type(handler)}")
                _config_logger.debug(f"Current handlers: {_config_logger.handlers}")


def configure_logging(
    module_name: str,
    log_file_name: str | None = None,
    log_level: int | None = LOG_LEVEL,
    force: bool | None = False,
) -> logging.Logger:
    """Configure a logger for a specific module.

    Configures logging for a module if not already configured.
    Acts as a singleton per module_name.

    Args:
        module_name (str): The name of the module to configure logging for.
        - This should be passed in as the `__name__` variable of the module.

        log_file_name (str): The name of the log file to use for the module.
        - This should be a valid file name with no file extension.

        - It should only be the file name desired for that module, not the full path.

        log_level (int): The log level to use for the module.
        - This should be a valid python logging log level.

        force (bool): Whether to force the creation of a new logger instance.
        - If a logger instance for the module already exists and `force` is False, the existing instance will be returned.

        - If a logger instance for the module already exists and `force` is True, a new instance will be created.

    Returns:
        logging.Logger: The configured logger for the module.

    """
    if module_name in _configured_loggers and not force:
        _config_logger.debug(
            f"Logger for module `{module_name}` already configured. "
            "Returning existing instance."
            "To create a new instance, set `force=True`."
        )
        return _configured_loggers[module_name]
    elif module_name in _configured_loggers and force:
        _config_logger.debug(
            f"Logger for module `{module_name}` already configured. "
            f"But `force` flag is set to `{force}`. "
            "Therefore, creating a new instance."
        )
        _configured_loggers[module_name] = None

    if log_file_name is None:
        # The stacklevel=2 argument specifies the level in the stack trace where the warning originates.
        # By setting stacklevel to 2, the warning will point to the caller of the function that issued the warning,
        # rather than the line inside the function itself. This makes it easier for the individual who called the function to locate the source of the warning in their code.
        # And determine if they meant not to pass in log_file_name.
        _config_logger.info(
            "`log_file_name` was not provided. "
            f"`log_file_name` is of value: `{log_file_name}`. "
            f"It will be replaced with: `{module_name}`.",
            stacklevel=2,
        )
        log_file_name = module_name

    # Validate the log_level argument is a valid log level
    if log_level is not None:
        if log_level not in VALID_LOG_LEVELS:
            _config_logger.warning(
                f"Invalid log level provided: `{log_level}`. "
                f"Must be one of: {VALID_LOG_LEVELS}. "
                f"Using default log level: `{LOG_LEVEL}` instead.",
                stacklevel=2,
            )
            log_level = LOG_LEVEL

    _config_logger.info(f"Creating new logger configuration for {module_name}")

    logger = logging.getLogger(module_name)
    logger.setLevel(log_level)

    # Prevent the logger from propagating to the root logger.
    # This should avoid duplicate log messages in the console.
    logger.propagate = False

    console_formatter = ColorFormatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    file_formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    if not RELEASE_MODE:
        log_file_path = LOG_DIR_PATH / f"{log_file_name}.log"
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
        file_handler = logging.FileHandler(log_file_path)
        file_handler.setLevel(log_level)
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)

    if LOG_TO_CONSOLE:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(log_level)
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)
        _config_logger.info(f"Added console handler for {module_name}")

    _configured_loggers[module_name] = logger
    return logger

            ```

            src/academic_metrics/constants/__init__.py:
            ```
from .dir_paths import (
    INPUT_FILES_DIR_PATH,
    LOG_DIR_PATH,
    RELEASE_MODE,
    OUTPUT_FILES_DIR_PATH,
    SPLIT_FILES_DIR_PATH,
    locate_academic_metrics_root,
    locate_src_root,
)

            ```

            src/academic_metrics/constants/dir_paths.py:
            ```
from pathlib import Path
import os
from typing import Optional, cast
import sys
import platformdirs

RELEASE_MODE = True


def get_package_data_dir() -> Path:
    """Get the appropriate data directory for the package based on platform."""
    return Path(platformdirs.user_data_dir("academic_metrics"))


def locate_academic_metrics_root(marker: str | None = "COSC425-DATA") -> Path:
    """Find the repository root directory.

    Args:
        marker (str | None): Marker string to search for in the directory hierarchy.
            Defaults to "COSC425-DATA".
    """
    current = Path(__file__).resolve()
    while current.parent != current:
        if current.name == marker:
            return current
        current = current.parent
    raise FileNotFoundError(
        f"Could not find repository root '{marker}'. "
        "Are you running from the correct directory?"
    )


def locate_src_root(repo_root: Path | None = None) -> Path:
    """Find the source code root directory (PythonCode).

    Args:
        repo_root (Path | None): Repository root directory.
            Defaults to None.
    """
    if cast(Optional[Path], repo_root) is None:
        repo_root = locate_academic_metrics_root()
    src_dir = repo_root / "src"
    if not src_dir.exists():
        raise FileNotFoundError(
            "Could not find source directory 'PythonCode'. "
            "Has the project structure changed?"
        )
    return src_dir


if os.environ.get("READTHEDOCS") == "True":
    # Dummy paths for documentation/import
    # Without this, the `locate_academic_metrics_root` will throw the FileNotFoundError
    # everywhere this file is imported.
    _PROJECT_ROOT = Path("/dummy/project")
    _SRC_ROOT = Path("/dummy/src")
    _ACADEMIC_METRICS_ROOT = Path("/dummy/academic_metrics")
    _DATA_ROOT = Path("/dummy/data")
    _DATA_CORE_ROOT = Path("/dummy/data/core")
    _DATA_OTHER_ROOT = Path("/dummy/data/other")
    SPLIT_FILES_DIR_PATH = Path("/dummy/data/core/crossref_split_files")
    INPUT_FILES_DIR_PATH = Path("/dummy/data/core/input_files")
    OUTPUT_FILES_DIR_PATH = Path("/dummy/data/core/output_files")
    _ACADEMIC_METRICS_PACKAGE_ROOT = Path("/dummy/academic_metrics")
    LOG_DIR_PATH = Path("/dummy/academic_metrics/logs")

elif getattr(sys, "frozen", False):
    # Handle PyInstaller case if needed

    _PROJECT_ROOT = Path(sys._MEIPASS)
    _DATA_ROOT = get_package_data_dir()

else:

    # System executing, set paths to actual locations

    try:
        # Try development paths first
        _PROJECT_ROOT = locate_academic_metrics_root()
        _SRC_ROOT = locate_src_root()
        _DATA_ROOT = _SRC_ROOT / "data"
    except FileNotFoundError:
        # Fallback to installed package paths
        _PROJECT_ROOT = Path(__file__).parent.parent.parent
        _DATA_ROOT = get_package_data_dir()

# Common path definitions that work for both dev and installed scenarios
if RELEASE_MODE:
    _DATA_CORE_ROOT = Path(".") / "core"
    _DATA_OTHER_ROOT = _DATA_ROOT / "other"
    SPLIT_FILES_DIR_PATH = _DATA_CORE_ROOT / "crossref_split_files"
    INPUT_FILES_DIR_PATH = _DATA_CORE_ROOT / "input_files"
    OUTPUT_FILES_DIR_PATH = _DATA_CORE_ROOT / "output_files"
    LOG_DIR_PATH = Path(".") / "logs"  # Changed to use current directory
else:
    _DATA_CORE_ROOT = _DATA_ROOT / "core"
    _DATA_OTHER_ROOT = _DATA_ROOT / "other"
    SPLIT_FILES_DIR_PATH = _DATA_CORE_ROOT / "crossref_split_files"
    INPUT_FILES_DIR_PATH = _DATA_CORE_ROOT / "input_files"
    OUTPUT_FILES_DIR_PATH = _DATA_CORE_ROOT / "output_files"
    LOG_DIR_PATH = _DATA_ROOT / "logs"

# Create directories if they don't exist
for path in [
    SPLIT_FILES_DIR_PATH,
    INPUT_FILES_DIR_PATH,
    OUTPUT_FILES_DIR_PATH,
    LOG_DIR_PATH,
]:
    if RELEASE_MODE:
        # In release mode, create directories in current working directory
        Path(path).mkdir(parents=True, exist_ok=True)
    else:
        # In development mode, use the original behavior
        path.parent.mkdir(parents=True, exist_ok=True)

            ```

            src/academic_metrics/core/__init__.py:
            ```
from .category_processor import CategoryProcessor

            ```

            src/academic_metrics/core/category_processor.py:
            ```
from __future__ import annotations

import logging
import os
import json
from typing import TYPE_CHECKING, Any, Dict, List, Set, Tuple
from urllib.parse import quote

from academic_metrics.configs import (
    configure_logging,
    LOG_TO_CONSOLE,
    DEBUG,
)
from academic_metrics.enums import AttributeTypes, DataClassTypes

if TYPE_CHECKING:
    from academic_metrics.dataclass_models import (
        CategoryInfo,
        FacultyStats,
        GlobalFacultyStats,
        CrossrefArticleStats,
        CrossrefArticleDetails,
    )
    from academic_metrics.utils import Utilities
    from academic_metrics.utils import WarningManager
    from academic_metrics.factories import DataClassFactory
    from academic_metrics.utils.taxonomy_util import Taxonomy


class CategoryProcessor:
    """Processes and organizes academic publication data by categories.

    This class handles the processing of classified publication data, organizing it into
    categories and generating various statistics. It manages faculty affiliations,
    article details, and category relationships.

    Args:
        None

    Attributes:
        utils (Utilities): Utility functions for data processing.
        warning_manager (WarningManager): System for handling and logging warnings.
        dataclass_factory (DataClassFactory): Factory for creating data model instances.
        taxonomy_util (Taxonomy): Utility for managing publication taxonomy.
        category_data (Dict[str, CategoryInfo]): Mapping of categories to their information.
        faculty_stats (Dict[str, FacultyStats]): Faculty statistics by category.
        global_faculty_stats (Dict[str, GlobalFacultyStats]): Global faculty statistics.
        category_article_stats (Dict[str, CrossrefArticleStats]): Article statistics by category.
        articles (List[CrossrefArticleDetails]): List of processed article details.
        logger (logging.Logger): Logger instance for this class.
        log_file_path (str): Path to the log file.

    Methods:
        process_data_list: Process a list of publication data items
        get_category_data: Get processed category data
        get_category_article_stats: Get article statistics by category
        get_articles: Get list of processed articles
        get_faculty_stats: Get faculty statistics by category
        get_global_faculty_stats: Get global faculty statistics
        call_get_attributes: Extract attributes from raw data
        update_category_stats: Update statistics for a category
        update_faculty_stats: Update faculty statistics
        update_global_faculty_stats: Update global faculty statistics
        update_category_article_stats: Update article statistics by category
        create_article_object: Create a new article object
        clean_faculty_affiliations: Clean faculty affiliation data
        clean_faculty_members: Clean faculty member data
        initialize_categories: Initialize category data structures
        _collect_all_affiliations: Collect all faculty affiliations
        _generate_url: Generate URL from string
        _generate_normal_id: Generate normalized ID from strings
    """

    def __init__(
        self,
        utils: Utilities,
        dataclass_factory: DataClassFactory,
        warning_manager: WarningManager,
        taxonomy_util: Taxonomy,
        log_to_console: bool | None = LOG_TO_CONSOLE,
    ) -> None:
        """Initialize the CategoryProcessor with required dependencies.

        Sets up logging configuration and initializes all required components for
        processing publication data, including utilities, factories, and data structures
        for storing category, faculty, and article information.

        Args:
            utils (Utilities): Utility functions for data processing.
                Type: :class:`academic_metrics.core.utilities.Utilities`
            dataclass_factory (DataClassFactory): Factory for creating data model instances.
                Type: :class:`academic_metrics.core.data_class_factory.DataClassFactory`
            warning_manager (WarningManager): System for handling and logging warnings.
                Type: :class:`academic_metrics.core.warning_manager.WarningManager`
            taxonomy_util (Taxonomy): Utility for managing publication taxonomy.
                Type: :class:`academic_metrics.core.taxonomy.Taxonomy`
            log_to_console (bool | None): Whether to log output to console.
                Type: bool | None
                Defaults to LOG_TO_CONSOLE.

        Raises:
            ValueError: If required dependencies are not properly initialized
            IOError: If log file cannot be created or accessed

        Notes:
            Initializes the following data structures:
            - category_data: Dictionary mapping categories to their information
            - faculty_stats: Dictionary tracking faculty statistics by category
            - global_faculty_stats: Dictionary tracking global faculty statistics
            - category_article_stats: Dictionary tracking article stats per category
            - articles: List of CrossrefArticleDetails objects for ground truth data
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="category_processor",
            log_level=DEBUG,
        )

        self.utils: Utilities = utils
        self.warning_manager: WarningManager = warning_manager
        self.dataclass_factory: DataClassFactory = dataclass_factory
        self.taxonomy_util: Taxonomy = taxonomy_util
        self.category_data: Dict[str, CategoryInfo] = {}

        # influential stats dictionaries
        self.faculty_stats: Dict[str, FacultyStats] = {}
        self.global_faculty_stats: Dict[str, GlobalFacultyStats] = {}

        # Seperately tracks article stats for each category the article is in
        # This gives the article stats for each article under a given category
        self.category_article_stats: Dict[str, CrossrefArticleStats] = {}

        # This creates a CrossrefArticleDetails object for each article
        # This gives an object per article so that we can have ground truth articles
        # This is what allows for category stats to just have a list of DOIs as those can
        # be used to look up the article details object for that doi from this list of CrossrefArticleDetails
        # objects.
        self.articles: List[CrossrefArticleDetails] = []

    def process_data_list(self, data: List[Dict]) -> None:
        """Process a list of publication data items.

        Takes raw publication data and processes each item through several stages:
        1. Extracts base attributes
        2. Initializes category information
        3. Generates URL maps for categories
        4. Cleans faculty and affiliation data
        5. Updates various statistics (category, faculty, article)
        6. Creates article objects

        Args:
            data (List[Dict]): List of raw publication data dictionaries to process.
                Type: List[Dict[str, Any]]

        Raises:
            ValueError: If required attributes are missing from data
            Exception: If category information cannot be initialized

        Notes:
            - Processes each publication through all stages sequentially
            - Updates multiple data structures during processing
            - Maintains relationships between categories, faculty, and articles
            - Performs data cleaning and normalization
        """
        self.logger.info("Starting to process data list...")
        for i, item in enumerate(data):
            self.logger.info(f"Processing item: {i + 1} / {len(data)}")

            # Get base attributes
            self.logger.info("Calling get_attributes...")
            raw_attributes = self.call_get_attributes(data=item)
            self.logger.info(f"Raw attributes: {raw_attributes}")

            # Get category information
            self.logger.info("Starting category initialization...")
            category_levels: Dict[str, List[str]] = self.initialize_categories(
                raw_attributes.get("categories", [])
            )
            self.logger.info(
                f"Category levels:\n{json.dumps(category_levels, indent=4)}"
            )
            self.logger.info(f"Completed category initialization.")

            # Fetch out seperate category levels
            self.logger.info("Fetching category levels...")
            top_categories: List[str] = category_levels.get("top_level_categories", [])
            self.logger.info(f"Top categories: {top_categories}")
            mid_categories: List[str] = category_levels.get("mid_level_categories", [])
            self.logger.info(f"Mid categories: {mid_categories}")
            low_categories: List[str] = category_levels.get("low_level_categories", [])
            self.logger.info(f"Low categories: {low_categories}")
            all_categories: List[str] = category_levels.get("all_categories", [])
            self.logger.info(f"All categories: {all_categories}")
            self.logger.info("Completed category level fetch.")

            # Create URL maps for each category level
            self.logger.info("Starting URL map creation...")
            top_level_url_map: Dict[str, str] = {}
            mid_level_url_map: Dict[str, str] = {}
            low_level_url_map: Dict[str, str] = {}
            self.logger.info("Completed URL map creation.")

            self.logger.info("Populating URL maps...")
            for category in all_categories:
                self.logger.info(f"Processing category: {category}")
                if category in low_categories:
                    self.logger.info("Category is in low categories.")

                    self.logger.info("Getting mid category for low category...")
                    mid_cat = self.taxonomy_util.get_mid_cat_for_low_cat(category)
                    self.logger.info(f"Mid category: {mid_cat}")

                    self.logger.info("Getting top category for mid category...")
                    top_cat = self.taxonomy_util.get_top_cat_for_mid_cat(mid_cat)
                    self.logger.info(f"Top category: {top_cat}")

                    self.logger.info("Generating URL for low category...")
                    low_level_url_map[category] = self._generate_url(
                        f"{top_cat}/{mid_cat}/{category}", self.logger
                    )
                    self.logger.info(f"Low level URL: {low_level_url_map[category]}")

                elif category in mid_categories:
                    self.logger.info("Category is in mid categories.")

                    self.logger.info("Getting top category for mid category...")
                    top_cat = self.taxonomy_util.get_top_cat_for_mid_cat(category)
                    self.logger.info(f"Top category: {top_cat}")

                    self.logger.info("Generating URL for mid category...")
                    mid_level_url_map[category] = self._generate_url(
                        f"{top_cat}/{category}", self.logger
                    )
                    self.logger.info(f"Mid level URL: {mid_level_url_map[category]}")

                else:
                    self.logger.info("Category is in top categories.")

                    self.logger.info("Generating URL for top category...")
                    top_level_url_map[category] = self._generate_url(
                        category, self.logger
                    )
                    self.logger.info(f"Top level URL: {top_level_url_map[category]}")

            self.logger.info(
                f"Top level URL map:\n{json.dumps(top_level_url_map, indent=4)}"
            )
            self.logger.info(
                f"Mid level URL map:\n{json.dumps(mid_level_url_map, indent=4)}"
            )
            self.logger.info(
                f"Low level URL map:\n{json.dumps(low_level_url_map, indent=4)}"
            )
            self.logger.info("Completed URL map population.")

            # Clean special fields
            self.logger.info("Cleaning faculty members...")
            faculty_members: List[str] = self.clean_faculty_members(
                raw_attributes.get("faculty_members", [])
            )
            self.logger.info(
                f"Cleaned faculty members:\n{json.dumps(faculty_members, indent=4)}"
            )
            self.logger.info("Cleaning faculty affiliations...")
            faculty_affiliations: Dict[str, List[str]] = (
                self.clean_faculty_affiliations(
                    raw_attributes.get("faculty_affiliations", [])
                )
            )
            self.logger.info(
                f"Cleaned faculty affiliations:\n{json.dumps(faculty_affiliations, indent=4)}"
            )
            self.logger.info("Collecting all affiliations...")
            all_affiliations: Set[str] = self._collect_all_affiliations(
                faculty_affiliations, logger=self.logger
            )
            self.logger.info(
                f"Collected all affiliations:\n{json.dumps(list(all_affiliations), indent=4)}"
            )

            # Unpack everything into kwargs
            self.logger.info("Unpacking everything into kwargs...")
            kwargs = {
                # Basic article info
                "title": raw_attributes.get("title", ""),
                "doi": raw_attributes.get("doi", ""),
                "tc_count": raw_attributes.get("tc_count", 0),
                "abstract": raw_attributes.get("abstract", ""),
                "license_url": raw_attributes.get("license_url", ""),
                "date_published_print": raw_attributes.get("date_published_print", ""),
                "date_published_online": raw_attributes.get(
                    "date_published_online", ""
                ),
                "journal": raw_attributes.get("journal", ""),
                "download_url": raw_attributes.get("download_url", ""),
                "themes": raw_attributes.get("themes", []),
                # Faculty and affiliations
                "faculty_members": faculty_members,
                "faculty_affiliations": faculty_affiliations,
                "all_affiliations": all_affiliations,
                # Category information
                "all_categories": all_categories,
                "top_level_categories": top_categories,
                "mid_level_categories": mid_categories,
                "low_level_categories": low_categories,
                "url_maps": {
                    "top": top_level_url_map,
                    "mid": mid_level_url_map,
                    "low": low_level_url_map,
                },
            }
            self.logger.info("Completed unpacking everything into kwargs.")

            self.logger.info("Updating category stats...")
            self.update_category_stats(**kwargs)
            self.logger.info("Completed updating category stats.")

            self.logger.info("Updating faculty stats...")
            self.update_faculty_stats(**kwargs)
            self.logger.info("Completed updating faculty stats.")

            self.logger.info("Updating global faculty stats...")
            self.update_global_faculty_stats(**kwargs)
            self.logger.info("Completed updating global faculty stats.")

            self.logger.info("Updating category article stats...")
            self.update_category_article_stats(**kwargs)
            self.logger.info("Completed updating category article stats.")

            self.logger.info("Creating article object...")
            self.create_article_object(**kwargs)
            self.logger.info("Completed creating article object.")

    def _test_category_processor(self, raw_attributes: Dict[str, Any]) -> None:
        """Test method for validating category processing functionality.

        This private method is used for testing the category processor's ability to handle
        raw attribute data and properly process it through the category system.

        Args:
            raw_attributes (Dict[str, Any]): Dictionary of raw attributes to test processing.
                Type: Dict[str, Any]

        Notes:
            - Used for internal testing purposes only
            - Validates category processing pipeline
            - Does not modify production data
            - Helps ensure data integrity
        """
        # Get base attributes
        self.logger.info("Calling get_attributes...")
        self.logger.info(f"Raw attributes: {raw_attributes}")

        # Get category information
        self.logger.info("Starting category initialization...")
        category_levels: Dict[str, List[str]] = self.initialize_categories(
            raw_attributes.get("categories", [])
        )
        self.logger.info(f"Category levels:\n{json.dumps(category_levels, indent=4)}")
        self.logger.info(f"Completed category initialization.")

        # Fetch out seperate category levels
        self.logger.info("Fetching category levels...")
        top_categories: List[str] = category_levels.get("top_level_categories", [])
        self.logger.info(f"Top categories: {top_categories}")
        mid_categories: List[str] = category_levels.get("mid_level_categories", [])
        self.logger.info(f"Mid categories: {mid_categories}")
        low_categories: List[str] = category_levels.get("low_level_categories", [])
        self.logger.info(f"Low categories: {low_categories}")
        all_categories: List[str] = category_levels.get("all_categories", [])
        self.logger.info(f"All categories: {all_categories}")
        self.logger.info("Completed category level fetch.")

        # Create URL maps for each category level
        self.logger.info("Starting URL map creation...")
        top_level_url_map: Dict[str, str] = {}
        mid_level_url_map: Dict[str, str] = {}
        low_level_url_map: Dict[str, str] = {}
        self.logger.info("Completed URL map creation.")

        self.logger.info("Populating URL maps...")
        for category in all_categories:
            self.logger.info(f"Processing category: {category}")
            if category in low_categories:
                self.logger.info("Category is in low categories.")

                self.logger.info("Getting mid category for low category...")
                mid_cat = self.taxonomy_util.get_mid_cat_for_low_cat(category)
                self.logger.info(f"Mid category: {mid_cat}")

                self.logger.info("Getting top category for mid category...")
                top_cat = self.taxonomy_util.get_top_cat_for_mid_cat(mid_cat)
                self.logger.info(f"Top category: {top_cat}")

                self.logger.info("Generating URL for low category...")
                low_level_url_map[category] = self._generate_url(
                    f"{top_cat}/{mid_cat}/{category}", self.logger
                )
                self.logger.info(f"Low level URL: {low_level_url_map[category]}")

            elif category in mid_categories:
                self.logger.info("Category is in mid categories.")

                self.logger.info("Getting top category for mid category...")
                top_cat = self.taxonomy_util.get_top_cat_for_mid_cat(category)
                self.logger.info(f"Top category: {top_cat}")

                self.logger.info("Generating URL for mid category...")
                mid_level_url_map[category] = self._generate_url(
                    f"{top_cat}/{category}", self.logger
                )
                self.logger.info(f"Mid level URL: {mid_level_url_map[category]}")

            else:
                self.logger.info("Category is in top categories.")

                self.logger.info("Generating URL for top category...")
                top_level_url_map[category] = self._generate_url(category, self.logger)
                self.logger.info(f"Top level URL: {top_level_url_map[category]}")

            self.logger.info(
                f"Top level URL map:\n{json.dumps(top_level_url_map, indent=4)}"
            )
            self.logger.info(
                f"Mid level URL map:\n{json.dumps(mid_level_url_map, indent=4)}"
            )
            self.logger.info(
                f"Low level URL map:\n{json.dumps(low_level_url_map, indent=4)}"
            )
            self.logger.info("Completed URL map population.")

            # Clean special fields
            self.logger.info("Cleaning faculty members...")
            faculty_members: List[str] = self.clean_faculty_members(
                raw_attributes.get("faculty_members", [])
            )
            self.logger.info(
                f"Cleaned faculty members:\n{json.dumps(faculty_members, indent=4)}"
            )
            self.logger.info("Cleaning faculty affiliations...")
            faculty_affiliations: Dict[str, List[str]] = (
                self.clean_faculty_affiliations(
                    raw_attributes.get("faculty_affiliations", [])
                )
            )
            self.logger.info(
                f"Cleaned faculty affiliations:\n{json.dumps(faculty_affiliations, indent=4)}"
            )
            self.logger.info("Collecting all affiliations...")
            all_affiliations: Set[str] = self._collect_all_affiliations(
                faculty_affiliations, logger=self.logger
            )
            self.logger.info(
                f"Collected all affiliations:\n{json.dumps(list(all_affiliations), indent=4)}"
            )

            # Unpack everything into kwargs
            self.logger.info("Unpacking everything into kwargs...")
            kwargs = {
                # Basic article info
                "title": raw_attributes.get("title", ""),
                "doi": raw_attributes.get("doi", ""),
                "tc_count": raw_attributes.get("tc_count", 0),
                "abstract": raw_attributes.get("abstract", ""),
                "license_url": raw_attributes.get("license_url", ""),
                "date_published_print": raw_attributes.get("date_published_print", ""),
                "date_published_online": raw_attributes.get(
                    "date_published_online", ""
                ),
                "journal": raw_attributes.get("journal", ""),
                "download_url": raw_attributes.get("download_url", ""),
                "themes": raw_attributes.get("themes", []),
                # Faculty and affiliations
                "faculty_members": faculty_members,
                "faculty_affiliations": faculty_affiliations,
                "all_affiliations": all_affiliations,
                # Category information
                "all_categories": all_categories,
                "top_level_categories": top_categories,
                "mid_level_categories": mid_categories,
                "low_level_categories": low_categories,
                "url_maps": {
                    "top": top_level_url_map,
                    "mid": mid_level_url_map,
                    "low": low_level_url_map,
                },
            }
            self.logger.info("Completed unpacking everything into kwargs.")

            self.logger.info("Updating category stats...")
            self.update_category_stats(**kwargs)
            self.logger.info("Completed updating category stats.")

            self.logger.info("Updating faculty stats...")
            self.update_faculty_stats(**kwargs)
            self.logger.info("Completed updating faculty stats.")

            self.logger.info("Updating global faculty stats...")
            self.update_global_faculty_stats(**kwargs)
            self.logger.info("Completed updating global faculty stats.")

            self.logger.info("Updating category article stats...")
            self.update_category_article_stats(**kwargs)
            self.logger.info("Completed updating category article stats.")

            self.logger.info("Creating article object...")
            self.create_article_object(**kwargs)
            self.logger.info("Completed creating article object.")

    def call_get_attributes(self, *, data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract and process attributes from raw publication data.

        Extracts various attributes including categories, authors, departments, titles,
        citations, abstracts, licenses, publication dates, journal info, URLs, DOIs,
        and themes from the raw data.

        Args:
            data (Dict[str, Any]): Raw publication data dictionary.
                Type: Dict[str, Any]

        Returns:
            Dict[str, Any]: Dictionary containing extracted and processed attributes.
                Type: Dict[str, Any]
                Contains:
                - categories (List[str]): List of publication categories
                - faculty_members (List[str]): List of faculty authors
                - faculty_affiliations (Dict[str, str]): Faculty to department mapping
                - title (str): Publication title
                - tc_count (int): Citation count
                - abstract (str): Publication abstract
                - license_url (str): License URL
                - date_published_print (str): Print publication date
                - date_published_online (str): Online publication date
                - journal (str): Journal name
                - download_url (str): Download URL
                - doi (str): Digital Object Identifier
                - themes (List[str]): List of publication themes

        Raises:
            Exception: If no category is found in the data

        Notes:
            - Extracts all available attributes from raw data
            - Performs basic validation of required fields
            - Handles missing optional fields gracefully
            - Maintains data types for each attribute
        """
        self.logger.info("Calling get_attributes...")

        attribute_results: Dict[AttributeTypes, Tuple[bool, Any]] = (
            self.utils.get_attributes(
                data,
                [
                    AttributeTypes.CROSSREF_CATEGORIES,
                    AttributeTypes.CROSSREF_AUTHORS,
                    AttributeTypes.CROSSREF_DEPARTMENTS,
                    AttributeTypes.CROSSREF_TITLE,
                    AttributeTypes.CROSSREF_CITATION_COUNT,
                    AttributeTypes.CROSSREF_ABSTRACT,
                    AttributeTypes.CROSSREF_LICENSE_URL,
                    AttributeTypes.CROSSREF_PUBLISHED_PRINT,
                    AttributeTypes.CROSSREF_PUBLISHED_ONLINE,
                    AttributeTypes.CROSSREF_JOURNAL,
                    AttributeTypes.CROSSREF_URL,
                    AttributeTypes.CROSSREF_DOI,
                    AttributeTypes.CROSSREF_THEMES,
                ],
            )
        )
        self.logger.info("Completed calling get_attributes.")

        self.logger.info("Checking if categories exist...")
        if attribute_results[AttributeTypes.CROSSREF_CATEGORIES][0]:
            categories: List[str] = attribute_results[
                AttributeTypes.CROSSREF_CATEGORIES
            ][1]
            self.logger.info(f"Got categories: {categories}")
        else:
            raise Exception(f"No category found for data: {data}")

        self.logger.info("Checking if faculty members exist...")
        faculty_members: List[str] | None = (
            attribute_results[AttributeTypes.CROSSREF_AUTHORS][1]
            if attribute_results[AttributeTypes.CROSSREF_AUTHORS][0]
            else None
        )
        self.logger.info(f"Got faculty members: {faculty_members}")

        self.logger.info("Checking if faculty affiliations exist...")
        faculty_affiliations: Dict[str, List[str]] | None = (
            attribute_results[AttributeTypes.CROSSREF_DEPARTMENTS][1]
            if attribute_results[AttributeTypes.CROSSREF_DEPARTMENTS][0]
            else None
        )
        self.logger.info(f"Got faculty affiliations: {faculty_affiliations}")

        self.logger.info("Checking if title exists...")
        title: str | None = (
            attribute_results[AttributeTypes.CROSSREF_TITLE][1]
            if attribute_results[AttributeTypes.CROSSREF_TITLE][0]
            else None
        )
        self.logger.info(f"Got title: {title}")

        self.logger.info("Checking if citation count exists...")
        tc_count: int | None = (
            attribute_results[AttributeTypes.CROSSREF_CITATION_COUNT][1]
            if attribute_results[AttributeTypes.CROSSREF_CITATION_COUNT][0]
            else None
        )
        self.logger.info(f"Got citation count: {tc_count}")

        self.logger.info("Checking if abstract exists...")
        abstract: str | None = (
            attribute_results[AttributeTypes.CROSSREF_ABSTRACT][1]
            if attribute_results[AttributeTypes.CROSSREF_ABSTRACT][0]
            else None
        )
        self.logger.info(f"Got abstract: {abstract}")

        self.logger.info("Checking if license URL exists...")
        license_url: str | None = (
            attribute_results[AttributeTypes.CROSSREF_LICENSE_URL][1]
            if attribute_results[AttributeTypes.CROSSREF_LICENSE_URL][0]
            else None
        )
        self.logger.info(f"Got license URL: {license_url}")

        self.logger.info("Checking if print publication date exists...")
        date_published_print: str | None = (
            attribute_results[AttributeTypes.CROSSREF_PUBLISHED_PRINT][1]
            if attribute_results[AttributeTypes.CROSSREF_PUBLISHED_PRINT][0]
            else None
        )
        self.logger.info(f"Got print publication date: {date_published_print}")

        self.logger.info("Checking if online publication date exists...")
        date_published_online: str | None = (
            attribute_results[AttributeTypes.CROSSREF_PUBLISHED_ONLINE][1]
            if attribute_results[AttributeTypes.CROSSREF_PUBLISHED_ONLINE][0]
            else None
        )
        self.logger.info(f"Got online publication date: {date_published_online}")

        self.logger.info("Checking if journal exists...")
        journal: str | None = (
            attribute_results[AttributeTypes.CROSSREF_JOURNAL][1]
            if attribute_results[AttributeTypes.CROSSREF_JOURNAL][0]
            else None
        )
        self.logger.info(f"Got journal: {journal}")

        self.logger.info("Checking if download URL exists...")
        download_url: str | None = (
            attribute_results[AttributeTypes.CROSSREF_URL][1]
            if attribute_results[AttributeTypes.CROSSREF_URL][0]
            else None
        )
        self.logger.info(f"Got download URL: {download_url}")

        self.logger.info("Checking if DOI exists...")
        doi: str | None = (
            attribute_results[AttributeTypes.CROSSREF_DOI][1]
            if attribute_results[AttributeTypes.CROSSREF_DOI][0]
            else None
        )
        self.logger.info(f"Got DOI: {doi}")

        self.logger.info("Checking if themes exist...")
        themes: List[str] | None = (
            attribute_results[AttributeTypes.CROSSREF_THEMES][1]
            if attribute_results[AttributeTypes.CROSSREF_THEMES][0]
            else None
        )
        self.logger.info(f"Got themes: {themes}")

        self.logger.info("Completed calling get_attributes.")

        self.logger.info("Returning attribute results...")
        return {
            "categories": categories,
            "faculty_members": faculty_members,
            "faculty_affiliations": faculty_affiliations,
            "title": title,
            "tc_count": tc_count,
            "abstract": abstract,
            "license_url": license_url,
            "date_published_print": date_published_print,
            "date_published_online": date_published_online,
            "journal": journal,
            "download_url": download_url,
            "doi": doi,
            "themes": themes,
        }

    def update_category_stats(self, **kwargs) -> None:
        """Update statistics for each category based on processed article data.

        Updates category information including faculty members, departments, titles,
        citation counts, DOIs, and themes. Also calculates derived statistics like
        faculty count, department count, article count, and citation averages.

        Args:
            **kwargs: Keyword arguments containing article data.
                Required arguments:
                - title (str): Article title
                    Type: str
                - doi (str): Digital Object Identifier
                    Type: str
                - tc_count (int): Citation count
                    Type: int
                - faculty_members (list): List of faculty authors
                    Type: List[str]
                - all_affiliations (set): Set of department affiliations
                    Type: Set[str]
                - themes (list): List of article themes
                    Type: List[str]
                - all_categories (list): List of all categories
                    Type: List[str]
                - url_maps (dict): Category URL mappings
                    Type: Dict[str, Dict[str, str]]

        Raises:
            KeyError: If required kwargs are missing
            ValueError: If category information cannot be updated

        Notes:
            - Updates multiple statistics per category
            - Calculates derived metrics from raw data
            - Maintains relationships between entities
            - Handles missing optional data gracefully
            - Updates both raw counts and computed averages
        """
        self.logger.info("Updating category stats...")
        for category in kwargs["all_categories"]:
            self.logger.info(f"Updating category stats for category: {category}")
            self.logger.info(f"Getting out category for URL via URL map...")
            url: str = (
                kwargs["url_maps"]["low"].get(category, "")
                or kwargs["url_maps"]["mid"].get(category, "")
                or kwargs["url_maps"]["top"].get(category, "")
            )
            self.logger.info(f"Retrieved URL: {url}")

            self.logger.info("Getting category info...")
            category_info: CategoryInfo = self.category_data[category]
            self.logger.info(f"Retrieved object type: {type(category_info)}")

            self.logger.info("Setting params for category info...")
            category_info.set_params(
                {
                    "_id": url,
                    "url": url,
                    "faculty": kwargs["faculty_members"],
                    "departments": kwargs["all_affiliations"],
                    "titles": kwargs["title"],
                    "tc_count": category_info.tc_count + kwargs["tc_count"],
                    "doi_list": kwargs["doi"],
                    "themes": kwargs["themes"],
                }
            )
            self.logger.info("Completed setting params for category info.")

            # Update counts based on set lengths after deduplication by the set_params() method
            self.logger.info(
                "Updating counts based on set lengths after deduplication..."
            )
            self.logger.info(f"Faculty count: {len(category_info.faculty)}")
            self.logger.info(f"Department count: {len(category_info.departments)}")
            self.logger.info(f"Article count: {len(category_info.titles)}")
            self.logger.info(f"Citation average: {category_info.citation_average}")
            category_info.faculty_count = len(category_info.faculty)
            category_info.department_count = len(category_info.departments)
            category_info.article_count = len(category_info.titles)
            category_info.citation_average = (
                category_info.tc_count / category_info.article_count
            )
            self.logger.info("Updated counts based on set lengths after deduplication.")
            self.logger.info(f"Updated faculty count: {category_info.faculty_count}")
            self.logger.info(
                f"Updated department count: {category_info.department_count}"
            )
            self.logger.info(f"Updated article count: {category_info.article_count}")
            self.logger.info(
                f"Updated citation average: {category_info.citation_average}"
            )
            self.logger.info("Completed updating category stats.")

    def update_faculty_stats(self, **kwargs) -> None:
        """Update faculty statistics for each category.

        Updates faculty member information including department affiliations,
        publication titles, DOIs, citation counts, and article counts. Creates
        or updates faculty statistics entries for each category.

        Args:
            **kwargs: Keyword arguments containing faculty and article data.
                Required arguments:
                - faculty_members (List): List of faculty authors
                    Type: List[str]
                - faculty_affiliations (Dict): Faculty department mappings
                    Type: Dict[str, List[str]]
                - title (str): Article title
                    Type: str
                - doi (str): Digital Object Identifier
                    Type: str
                - tc_count (int): Citation count
                    Type: int
                - all_categories (List): List of all categories
                    Type: List[str]
                - url_maps (Dict): Category URL mappings
                    Type: Dict[str, Dict[str, str]]

        Raises:
            KeyError: If required kwargs are missing
            ValueError: If faculty statistics cannot be updated

        Notes:
            - Updates statistics for each faculty member
            - Maintains faculty-department relationships
            - Tracks publication metrics per faculty
            - Handles multiple department affiliations
            - Updates both individual and aggregate statistics
        """
        self.logger.info("Updating faculty stats...")
        for category in kwargs["all_categories"]:
            self.logger.info(f"Updating faculty stats for category: {category}")

            self.logger.info("Checking if faculty stats for category exists...")
            if category not in self.faculty_stats:
                self.logger.info(
                    "Faculty stats for category does not exist, creating new..."
                )
                self.faculty_stats[category] = self.dataclass_factory.get_dataclass(
                    DataClassTypes.FACULTY_STATS,
                )
                self.logger.info("Created new faculty stats for category.")

            self.logger.info("Getting URL for category via URL map...")
            url: str = (
                kwargs["url_maps"]["low"].get(category, "")
                or kwargs["url_maps"]["mid"].get(category, "")
                or kwargs["url_maps"]["top"].get(category, "")
            )
            self.logger.info(f"Retrieved URL: {url}")

            self.logger.info("Updating faculty stats for each faculty member...")
            for faculty_member in kwargs["faculty_members"]:
                self.logger.info(
                    f"Updating faculty stats for faculty member: {faculty_member}"
                )

                self.logger.info(
                    "Generating normal ID for faculty member and category..."
                )
                faculty_data: Dict[str, Any] = {
                    "_id": self._generate_normal_id(
                        strings=[faculty_member, category], logger=self.logger
                    ),
                    "name": faculty_member,
                    "category": category,
                    "category_url": url,
                    "department_affiliations": kwargs["faculty_affiliations"].get(
                        faculty_member, []
                    ),
                    "titles": kwargs["title"],
                    "dois": kwargs["doi"],
                    "total_citations": kwargs["tc_count"],
                    "article_count": 1,
                    "doi_citation_map": {kwargs["doi"]: kwargs["tc_count"]},
                }

                self.logger.info("Setting params for faculty stats...")
                self.faculty_stats[category].set_params({faculty_member: faculty_data})
                self.logger.info("Completed setting params for faculty stats.")

        self.logger.info("Completed updating faculty stats.")

    def update_global_faculty_stats(self, **kwargs) -> None:
        """Update global statistics for each faculty member.

        Creates or updates global faculty statistics including total citations,
        article counts, department affiliations, DOIs, titles, categories,
        and category URLs across all publication categories.

        Args:
            **kwargs: Keyword arguments containing faculty and article data.
                Required arguments:
                - faculty_members (List): List of faculty authors
                    Type: List[str]
                - faculty_affiliations (Dict): Faculty department mappings
                    Type: Dict[str, List[str]]
                - title (str): Article title
                    Type: str
                - doi (str): Digital Object Identifier
                    Type: str
                - tc_count (int): Citation count
                    Type: int
                - all_categories (List): List of all categories
                    Type: List[str]
                - top_level_categories (List): Top-level categories
                    Type: List[str]
                - mid_level_categories (List): Mid-level categories
                    Type: List[str]
                - low_level_categories (List): Low-level categories
                    Type: List[str]
                - url_maps (Dict): Category URL mappings
                    Type: Dict[str, Dict[str, str]]
                - themes (List): Article themes
                    Type: List[str]
                - journal (str): Journal name
                    Type: str

        Raises:
            KeyError: If required kwargs are missing
            ValueError: If global faculty statistics cannot be updated

        Notes:
            - Updates global metrics for each faculty member
            - Tracks statistics across all categories
            - Maintains hierarchical category relationships
            - Handles multiple department affiliations
            - Aggregates publication metrics globally
        """
        self.logger.info("Updating global faculty stats...")
        for faculty_member in kwargs["faculty_members"]:
            self.logger.info(
                f"Updating global faculty stats for faculty member: {faculty_member}"
            )

            self.logger.info(
                "Checking if global faculty stats for faculty member exists..."
            )
            if faculty_member not in self.global_faculty_stats:
                self.logger.info(
                    "Global faculty stats for faculty member does not exist, creating new..."
                )
                self.global_faculty_stats[faculty_member] = (
                    self.dataclass_factory.get_dataclass(
                        DataClassTypes.GLOBAL_FACULTY_STATS, name=faculty_member
                    )
                )
                self.logger.info("Created new global faculty stats for faculty member.")

            # Get all URLs from maps
            # This logic even confused me so here's a not of why they get everything
            # This is global faculty stats, so the goal is after processing all papers
            # We can see the faculty members stats across papers without having to sum up all the individual records provided by the standard faculty stats which only look an individual category stat totals
            self.logger.info("Getting all URLs from maps...")
            self.logger.info(f"Getting top category URLs...")
            top_cat_urls: List[str] = [
                kwargs["url_maps"]["top"].get(cat)
                for cat in kwargs["top_level_categories"]
                if kwargs["url_maps"]["top"].get(cat)
            ]
            self.logger.info(f"Got top category URLs: {top_cat_urls}")

            self.logger.info("Getting mid category URLs...")
            mid_cat_urls: List[str] = [
                kwargs["url_maps"]["mid"].get(cat)
                for cat in kwargs["mid_level_categories"]
                if kwargs["url_maps"]["mid"].get(cat)
            ]
            self.logger.info(f"Got mid category URLs: {mid_cat_urls}")

            self.logger.info("Getting low category URLs...")
            low_cat_urls: List[str] = [
                kwargs["url_maps"]["low"].get(cat)
                for cat in kwargs["low_level_categories"]
                if kwargs["url_maps"]["low"].get(cat)
            ]
            self.logger.info(f"Got low category URLs: {low_cat_urls}")

            self.logger.info("Combining all category URLs...")
            all_cat_urls: List[str] = top_cat_urls + mid_cat_urls + low_cat_urls
            self.logger.info(f"Combined all category URLs: {all_cat_urls}")

            self.logger.info("Getting global faculty stats for faculty member...")
            global_stats: GlobalFacultyStats = self.global_faculty_stats[faculty_member]
            self.logger.info("Setting params for global faculty stats...")
            global_stats.set_params(
                {
                    "_id": self._generate_normal_id(
                        strings=[faculty_member], logger=self.logger
                    ),
                    "total_citations": global_stats.total_citations
                    + kwargs["tc_count"],
                    "article_count": global_stats.article_count + 1,
                    "department_affiliations": kwargs["faculty_affiliations"].get(
                        faculty_member, []
                    ),
                    "dois": kwargs["doi"],
                    "titles": kwargs["title"],
                    "categories": kwargs["all_categories"],
                    "category_urls": all_cat_urls,
                    "top_level_categories": kwargs["top_level_categories"],
                    "mid_level_categories": kwargs["mid_level_categories"],
                    "low_level_categories": kwargs["low_level_categories"],
                    "top_category_urls": top_cat_urls,
                    "mid_category_urls": mid_cat_urls,
                    "low_category_urls": low_cat_urls,
                    "themes": kwargs["themes"],
                    "journals": kwargs["journal"],
                    "citation_map": {kwargs["doi"]: kwargs["tc_count"]},
                }
            )
            self.logger.info("Completed setting params for global faculty stats.")

        self.logger.info("Completed updating global faculty stats.")

    def update_category_article_stats(self, **kwargs) -> None:
        """Update article statistics for each category.

        Creates or updates article statistics including titles, citations, faculty members,
        affiliations, abstracts, licenses, publication dates, and URLs. Organizes articles
        by their category levels (top, mid, low).

        Args:
            **kwargs: Keyword arguments containing article data.
                Required arguments:
                - title (str): Article title
                    Type: str
                - doi (str): Digital Object Identifier
                    Type: str
                - tc_count (int): Citation count
                    Type: int
                - faculty_members (List): List of faculty authors
                    Type: List[str]
                - faculty_affiliations (Dict): Faculty department mappings
                    Type: Dict[str, List[str]]
                - abstract (str): Article abstract
                    Type: str
                - license_url (str): License URL
                    Type: str
                - date_published_print (str): Print publication date
                    Type: str
                - date_published_online (str): Online publication date
                    Type: str
                - journal (str): Journal name
                    Type: str
                - download_url (str): Download URL
                    Type: str
                - themes (List): Article themes
                    Type: List[str]
                - all_categories (List): List of all categories
                    Type: List[str]
                - low_level_categories (List): Low-level categories
                    Type: List[str]
                - mid_level_categories (List): Mid-level categories
                    Type: List[str]
                - url_maps (Dict): Category URL mappings
                    Type: Dict[str, Dict[str, str]]

        Raises:
            KeyError: If required kwargs are missing
            ValueError: If article statistics cannot be updated

        Notes:
            - Updates statistics for each category level
            - Maintains hierarchical relationships
            - Tracks detailed article metadata
            - Links articles to faculty and departments
            - Preserves publication timeline information
        """
        self.logger.info("Updating category article stats...")
        for category in kwargs["all_categories"]:
            self.logger.info(
                f"Updating category article stats for category: {category}"
            )

            self.logger.info(
                "Checking if category article stats for category exists..."
            )
            if category not in self.category_article_stats:
                self.logger.info(
                    "Category article stats for category does not exist, creating new..."
                )
                self.category_article_stats[category] = (
                    self.dataclass_factory.get_dataclass(
                        DataClassTypes.CROSSREF_ARTICLE_STATS
                    )
                )
                self.logger.info("Created new category article stats for category.")

            self.logger.info("Getting URL for category via URL map...")
            url: str = (
                kwargs["url_maps"]["low"].get(category)
                or kwargs["url_maps"]["mid"].get(category)
                or kwargs["url_maps"]["top"].get(category)
            )
            self.logger.info(f"Retrieved URL: {url}")

            # Base article data
            self.logger.info("Setting base article data...")
            article_data: Dict[str, Any] = {
                "_id": kwargs["doi"],
                "title": kwargs["title"],
                "tc_count": kwargs["tc_count"],
                "faculty_members": kwargs["faculty_members"],
                "faculty_affiliations": kwargs["faculty_affiliations"],
                "abstract": kwargs["abstract"],
                "license_url": kwargs["license_url"],
                "date_published_print": kwargs["date_published_print"],
                "date_published_online": kwargs["date_published_online"],
                "journal": kwargs["journal"],
                "download_url": kwargs["download_url"],
                "doi": kwargs["doi"],
                "themes": kwargs["themes"],
                "categories": category,
                "category_urls": url,
                "url": self._generate_url(kwargs["doi"], self.logger),
            }
            self.logger.info("Completed setting base article data.")

            # Add category and URL to appropriate level
            self.logger.info("Adding category and URL to appropriate level...")
            if category in kwargs["low_level_categories"]:
                self.logger.info("Category is in low level categories...")
                article_data["low_level_categories"] = category
                article_data["low_category_urls"] = url
            elif category in kwargs["mid_level_categories"]:
                self.logger.info("Category is in mid level categories...")
                article_data["mid_level_categories"] = category
                article_data["mid_category_urls"] = url
            else:
                self.logger.info("Category is in top level categories...")
                article_data["top_level_categories"] = category
                article_data["top_category_urls"] = url

            self.logger.info("Setting params for category article stats...")
            self.category_article_stats[category].set_params(
                {kwargs["doi"]: article_data}
            )
            self.logger.info("Completed setting params for category article stats.")
        self.logger.info("Completed updating category article stats.")

    def create_article_object(self, **kwargs) -> None:
        """Create a new article object with complete metadata.

        Creates a CrossrefArticleDetails object containing all article information,
        including category relationships, URLs, and metadata. Handles URL generation
        for different category levels and maintains category hierarchies.

        Args:
            **kwargs: Keyword arguments containing article data.
                Required arguments:
                - doi (str): Digital Object Identifier
                    Type: str
                - title (str): Article title
                    Type: str
                - tc_count (int): Citation count
                    Type: int
                - faculty_members (List): Faculty authors
                    Type: List[str]
                - faculty_affiliations (Dict): Faculty affiliations
                    Type: Dict[str, List[str]]
                - abstract (str): Article abstract
                    Type: str
                - license_url (str): License URL
                    Type: str
                - date_published_print (str): Print publication date
                    Type: str
                - date_published_online (str): Online publication date
                    Type: str
                - journal (str): Journal name
                    Type: str
                - download_url (str): Download URL
                    Type: str
                - themes (List): Article themes
                    Type: List[str]
                - all_categories (List): All categories
                    Type: List[str]
                - top_level_categories (List): Top-level categories
                    Type: List[str]
                - mid_level_categories (List): Mid-level categories
                    Type: List[str]
                - low_level_categories (List): Low-level categories
                    Type: List[str]

        Raises:
            KeyError: If required kwargs are missing
            ValueError: If article object cannot be created

        Notes:
            - Creates CrossrefArticleDetails instance
            - Generates URLs for all category levels
            - Maintains category hierarchies
            - Preserves all article metadata
            - Links faculty and department relationships

        """
        self.logger.info("Creating article object...")

        # Create the article
        article: CrossrefArticleDetails = self.dataclass_factory.get_dataclass(
            DataClassTypes.CROSSREF_ARTICLE_DETAILS
        )
        self.logger.info("Created article object.")
        # Generate URLs for all categories
        # Initialize empty lists for top, mid, low category URLs
        # We don't need lists for the categories themselves as they're already in **kwargs
        self.logger.info("Generating URLs for all categories...")
        top_cat_urls: List[str] = []
        mid_cat_urls: List[str] = []
        low_cat_urls: List[str] = []

        # Parse through the categories and check if the level of the current category
        # This is to:
        # 1. Track top, mid, low category names
        # 2. Generate the URLs for each top, mid, low category
        # This allows for displaying what categories the articles what classified under
        # and under which level, as well as directly link to those pages on the frontend
        for category in kwargs["all_categories"]:
            self.logger.info(f"Processing category: {category}")
            if category in kwargs["low_level_categories"]:
                self.logger.info("Category is in low level categories...")
                # Get the mid category for the low category
                mid_cat = self.taxonomy_util.get_mid_cat_for_low_cat(category)
                self.logger.info(f"Got mid category for low category: {mid_cat}")
                # Get the top category for the mid category
                top_cat = self.taxonomy_util.get_top_cat_for_mid_cat(mid_cat)
                self.logger.info(f"Got top category for mid category: {top_cat}")
                # Generate the URL for the low category
                # If the low category is software development
                # and the mid category is software engineering
                # and the top category is computer science
                # the URL will be Computer%20science/software%20engineering/software%20development
                low_cat_urls.append(
                    self._generate_url(f"{top_cat}/{mid_cat}/{category}", self.logger)
                )

            elif category in kwargs["mid_level_categories"]:
                self.logger.info("Category is in mid level categories...")
                # Get the top category for the mid category
                top_cat = self.taxonomy_util.get_top_cat_for_mid_cat(category)
                self.logger.info(f"Got top category for mid category: {top_cat}")
                # Generate the URL for the mid category
                # If the mid category is software engineering
                # and the top category is computer science
                # the URL will be Computer%20science/software%20engineering
                mid_cat_urls.append(
                    self._generate_url(f"{top_cat}/{category}", self.logger)
                )
            else:
                self.logger.info("Category is in top level categories...")
                # If the top category is computer science
                # the URL will be Computer%20science
                top_cat_urls.append(self._generate_url(category, self.logger))

        # Combine all the URLs to get a compendium of all the URLs
        # This doesn't have a defined use yet, but it seems like it has the potential
        # to be a valuable piece of data for something so tracking it now so I don't
        # have to add it later and rerun all the articles ran up to that point
        # in the event we end up wanting/needing it for something
        self.logger.info(
            "Combining all the URLs to get a compendium of all the URLs..."
        )
        all_cat_urls: List[str] = top_cat_urls + mid_cat_urls + low_cat_urls
        self.logger.info(f"Combined all the URLs: {json.dumps(all_cat_urls, indent=4)}")

        # Direct field updates for CrossrefArticleDetails
        # .set_params() is used to update the dataclass fields for this article object
        self.logger.info("Setting params for article object...")
        article.set_params(
            {
                # Set _id to the doi to allow for easy lookup from the categories
                # As mentioned before each category has a list of dois
                # So through the use of that list we can look up in the DB each one of these article objects
                "_id": kwargs["doi"],
                "title": kwargs["title"],
                "tc_count": kwargs["tc_count"],
                "faculty_members": kwargs["faculty_members"],
                "faculty_affiliations": kwargs["faculty_affiliations"],
                "abstract": kwargs["abstract"],
                "license_url": kwargs["license_url"],
                "date_published_print": kwargs["date_published_print"],
                "date_published_online": kwargs["date_published_online"],
                "journal": kwargs["journal"],
                "download_url": kwargs["download_url"],
                "doi": kwargs["doi"],
                "themes": kwargs["themes"],
                "categories": kwargs["all_categories"],
                "category_urls": all_cat_urls,
                # Pull out and add lists for the category levels
                "top_level_categories": kwargs["top_level_categories"],
                "mid_level_categories": kwargs["mid_level_categories"],
                "low_level_categories": kwargs["low_level_categories"],
                # Add in the URLs fetched and created at the top of this method
                "top_category_urls": top_cat_urls,
                "mid_category_urls": mid_cat_urls,
                "low_category_urls": low_cat_urls,
                # Actual page URL for this article is quote(doi)
                "url": self._generate_url(kwargs["doi"], self.logger),
            }
        )
        self.logger.info("Completed setting params for article object.")

        # Append the created article object to the articles list
        self.logger.info("Appending the created article object to the articles list...")
        self.articles.append(article)
        self.logger.info(
            "Completed appending the created article object to the articles list."
        )

    def clean_faculty_affiliations(
        self, faculty_affiliations: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Clean and format faculty affiliation data.

        Processes raw faculty affiliation mappings to ensure consistent formatting
        and remove any invalid or malformed data.

        Args:
            faculty_affiliations (Dict): Raw faculty affiliation mappings.
                Type: Dict[str, Any]

        Returns:
            Dict: Cleaned faculty affiliation mappings.
                Type: Dict[str, Any]

        Notes:
            - Removes invalid entries
            - Normalizes department names
            - Handles missing or malformed data
            - Maintains faculty-department relationships
        """
        self.logger.info("Cleaning and formatting faculty affiliation data...")
        department_affiliations: Dict[str, Any] = {}
        for faculty_member, affiliations in faculty_affiliations.items():
            department_affiliations[faculty_member] = affiliations
        self.logger.info("Completed cleaning and formatting faculty affiliation data.")
        return department_affiliations

    def clean_faculty_members(self, faculty_members: List[str]) -> List[str]:
        """Clean and filter faculty member names.

        Processes raw faculty member names to ensure consistent formatting
        and remove any invalid or empty entries.

        Args:
            faculty_members (List): Raw list of faculty member names.
                Type: List[str]

        Returns:
            List: Cleaned list of faculty member names.
                Type: List[str]
                Excludes empty strings and invalid entries.

        Notes:
            - Removes empty strings
            - Normalizes name formats
            - Filters invalid entries
            - Maintains unique entries
        """
        self.logger.info("Cleaning and filtering faculty member names...")
        clean_faculty_members: List[str] = []
        for faculty_member in faculty_members:
            if faculty_member != "":
                clean_faculty_members.append(faculty_member)
        self.logger.info("Completed cleaning and filtering faculty member names.")
        return clean_faculty_members

    def initialize_categories(
        self, categories: Dict[str, List[str]]
    ) -> Dict[str, List[str]]:
        """Initialize category data structures for all category levels.

        Creates CategoryInfo instances for each category and organizes them by level
        in the taxonomy hierarchy (top, mid, low).

        Args:
            categories (Dict): Categories organized by level.
                Type: Dict[str, List[str]]
                Keys must be: "top", "mid", "low"

        Returns:
            Dict: Organized category data.
                Type: Dict[str, List[str]]
                Contains:
                - top_level_categories (List[str]): List of top-level categories
                - mid_level_categories (List[str]): List of mid-level categories
                - low_level_categories (List[str]): List of low-level categories
                - all_categories (List[str]): List of all categories

        Raises:
            ValueError: If category initialization fails

        Notes:
            - Creates CategoryInfo instances for each category
            - Maintains hierarchical relationships
            - Validates category levels
            - Ensures unique category names
            - Preserves taxonomy structure
        """
        self.logger.info("Initializing categories...")
        top_level_categories: List[str] = []
        mid_level_categories: List[str] = []
        low_level_categories: List[str] = []
        for category_level in ["top", "mid", "low"]:
            self.logger.info(
                f"Category being initialized is in level: {category_level}"
            )
            for category in categories.get(category_level, []):
                self.logger.info(f"Category being initialized: {category}")

                self.logger.info(
                    "Checking if category already exists in category data..."
                )
                if category not in self.category_data:
                    self.logger.info(
                        "Category does not exist, creating new category info..."
                    )
                    self.category_data[category] = self.dataclass_factory.get_dataclass(
                        DataClassTypes.CATEGORY_INFO, category_name=category
                    )
                    self.logger.info("Created new category info.")

                if category_level == "top":
                    self.logger.info(
                        "Category is in top level categories, appending to top level categories list..."
                    )
                    top_level_categories.append(category)
                elif category_level == "mid":
                    self.logger.info(
                        "Category is in mid level categories, appending to mid level categories list..."
                    )
                    mid_level_categories.append(category)
                elif category_level == "low":
                    self.logger.info(
                        "Category is in low level categories, appending to low level categories list..."
                    )
                    low_level_categories.append(category)

        self.logger.info("Completed initializing categories.")
        return {
            "top_level_categories": top_level_categories,
            "mid_level_categories": mid_level_categories,
            "low_level_categories": low_level_categories,
            "all_categories": top_level_categories
            + mid_level_categories
            + low_level_categories,
        }

    # Set of public Getter methods to access the data
    def get_category_data(self) -> Dict[str, CategoryInfo]:
        """Get the processed category data.

        Provides access to the complete mapping of categories and their associated
        information, including statistics and relationships.

        Returns:
            Dict: Mapping of categories to their information.
                Type: Dict[str, :class:`academic_metrics.models.category_info.CategoryInfo`]

        Notes:
            - Returns complete category hierarchy
            - Includes all category statistics
            - Contains faculty and article relationships
            - Preserves category metadata
        """
        self.logger.info("Returning category data...")
        return self.category_data

    def get_category_article_stats(self) -> Dict[str, CrossrefArticleStats]:
        """Get article statistics organized by category.

        Provides access to the complete mapping of categories to their associated
        article statistics, including metrics and metadata.

        Returns:
            Dict: Mapping of categories to their article statistics.
                Type: Dict[str, :class:`academic_metrics.models.crossref_article_stats.CrossrefArticleStats`]

        Notes:
            - Returns statistics for all categories
            - Includes article counts and metrics
            - Contains citation information
            - Preserves publication metadata
            - Maintains category relationships
        """
        self.logger.info("Returning article statistics organized by category...")
        return self.category_article_stats

    def get_articles(self) -> List[CrossrefArticleDetails]:
        """Get the list of processed articles.

        Provides access to the complete list of processed articles with their
        full details and metadata.

        Returns:
            List: List of all processed article details.
                Type: List[:class:`academic_metrics.models.crossref_article_details.CrossrefArticleDetails`]

        Notes:
            - Returns all processed articles
            - Includes complete article metadata
            - Contains category assignments
            - Preserves faculty relationships
            - Maintains publication details
        """
        self.logger.info("Returning list of processed articles...")
        return self.articles

    def get_faculty_stats(self) -> Dict[str, FacultyStats]:
        """Get faculty statistics organized by category.

        Provides access to the complete mapping of categories to their associated
        faculty statistics, including publication metrics and relationships.

        Returns:
            Dict: Mapping of categories to their faculty statistics.
                Type: Dict[str, :class:`academic_metrics.models.faculty_stats.FacultyStats`]

        Notes:
            - Returns statistics for all categories
            - Includes faculty publication counts
            - Contains citation metrics
            - Preserves department affiliations
            - Maintains category-specific metrics
        """
        self.logger.info("Returning faculty statistics organized by category...")
        return self.faculty_stats

    def get_global_faculty_stats(self) -> Dict[str, GlobalFacultyStats]:
        """Get global statistics for all faculty members.

        Provides access to the complete mapping of faculty members to their global
        statistics across all categories and publications.

        Returns:
            Dict: Mapping of faculty members to their global statistics.
                Type: Dict[str, :class:`academic_metrics.models.global_faculty_stats.GlobalFacultyStats`]

        Notes:
            - Returns aggregate statistics per faculty
            - Includes cross-category metrics
            - Contains total publication counts
            - Preserves all department affiliations
            - Maintains complete publication history
        """
        self.logger.info("Returning global statistics for all faculty members...")
        return self.global_faculty_stats

    # End of public Getter methods

    @staticmethod
    def _collect_all_affiliations(
        faculty_affiliations: Dict[str, Any], logger: logging.Logger
    ) -> Set[str]:
        """Collect all unique department affiliations.

        Extracts and deduplicates all department affiliations from the faculty
        to department mapping dictionary.

        Args:
            faculty_affiliations (Dict): Faculty to department mappings.
                Type: Dict[str, Any]
            logger (logging.Logger): Logger instance for tracking operations.
                Type: logging.Logger

        Returns:
            set: Set of unique department affiliations.
                Type: Set[str]

        Notes:
            - Removes duplicate departments
            - Handles missing affiliations
            - Validates department names
            - Maintains unique entries only
        """
        logger.info("Collecting all unique department affiliations...")
        logger.info(f"Initializing all affiliations empty set...")
        all_affiliations: set[str] = set()
        logger.info(f"All affiliations empty set initialized.")

        logger.info(f"Iterating through faculty affiliations...")
        for department_affiliation in faculty_affiliations.values():
            logger.info(f"Processing department affiliation: {department_affiliation}")

            logger.info(f"Checking if department affiliation is a set...")
            if isinstance(department_affiliation, set):
                logger.info(
                    f"Department affiliation is a set, updating all affiliations set..."
                )
                all_affiliations.update(department_affiliation)
            elif isinstance(department_affiliation, list):
                logger.info(
                    "Department affiliation is a list, updating all affiliations set..."
                )
                all_affiliations.update(department_affiliation)
            elif isinstance(department_affiliation, str):
                logger.info(
                    "Department affiliation is a string, adding to all affiliations set..."
                )
                all_affiliations.add(department_affiliation)

        logger.info(
            f"Completed iterating through faculty affiliations, returning all affiliations set..."
        )
        return all_affiliations

    @staticmethod
    def _generate_url(string: str, logger: logging.Logger | None = None) -> str:
        """Generate a URL-safe string.

        Converts an input string into a URL-safe format by removing special characters,
        replacing spaces, and ensuring proper encoding.

        Args:
            string (str): Input string to encode.
                Type: str
            logger (logging.Logger | None): Logger instance to use for logging.
                Type: logging.Logger | None
                Defaults to None.

        Returns:
            str: URL-encoded string.
                Type: str

        Notes:
            - Removes special characters
            - Replaces spaces with hyphens
            - Converts to lowercase
            - Ensures URL-safe encoding
        """
        logger.info(f"Generating URL-encoded string for: {string}")
        url_encoded_string: str = quote(string)
        logger.info(f"Generated URL-encoded string: {url_encoded_string}")
        return url_encoded_string

    @staticmethod
    def _generate_normal_id(
        strings: List[str], logger: logging.Logger | None = None
    ) -> str:
        """Generate a normalized ID from a list of strings.

        Combines multiple strings into a single normalized identifier, ensuring
        consistent formatting and URL-safe characters.

        Args:
            strings (list): List of strings to combine into an ID.
                Type: List[str]
            logger (logging.Logger | None): Logger instance to use for logging.
                Type: logging.Logger | None
                Defaults to None.

        Returns:
            str: Normalized ID string.
                Type: str
                Format: lowercase, hyphen-separated

        Notes:
            - Combines multiple strings
            - Converts to lowercase
            - Replaces spaces with hyphens
            - Removes special characters
            - Ensures consistent formatting
        """
        logger.info(f"Generating normalized ID from strings: {strings}")
        logger.info(f"Initializing normalized ID as empty string...")
        normal_id: str = ""
        logger.info(f"Initialized normalized ID as empty string.")

        logger.info(f"Iterating through strings to generate normalized ID...")
        for string in strings:
            logger.info(f"Processing string: {string}")
            normal_id += f'{string.lower().replace(" ", "-")}_'
            logger.info(f"Updated normalized ID: {normal_id}")

        logger.info(f"Completed iterating through strings, returning normalized ID...")
        normalized_id: str = normal_id.rstrip("_")
        logger.info(f"Normalized ID: {normalized_id}")
        return normalized_id


if __name__ == "__main__":
    from academic_metrics.factories import DataClassFactory, StrategyFactory
    from academic_metrics.utils import Taxonomy, Utilities, WarningManager

    dc_factory = DataClassFactory()
    taxonomy = Taxonomy()
    strategy_factory = StrategyFactory()
    warning_manager = WarningManager()
    utilities = Utilities(
        strategy_factory=strategy_factory, warning_manager=warning_manager
    )

    category_processor = CategoryProcessor(
        utils=utilities,
        dataclass_factory=dc_factory,
        warning_manager=warning_manager,
        taxonomy_util=taxonomy,
    )

    raw_attributes: Dict[str, Any] = {
        "categories": {
            "top": [
                "Agricultural sciences and natural resources",
                "Geosciences, atmospheric, and ocean sciences",
            ],
            "mid": [
                "Agricultural, animal, plant, and veterinary sciences",
                "Geological and earth sciences",
                "Ocean/ marine sciences and atmospheric science",
            ],
            "low": [
                "Aquaculture",
                "Food science and technology",
                "Plant sciences",
                "Geology/ earth science, general",
                "Hydrology and water resources science",
                "Marine biology and biological oceanography",
                "Marine sciences",
            ],
        },
        "faculty_members": [
            "Christopher Mulanda Aura",
            "Safina Musa",
            "Chrisphine S. Nyamweya",
            "Zachary Ogari",
            "James M. Njiru",
            "Stuart E. Hamilton",
            "Linda May",
        ],
        "faculty_affiliations": {
            "Christopher Mulanda Aura": [
                "Kenya Marine and Fisheries Research Institute Kisumu Kenya"
            ],
            "Safina Musa": [
                "Kenya Marine and Fisheries Research Institute Kegati Kenya"
            ],
            "Chrisphine S. Nyamweya": [
                "Kenya Marine and Fisheries Research Institute Kisumu Kenya"
            ],
            "Zachary Ogari": [
                "Kenya Marine and Fisheries Research Institute Kisumu Kenya"
            ],
            "James M. Njiru": [
                "Kenya Marine and Fisheries Research Institute Mombasa Kenya"
            ],
            "Stuart E. Hamilton": ["Salisbury University Salisbury USA"],
            "Linda May": [
                "UK Centre for Ecology &amp; Hydrology Penicuik Midlothian UK"
            ],
        },
        "title": [
            "A GISâ€based approach for delineating suitable areas for cage fish culture in a lake"
        ],
        "tc_count": 4,
        "abstract": "We present a GISâ€based approach to the delineation of areas that have different levels of suitability for use as tilapia cage culture sites the Kenyan part of Lake Victoria, Africa. The study area was 4,100\xa0km2. The method uses highâ€resolution bathymetric data, newly collected water quality data from all major fishing grounds and cage culture sites, and existing spatial information from previous studies. The parameters considered are water depth, water temperature, levels of dissolved oxygen, chlorophyllâ€aconcentrations, distances to the lake shoreline and proximity to other constraints on cage culture development. The results indicated that the area most suitable for fish cages comprised about 362\xa0km2, or approximately 9% of the total area; the remaining 91% (i.e., 3,737\xa0km2) was found to be unsuitable for tilapia cage culture. We conclude that the successful implementation of this approach would need stakeholder involvement in the validation and approval of potential sites, and in the incorporation of lake zoning into spatial planning policy and the regulations that support sustainable use while minimising resource use conflicts. The results of this study have broader applicability to the whole of Lake Victoria, other African Great Lakes, and any lakes in the world where tilapia cage culture already occurs or may occur in the future.\n",
        "license_url": "http://onlinelibrary.wiley.com/termsAndConditions#vor",
        "date_published_print": "2021-6",
        "date_published_online": "2021-4-13",
        "journal": "Lakes &amp; Reservoirs: Science, Policy and Management for Sustainable Use",
        "download_url": "http://dx.doi.org/10.1111/lre.12357",
        "doi": "10.1111/lre.12357",
        "themes": [
            "Aquaculture site suitability",
            "GIS applications in environmental science",
            "Water quality assessment",
            "Sustainable resource management",
        ],
    }

    category_processor._test_category_processor(raw_attributes)

            ```

            src/academic_metrics/data_collection/CrossrefWrapper.py:
            ```
from __future__ import annotations

import asyncio
import json
import logging
import os
import time
from typing import TYPE_CHECKING, Self, Any, Union, List, Dict

import aiohttp

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)

if TYPE_CHECKING:
    from .scraper import Scraper


class CrossrefWrapper:
    """A wrapper class for interacting with the Crossref API to fetch and process publication data.

    Attributes:
        base_url (str): The base URL for the Crossref API.
        affiliation (str): The affiliation to filter publications by.
        from_year (int): The starting year for the publication search.
        to_year (int): The ending year for the publication search.
        logger (logging.Logger): Logger for logging messages.
        MAX_CONCURRENT_REQUESTS (int): Maximum number of concurrent requests allowed.
        semaphore (asyncio.Semaphore): Semaphore to control the rate of concurrent requests.
        years (list): List of years to fetch data for.
        data (dict): Data fetched from the Crossref API.

    Methods:
        fetch_data(session: aiohttp.ClientSession, url: str, headers: dict[str, Any], retries: int, retry_delay: int) -> dict[str, Any] | None: Fetches data from the given URL using aiohttp.
        build_request_url(base_url: str, affiliation: str, from_date: str, to_date: str, n_element: str, sort_type: str, sort_ord: str, cursor: str, has_abstract: bool | None = False) -> str: Builds the request URL for the Crossref API.
        process_items(data: dict[str, Any], from_date: str, to_date: str, affiliation: str | None = "salisbury univ") -> list[dict[str, Any]]: Processes the items fetched from the Crossref API, filtering by date and affiliation.
        _get_last_day_of_month(year: int, month: int) -> int: Returns the last day of the given month in the given year.
        fetch_data_for_multiple_years() -> list[dict[str, Any]]: Fetches data for multiple years asynchronously.
        serialize_to_json(output_file: str) -> None: Serializes the fetched data to a JSON file.
        final_data_process() -> Self: Processes the final data, filling in missing abstracts.
        get_result_list() -> list[dict[str, Any]]: Get the result list
        run_all_process(save_offline: bool = False) -> Union[None, List[Dict[str, Any]]]: Run all data fetching and processing
    """

    def __init__(
        self,
        *,
        scraper: Scraper,
        base_url: str | None = "https://api.crossref.org/works",
        affiliation: str | None = "Salisbury%20University",
        from_year: int | None = 2017,
        to_year: int | None = 2024,
        from_month: int | None = 1,
        to_month: int | None = 12,
        test_run: bool | None = False,
        run_scraper: bool | None = True,
    ) -> Self:
        """Initializes the CrossrefWrapper with the given parameters.

        Args:
            base_url (str): The base URL for the Crossref API.
            affiliation (str): The affiliation to filter publications by.
            from_year (int): The starting year for the publication search.
            to_year (int): The ending year for the publication search.
            logger (logging.Logger, optional): Logger for logging messages. Defaults to None.
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_wrapper",
            log_level=DEBUG,
        )

        self.scraper = scraper
        self.run_scraper = run_scraper

        # Maximum number of concurrent requests allowed
        self.MAX_CONCURRENT_REQUESTS = 2  # Limit to 2 concurrent tasks at a time

        # Semaphore to control the rate of concurrent requests
        self.semaphore = asyncio.Semaphore(self.MAX_CONCURRENT_REQUESTS)

        if not isinstance(from_year, int):
            raise ValueError("from_year must be an integer")
        if not isinstance(to_year, int):
            raise ValueError("to_year must be an integer")
        if not isinstance(from_month, int):
            raise ValueError("from_month must be an integer")
        if not isinstance(to_month, int):
            raise ValueError("to_month must be an integer")

        self.from_year = from_year
        self.to_year = to_year
        self.from_month = from_month
        self.to_month = to_month
        self.base_url = base_url
        self.affiliation = affiliation
        self.test_run = test_run

        self.years = [year for year in range(from_year, to_year + 1)]

        # Prevent logger from being affected by parent loggers
        self.logger.propagate = False

        self.data = None

    async def fetch_data(
        self,
        session: aiohttp.ClientSession,
        url: str,
        headers: dict[str, Any],
        retries: int,
        retry_delay: int,
    ) -> dict[str, Any] | None:
        """Fetches data from the given URL using aiohttp.

        Args:
            session (aiohttp.ClientSession): The aiohttp session to use for the request.
            url (str): The URL to fetch data from.
            headers (dict): Headers to include in the request.
            retries (int): Number of retries in case of failure.
            retry_delay (int): Delay between retries in seconds.

        Returns:
            dict: The JSON data fetched from the URL, or None if an error occurs.
        """
        num_iter = 0
        while num_iter < retries:
            try:
                async with session.get(url, headers=headers) as response:
                    if response.status == 429:
                        retry_after = retry_delay
                        self.logger.debug(
                            f"Hit request limit, retrying in {retry_after} seconds..."
                        )
                        await asyncio.sleep(retry_after)
                        num_iter += 1
                        continue
                    elif response.status == 200:
                        logging.info("Getting data from response...")
                        data = await response.json()
                        return data
                    else:
                        self.logger.error(
                            f"Unexpected status {response.status} for URL: {url}"
                        )
                        return None
            except aiohttp.ClientError as e:
                self.logger.error(f"Network error: {e}")
                return None
        self.logger.warning(f"Exceeded max retries for URL: {url}")
        return None

    def build_request_url(
        self,
        base_url: str,
        affiliation: str,
        from_date: str,
        to_date: str,
        n_element: str,
        sort_type: str,
        sort_ord: str,
        cursor: str,
        has_abstract: bool | None = False,
    ) -> str:
        """Builds the request URL for the Crossref API.

        Args:
            base_url (str): The base URL for the Crossref API.
            affiliation (str): The affiliation to filter publications by.
            from_date (str): The starting date for the publication search.
            to_date (str): The ending date for the publication search.
            n_element (str): Number of elements to fetch per request.
            sort_type (str): The type of sorting to apply.
            sort_ord (str): The order of sorting (asc or desc).
            cursor (str): The cursor for pagination.
            has_abstract (bool, optional): Whether to filter for publications with abstracts. Defaults to False.

        Returns:
            str: The constructed request URL.
        """
        ab_state = ""
        if has_abstract:
            ab_state = ",has-abstract:1"

        return f"{base_url}?query.affiliation={affiliation}&filter=from-pub-date:{from_date},until-pub-date:{to_date}{ab_state}&sort={sort_type}&order={sort_ord}&rows={n_element}&cursor={cursor}"

    def process_items(
        self,
        data: dict[str, Any],
        from_date: str,
        to_date: str,
        affiliation: str | None = "salisbury univ",
    ) -> list[dict[str, Any]]:
        """Processes the items fetched from the Crossref API, filtering by date and affiliation.

        Args:
            data (dict): The data fetched from the Crossref API.
            from_date (str): The starting date for the publication search.
            to_date (str): The ending date for the publication search.
            affiliation (str, optional): The affiliation to filter publications by. Defaults to "salisbury univ".

        Returns:
            list: The filtered list of items.
        """
        filtered_data = []
        i = 0
        items = data.get("items", None)
        if items is None:
            raise ValueError("No items found in data")

        for item in data.get("items", []):
            # Get published date information
            item_published = item.get("published", None)
            item_date_parts = None
            if item_published is not None:
                item_date_parts = item_published.get("date-parts", None)

            # Skip if no date parts or invalid structure
            if not item_date_parts or not item_date_parts[0]:
                continue

            # Get year and month, defaulting to None if not available
            pub_year = item_date_parts[0][0] if len(item_date_parts[0]) > 0 else None
            pub_month = item_date_parts[0][1] if len(item_date_parts[0]) > 1 else None

            from_year = int(from_date.split("-")[0])
            from_month = int(from_date.split("-")[1])
            to_year = int(to_date.split("-")[0])
            to_month = int(to_date.split("-")[1])

            # Skip if year is missing or outside range
            if pub_year is None or pub_year < from_year or pub_year > to_year:
                continue

            # If same year as bounds, check months
            if pub_year == from_year and (pub_month is None or pub_month < from_month):
                continue
            if pub_year == to_year and (pub_month is None or pub_month > to_month):
                continue

            count = 0
            for author in item.get("author", []):
                for affil in author.get("affiliation", []):
                    if affiliation in affil.get("name", "").lower():
                        count += 1
            if count > 0:
                filtered_data.append(item)

        return filtered_data

    async def acollect_yrange(
        self,
        session: aiohttp.ClientSession,
        from_date: str = "2018-01-01",
        to_date: str = "2024-10-09",
        n_element: str = "1000",
        sort_type: str = "relevance",
        sort_ord: str = "desc",
        cursor: str = "*",
        retries: int = 5,
        retry_delay: int = 3,
    ) -> tuple[list[dict[str, Any]], str | None]:
        """Collects data for a range of years asynchronously.

        Args:
            session (aiohttp.ClientSession): The aiohttp session to use for the request.
            from_date (str): The starting date for the publication search.
            to_date (str): The ending date for the publication search.
            n_element (str): Number of elements to fetch per request.
            sort_type (str): The type of sorting to apply.
            sort_ord (str): The order of sorting (asc or desc).
            cursor (str): The cursor for pagination.
            retries (int): Number of retries in case of failure.
            retry_delay (int): Delay between retries in seconds.

        Returns:
            tuple: A tuple containing the list of items and the next cursor.
        """
        self.logger.info("Starting Crf_dict_cursor_async function")
        item_list = []
        processed_papers = 0
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "Rommel Center",
            "mailto": "cbarbes1@gulls.salisbury.edu",
        }

        async with self.semaphore:
            req_url = self.build_request_url(
                self.base_url,
                self.affiliation,
                from_date,
                to_date,
                n_element,
                sort_type,
                sort_ord,
                cursor,
            )
            self.logger.debug(f"Request URL: {req_url}")

            data = await self.fetch_data(
                session, req_url, headers, retries, retry_delay
            )
            if data is None:
                return (None, None)

            data = data.get("message", {})
            if data == {}:
                logging.debug("No data to process")
                return (None, None)
            total_docs = data.get("total-results", 0)
            if total_docs == 0:
                logging.debug("No docs to process")
                return (None, None)
            self.logger.info(f"Processing API Pages from {from_date} to {to_date}")

            while processed_papers < total_docs:
                filtered_data = self.process_items(data, from_date, to_date)

                item_list.extend(filtered_data)
                processed_papers += len(filtered_data)

                cursor = data.get("next-cursor", None)
                if cursor is None:
                    break

                req_url = self.build_request_url(
                    self.base_url,
                    self.affiliation,
                    from_date,
                    to_date,
                    n_element,
                    sort_type,
                    sort_ord,
                    cursor,
                )
                data = await self.fetch_data(
                    session, req_url, headers, retries, retry_delay
                )
                if data is None:
                    break

                data = data.get("message", {})
                if filtered_data == []:
                    break
                await asyncio.sleep(3)

        self.logger.info("Processing Complete")
        return (item_list, cursor)

    def _get_last_day_of_month(self, year: int, month: int) -> int:
        """Returns the last day of the given month in the given year. Handles leap years for February.

        Args:
            year (int): The year to check.
            month (int): The month to check.

        Returns:
            int: The last day of the given month in the given year.
        """
        # Handle February for leap years
        if month == 2:
            if year % 4 == 0 and (year % 100 != 0 or year % 400 == 0):
                return 29
            return 28

        # Handle months with 30 days
        # Months with 30 days: April (4), June (6), September (9), November (11)
        if month in [4, 6, 9, 11]:
            return 30

        # All other months have 31 days
        return 31

    async def fetch_data_for_multiple_years(self) -> list[dict[str, Any]]:
        """Fetches data for multiple years asynchronously.

        Returns:
            final_result (list[dict[str, Any]]): The list of items fetched from the Crossref API.
        """
        # creat the async session and send the tasks to each thread
        async with aiohttp.ClientSession() as session:
            # create the list of tasks to complete
            tasks = []
            for year in self.years:
                # Format dates with leading zeros for months
                from_date = f"{year}-{self.from_month:02d}-01"

                # Format to_date to have the correct last day of the month
                last_day = self._get_last_day_of_month(year, self.to_month)
                to_date = f"{year}-{self.to_month:02d}-{last_day}"

                task = self.acollect_yrange(
                    session=session,
                    from_date=from_date,
                    to_date=to_date,
                )
                tasks.append(task)

            # get the start time
            start_time = time.time()
            # await for the execution of each thread to finish
            results = await asyncio.gather(*tasks)

            final_result = []

            # create the result list
            for result, _ in results:
                if result is not None:
                    final_result.extend(result)

            end_time = time.time()

            # log the time it took
            self.logger.info(
                f"All data fetched. This took {end_time - start_time} seconds"
            )

            return final_result

    def run_afetch_yrange(self) -> Self:
        """Runs the asynchronous data fetch for multiple years.

        Returns:
            self: The instance of the class for method chaining.
        """
        # run the async function chain
        result_list = asyncio.run(self.fetch_data_for_multiple_years())

        # log the num items returned
        self.logger.info(f"Number of items: {len(result_list)}")

        self.result = result_list
        return self

    def serialize_to_json(self, output_file: str) -> None:
        """
        Serializes the fetched data to a JSON file.

        Args:
            output_file (str): The path to the output JSON file.
        """
        with open(output_file, "w") as file:
            json.dump(self.result, fp=file, indent=4)

    def final_data_process(self) -> Self:
        """Processes the final data, filling in missing abstracts.

        Returns:
            self: The instance of the class for method chaining.
        """
        # fill missing abstracts
        num_processed = 0
        items_removed = 0

        self.logger.info("Starting final_data_process")  # Debug log

        if not self.result:  # Check if result exists
            self.logger.error("No result data found")
            return self

        if self.test_run:
            self.result = self.result[:3]
        i = 0
        while i < len(self.result):
            item = self.result[i]
            try:
                self.logger.info("-" * 80)
                self.logger.info(f"Processing URL: {item.get('URL')}")

                abstract, extra_context = self.scraper.get_abstract(item.get("URL"))

                if abstract is None or extra_context is None:
                    self.logger.warning(f"No data returned for URL: {item.get('URL')}")
                    # Remove the item, but don't increment i
                    # We don't increment i as the items in the list will shift left 1 so we want to keep
                    # the same index to check the next item
                    self.result.pop(i)
                    items_removed += 1
                    continue

                self.logger.info(
                    f"\n\nRETURN FROM SCRAPER get_abstract:\nABSTRACT:\n{abstract}\nEXTRA CONTEXT:\n{extra_context}\n\n"
                )
                self.logger.info("-" * 80)

                if "abstract" not in item:
                    item["abstract"] = abstract

                item["extra_context"] = extra_context

                num_processed += 1
                self.logger.info(
                    f"\n\n\nPROCESSED **{num_processed}/{len(self.result)}** ITEMS\n\n\n"
                )
                i += 1

            except Exception as e:
                self.logger.error(
                    f"Error processing URL {item.get('URL')}: {str(e)}"
                    "Popping item from result list and continuing"
                )
                self.result.pop(i)
                items_removed += 1
                continue

        self.logger.info(f"\n\nItems removed: {items_removed}\n\n")
        self.logger.info(
            f"Final data processing complete. Processed {num_processed} abstracts"
        )
        self.scraper.save_raw_results()
        return self

    def get_result_list(self) -> list[dict[str, Any]]:
        """Get the result list

        Returns:
            self.result (list[dict[str, Any]]): The result list
        """
        return self.result

    def run_all_process(
        self, save_offline: bool = False
    ) -> Union[None, List[Dict[str, Any]]]:
        """Run all data fetching and processing

        Args:
            save_offline (bool): Whether to save the offline data.

        Returns:
            Union[None, List[Dict[str, Any]]]: The result list or None
        """
        if save_offline:
            return (
                self.run_afetch_yrange()
                .final_data_process()
                .serialize_to_json("postProcess.json")
            )
        elif not self.run_scraper:
            return self.run_afetch_yrange().get_result_list()
        else:
            return self.run_afetch_yrange().final_data_process().get_result_list()


if __name__ == "__main__":
    wrap = CrossrefWrapper()
    data = wrap.run_all_process().get_data()

            ```

            src/academic_metrics/data_collection/__init__.py:
            ```
from .CrossrefWrapper import CrossrefWrapper
from .scraper import Scraper

            ```

            src/academic_metrics/data_collection/scraper.py:
            ```
import json
import logging
import time
import os
from typing import Any, Dict, List

from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager

from academic_metrics.ChainBuilder import ChainManager
from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)


# create the cleaner output model
class CleanerOutput(BaseModel):
    """Pydantic model for the cleaner output.

    Attributes:
        page_content (str): The page content.
        extra_context (Dict[str, Any]): The extra context.
    """

    page_content: str
    extra_context: Dict[str, Any]


class Scraper:
    """Scraper class for fetching and processing abstracts from URLs.

    Attributes:
        api_key (str): The OpenAI API key.
        client (OpenAI): The OpenAI client.
        options (Options): The Selenium options.
        service (Service): The Selenium service.
        raw_results (list[dict[str, Any]]): The raw results.

    Methods:
        _setup_selenium_options(): Set up Selenium Firefox options.
        setup_chain(output_list: list[str]) -> dict[str, Any] | None: Set up and run the chain.
        get_abstract(url: str, return_raw_output: bool | None = False) -> tuple[str | None, dict[str, Any] | None]: Fetch and process the abstract from a given URL.
        save_raw_results(): Save the raw results to a JSON file.
    """

    def __init__(
        self,
        api_key: str,
    ):
        """Initialize the Scraper with API key and logger.

        Args:
            api_key (str): The OpenAI API key.
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="scraper",
            log_level=DEBUG,
        )

        self.logger.info("Initializing Scraper")

        # Initialize OpenAI client
        self.api_key = api_key
        self.client = OpenAI(api_key=api_key)

        # Initialize Selenium options
        self.options = self._setup_selenium_options()
        self.service = Service(GeckoDriverManager().install())

        self.logger.info("Scraper initialized successfully")

        self.raw_results = []

        self.page_load_timeout = 30  # seconds
        self.script_timeout = 30  # seconds
        self.max_retries = 3
        self.base_retry_delay = 5  # seconds
        self.max_delay = 125  # seconds

    def _setup_selenium_options(self):
        """Set up Selenium Firefox options.

        Returns:
            options (Options): The Selenium options.
        """
        options: Options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        return options

    def setup_chain(self, output_list: List[str]) -> Dict[str, Any] | None:
        """Set up and run the chain.

        Args:
            output_list (List[str]): The output list.

        Returns:
            Dict[str, Any] | None: The result of the chain.
        """
        # instanciate the chain manager
        # google_api_key = os.getenv("GOOGLE_API_KEY")
        import time
        import threading

        start_time = time.time()
        stop_timer = threading.Event()

        def print_elapsed_time():
            while not stop_timer.is_set():
                elapsed = time.time() - start_time
                self.logger.info(f"Waiting for LLM response... {elapsed:.1f}s elapsed")
                time.sleep(5)

        llm_kwargs = {
            "request_timeout": 300.0,  # 5 minutes
            "max_retries": 3,
        }

        chain_manager = ChainManager(
            llm_model="gpt-4o-mini",
            api_key=self.api_key,
            llm_kwargs=llm_kwargs,
        )

        # create a general schema for the model to output text in
        json_schema = """
        {
            "page_content": <this is where you should place all text found on the page or "" if one is not found>,
            "extra_context": <this is a dictionary of key value pairs of extra content you find, always make the key indicative of what the extra content in the value is>
        }
        """

        json_example_missing_abstract = """
        {
            "page_content": "",
            "extra_context": {
                "keywords": ["keyword1", "keyword2", "keyword3"],
                "authors": ["author1", "author2", "author3"],
                "date": "2024-01-01",
                ...
            }
        }
        """

        # create the system prompt
        system_prompt = """
        You are an expert at parsing HTML and extracting text off the page and formatting it in a structured way. You will be provided HTML and you are to do the following:

        1) Find all text on the page and format it into markdown.
        2) Find any extra context available on the page such as keywords, authors, date, journal, etc.
        
        You are to format your results in the following json structure:
        {json_structure}

        If you find page content you are to put in the page_content section, **if you don't find any page content put an empty string**.
        
        Here is an example of what to do if you don't find any page content:
        {json_example_missing_abstract}

        For extra_context you are to put a key value pair of anything else you find. **If you do not find anything else then you are to still make 1 key value pair indicating so**. DO NOT BE LAZY.
        
        You should always find at least 1 extra context key value pair.

        In extra_context be congnizant of what type of element you're putting in there, for example if you find keywords, do you not just provide a string but rather a list of strings that are the keywords. Be mindful of these types of things at all times, do not limit it to this example.

        IMPORTANT: You are to always output your response in the json format provided, if you do not output your response in the format provided you have failed.
        IMPORTANT: DO NOT WRAP YOUR JSON IN ```json\njson content\n``` JUST RETURN THE JSON
        IMPORTANT: The page_content field is required and must always be provided, if you cannot find any page content STILL PUT AN EMPTY STRING IN THERE.
        IMPORTANT: extra_context IS NOT AN OPTIONAL FIELD YOUR ARE ALWAYS TO PROVIDE AT LEAST ONE KEY VALUE PAIR WITHIN IT. 
        IMPORTANT: ENSURE THAT THE JSON YOU PROVIDE IS VALID JSON. BEFORE RETURNING REVIEW THE JSON YOU HAVE CONSTRUCTED AND FIX ANY ERRORS IF THERE ARE ANY.
        """

        # setup ability to pass the list into the prompt
        human_prompt = """
        Output list:
        {output_list}
        """

        self.logger.info("Starting LLM processing...")

        timer_thread = threading.Thread(target=print_elapsed_time)
        timer_thread.start()

        try:
            # add the chain
            chain_manager.add_chain_layer(
                system_prompt=system_prompt,
                human_prompt=human_prompt,
                output_passthrough_key_name="raw_output",
                parser_type="pydantic",
                pydantic_output_model=CleanerOutput,
            )

            prompt_variables = {
                "output_list": str(output_list),
                "json_structure": json_schema,
                "json_example_missing_abstract": json_example_missing_abstract,
            }

            results = chain_manager.run(prompt_variables_dict=prompt_variables)[
                "raw_output"
            ]

            return {
                "abstract": results.page_content,
                "extra_context": results.extra_context,
            }
        except Exception as e:
            self.logger.error(f"LLM processing failed: {str(e)}")
            return None

        finally:
            stop_timer.set()
            timer_thread.join()
            elapsed_time = time.time() - start_time
            self.logger.info(f"LLM processing completed in {elapsed_time:.2f} seconds")

    def get_abstract(self, url, return_raw_output: bool | None = False):
        """
        Fetches and processes the abstract from a given URL.

        This function uses Selenium to fetch the content of the provided URL in headless mode.
        It then parses the HTML content using BeautifulSoup and attempts to find the abstract
        in common HTML tags such as <meta>, <p>, <div>, <article>, and <span>. The collected
        content is processed using a chain of prompts managed by the ChainManager.

        Args:
            url (str): The URL of the web page to fetch the abstract from.

        Returns:
            (abstract: str | None, extra_context: dict[str, Any] | None):
            - The processed abstract and additional context,
            - or None if no abstract is found or an error occurs.
        """
        driver = None
        retry_count: int = 0
        if url:
            while retry_count < self.max_retries:
                try:
                    self.logger.debug(f"Fetching URL: {url}")

                    self.logger.debug("Setting up driver")
                    try:
                        driver = webdriver.Firefox(
                            service=self.service, options=self.options
                        )
                        driver.set_page_load_timeout(self.page_load_timeout)
                        driver.set_script_timeout(self.script_timeout)
                    except Exception as e:
                        self.logger.error(f"Error setting up driver: {e}")
                        return None

                    self.logger.debug("Getting page content")
                    try:
                        driver.get(url)
                        page_content = driver.page_source
                    except Exception as e:
                        self.logger.error(f"Error getting page content: {e}")
                        retry_count += 1
                        if retry_count >= self.max_retries:
                            self.logger.error(
                                f"Max retries ({self.max_retries}) reached for URL: {url}"
                            )
                            return None

                        # Calculate exponential delay: 5s, 25s, 125s
                        # 5^2 = 25, 5^3 = 125
                        current_delay = min(
                            self.max_delay, self.base_retry_delay ** (retry_count + 1)
                        )
                        self.logger.info(
                            f"Retrying in {current_delay} seconds... Attempt {retry_count + 1} of {self.max_retries}"
                        )
                        time.sleep(current_delay)
                        continue

                    self.logger.debug(
                        f"Page content preview:\n\n{page_content[:50]}\n\n"
                    )

                    # self.logger.debug("Quitting driver")
                    # try:
                    #     driver.quit()
                    # except Exception as e:
                    #     self.logger.error(f"Error quitting driver: {e}")

                    self.logger.debug(f"Fetched page content for URL: {url}")

                    # Parse the HTML content using BeautifulSoup
                    try:
                        self.logger.debug("Initializing BeautifulSoup parser")
                        soup = BeautifulSoup(page_content, "html.parser")
                        self.logger.debug(
                            f"Page parsed. Found {len(soup.find_all())} total elements"
                        )

                        # Debug the structure
                        self.logger.debug(
                            f"Found {len(soup.find_all('meta'))} meta tags"
                        )
                        self.logger.debug(
                            f"Found {len(soup.find_all('p'))} paragraph tags"
                        )
                        self.logger.debug(f"Found {len(soup.find_all('div'))} div tags")
                    except Exception as e:
                        self.logger.error(f"Error parsing HTML content: {e}")
                        return None

                    # Attempt to find the abstract in common locations
                    output_list = []

                    # 1. <meta> tags
                    try:
                        meta_names = [
                            "citation_abstract",
                            "description",
                            "og:description",
                        ]
                        text = ""
                        for name in meta_names:
                            meta_tag = soup.find("meta", attrs={"name": name})
                            if meta_tag and "content" in meta_tag.attrs:
                                output_list.append(meta_tag["content"])
                                text += meta_tag["content"]
                        self.logger.info("Finished processing meta tags")
                    except Exception as e:
                        self.logger.error(f"Error processing meta tags: {e}")

                    # 2. <p> tags
                    try:
                        p_tags = soup.find_all("p")
                        for p in p_tags:
                            if "abstract" in p.get_text().lower():
                                output_list.append(p.get_text())
                        self.logger.info("Finished processing paragraph tags")
                    except Exception as e:
                        self.logger.error(f"Error processing paragraph tags: {e}")

                    # 3. <div> tags with specific classes or IDs
                    try:
                        div_classes = ["abstract", "article-abstract", "summary"]
                        for class_name in div_classes:
                            div_tags = soup.find_all("div", class_=class_name)
                            for div_tag in div_tags:
                                output_list.append(div_tag.get_text())
                        self.logger.info("Finished processing div tags")
                    except Exception as e:
                        self.logger.error(f"Error processing div tags: {e}")

                    # 4. <article> tags
                    try:
                        article_tags = soup.find_all("article")
                        for article_tag in article_tags:
                            output_list.append(article_tag.get_text())
                        self.logger.info("Finished processing article tags")
                    except Exception as e:
                        self.logger.error(f"Error processing article tags: {e}")

                    # 5. <span> tags
                    try:
                        span_tags = soup.find_all("span")
                        for span_tag in span_tags:
                            output_list.append(span_tag.get_text())
                        self.logger.info("Finished processing span tags")
                    except Exception as e:
                        self.logger.error(f"Error processing span tags: {e}")

                    # get the number of tokens scraped so far
                    try:
                        total_tokens = 0
                        for item in output_list:
                            total_tokens += len(item)
                        token_end = 100000 - total_tokens
                        self.logger.info(
                            f"Total tokens calculated: {total_tokens}, tokens remaining: {token_end}"
                        )
                    except Exception as e:
                        self.logger.error(f"Error calculating total tokens: {e}")

                    if return_raw_output:
                        total_words = sum(len(item.split()) for item in output_list)
                        estimated_tokens = total_words * 0.75
                        total_characters = sum(len(item) for item in output_list)
                        self.logger.info(f"Total words: {total_words}")
                        self.logger.info(f"Total characters: {total_characters}")
                        self.logger.info(f"Estimated tokens: {estimated_tokens}")
                        return (
                            output_list,
                            total_words,
                            estimated_tokens,
                            total_characters,
                        )

                    results = self.setup_chain(output_list)

                    if results is None:
                        return None, None

                    self.raw_results.append(results)
                    abstract = results["abstract"]
                    if abstract == "":
                        # If no abstract is found, return None and extra context
                        # Don't return extra context as sometimes an abstract is not found
                        # as the url is to a book or some non-academic research article item
                        return None, None
                    extra_context = results["extra_context"]
                    self.logger.debug(f"\n\nAbstract:\n{abstract}\n\n")
                    self.logger.debug(
                        f"Successfully processed abstract for DOI {url}\n{abstract}\n\n"
                    )

                    # Abstract and extra context found
                    return abstract, extra_context

                except Exception as e:
                    self.logger.error(f"Error fetching {url}: {e}")
                    return None, None

                finally:
                    if driver:
                        self.logger.debug("Quitting driver")
                        try:
                            driver.quit()
                        except Exception as e:
                            self.logger.error(f"Error quitting driver: {e}")
        return None, None

    def save_raw_results(self):
        """Save the raw results to a JSON file."""
        with open("raw_results.json", "w") as f:
            json.dump(self.raw_results, f)


# Example usage
if __name__ == "__main__":
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    scraper = Scraper(api_key=api_key)
    abstract, extra_context = scraper.get_abstract(
        "http://dx.doi.org/10.1111/hequ.12450"
    )
    print(abstract)
    print(extra_context)

            ```

            src/academic_metrics/dataclass_models/__init__.py:
            ```
from .abstract_base_dataclass import AbstractBaseDataClass
from .concrete_dataclasses import *
from .string_variation import StringVariation

            ```

            src/academic_metrics/dataclass_models/abstract_base_dataclass.py:
            ```
from abc import ABC
from dataclasses import asdict, dataclass, fields
from typing import Any, Dict, List, Set


@dataclass
class AbstractBaseDataClass(ABC):
    """
    Abstract base class for all data model classes providing common functionality.

    Methods:
        to_dict: Converts the dataclass to a dictionary, handling Set conversion for JSON serialization.
        set_params: Sets the parameters from a dictionary, handling type conversions.
    """

    def to_dict(self, exclude_keys: List[str] | None = None) -> dict:
        """Convert the dataclass to a dictionary, handling Set conversion for JSON serialization.

        Returns:
            dict: A dictionary representation of the dataclass.
        """
        data_dict = asdict(self)

        # Remove excluded keys if any
        if exclude_keys:
            for key in exclude_keys:
                data_dict.pop(key, None)

        def convert_sets(obj):
            if isinstance(obj, dict):
                return {k: convert_sets(v) for k, v in obj.items()}
            elif isinstance(obj, Set):
                return list(obj)
            return obj

        # Convert sets to lists, including those in nested dicts
        return convert_sets(data_dict)

    def set_params(self, params: Dict[str, Any], debug: bool = False) -> None:
        """
        Updates the dataclass fields, merging sets and handling nested updates.

        It handles:
        1. Converting lists to sets for fields annotated as Set
        2. Merging sets instead of overwriting
        3. Ignoring keys that don't match attributes
        4. Handling nested dataclass updates

        Args:
            params (Dict[str, Any]): A dictionary of parameters to update the dataclass fields.

        Examples:
            >>> class MyClass(AbstractBaseDataClass):
            ...     items: Set[str] = field(default_factory=set)
            >>> obj = MyClass()
            >>> obj.set_params({"items": ["a", "b"]})
            >>> obj.set_params({"items": ["c", "d"]})
            >>> sorted(list(obj.items))  # Contains all items
            ['a', 'b', 'c', 'd']
        """
        # Get fields from the concrete class, not the base class
        if debug:
            print(
                f"AbstractBaseDataClass.set_params called on {self.__class__.__name__}"
            )
            input()
            print(f"Fields: {fields(self.__class__)}")
            input()

        field_types = {field.name: field.type for field in fields(self.__class__)}
        if debug:
            print(f"Field types: {field_types}")
            input()
        for key, value in params.items():
            if debug:
                print(f"Processing {key} = {value}")
                input()
            if hasattr(self, key) and value is not None:
                current_value = getattr(self, key)
                if debug:
                    print(f"Current value of {key}: {current_value}")
                    input()

                # Handle Set fields
                if key in field_types and field_types[key] == Set[str]:
                    if debug:
                        print(f"{key} is a Set[str] field")
                        input()
                    # Convert input to set if needed
                    if isinstance(value, (List, Set)):
                        new_value = set(value)
                    else:
                        new_value = {str(value)}

                    # Merge with existing set
                    if isinstance(current_value, Set):
                        current_value.update(new_value)
                    else:
                        setattr(self, key, new_value)

                # Handle other fields normally
                else:
                    if debug:
                        print(f"{key} is not a Set[str] field")
                        input()
                    setattr(self, key, value)

            ```

            src/academic_metrics/dataclass_models/concrete_dataclasses.py:
            ```
from dataclasses import dataclass, field
from typing import Any, Dict, List, Set

from academic_metrics.enums import DataClassTypes
from academic_metrics.factories import DataClassFactory

from .abstract_base_dataclass import AbstractBaseDataClass


@dataclass
@DataClassFactory.register_dataclass(DataClassTypes.CATEGORY_INFO)
class CategoryInfo(AbstractBaseDataClass):
    """
    A dataclass representing information about an academic category.

    This class stores various metrics and details related to an academic category,
    including counts of faculty, departments, and articles, as well as sets of
    related entities and citation information.

    Attributes:
        _id (str): Unique identifier for the category
        url (str): A URL-friendly version of the category name
        category_name (str): Name of the category
        faculty_count (int): Number of faculty members in this category
        department_count (int): Number of departments in this category
        article_count (int): Number of articles in this category
        files (Set[str]): File names associated with this category
        faculty (Set[str]): Faculty names in this category
        departments (Set[str]): Department names in this category
        titles (Set[str]): Article titles in this category
        tc_count (int): Total citation count for articles
        tc_list (List[int]): Individual citation counts for articles
        citation_average (int): Average citations per article
        doi_list (Set[str]): List of DOIs for articles
        themes (Set[str]): Themes associated with this category
    """

    _id: str = ""
    url: str = ""
    category_name: str = ""
    faculty_count: int = 0
    department_count: int = 0
    article_count: int = 0
    files: Set[str] = field(default_factory=set)
    faculty: Set[str] = field(default_factory=set)
    departments: Set[str] = field(default_factory=set)
    titles: Set[str] = field(default_factory=set)
    tc_count: int = 0
    citation_average: int = 0
    doi_list: Set[str] = field(default_factory=set)
    themes: Set[str] = field(default_factory=set)


@dataclass
@DataClassFactory.register_dataclass(DataClassTypes.GLOBAL_FACULTY_STATS)
class GlobalFacultyStats(AbstractBaseDataClass):
    """
    A dataclass representing all of a faculty member's articles across all categories.

    Attributes:
        _id (str): Unique identifier for the faculty member
        name (str): Name of the faculty member
        total_citations (int): Total number of citations across all articles
        article_count (int): Total number of articles
        average_citations (int): Average citations per article
        department_affiliations (Set[str]): All department affiliations
        dois (Set[str]): All DOIs of faculty's articles
        titles (Set[str]): All article titles
        categories (Set[str]): All categories
        category_ids (Set[str]): All category IDs
        top_level_categories (Set[str]): High-level category classifications
        mid_level_categories (Set[str]): Mid-level category classifications
        low_level_categories (Set[str]): Detailed category classifications
        themes (Set[str]): Research themes
        citation_map (Dict[str, int]): Mapping of articles to citation counts
        journals (Set[str]): All journals published in
    """

    _id: str = field(default="")
    name: str = field(default="")
    total_citations: int = 0
    article_count: int = 0
    average_citations: int = 0
    department_affiliations: Set[str] = field(default_factory=set)
    dois: Set[str] = field(default_factory=set)
    titles: Set[str] = field(default_factory=set)
    categories: Set[str] = field(default_factory=set)
    top_level_categories: Set[str] = field(default_factory=set)
    mid_level_categories: Set[str] = field(default_factory=set)
    low_level_categories: Set[str] = field(default_factory=set)
    category_urls: Set[str] = field(default_factory=set)
    top_category_urls: Set[str] = field(default_factory=set)
    mid_category_urls: Set[str] = field(default_factory=set)
    low_category_urls: Set[str] = field(default_factory=set)
    themes: Set[str] = field(default_factory=set)
    citation_map: Dict[str, int] = field(default_factory=dict)
    journals: Set[str] = field(default_factory=set)


@dataclass
@DataClassFactory.register_dataclass(DataClassTypes.FACULTY_INFO)
class FacultyInfo(AbstractBaseDataClass):
    """
    A dataclass representing detailed information about a faculty member.

    Attributes:
        _id (str): Unique identifier for faculty member
        name (str): Faculty member's name
        category (str): Associated category
        category_id (str): Category identifier
        total_citations (int): Total number of citations for all articles
        article_count (int): Number of articles authored
        average_citations (int): Average citations per article
        titles (Set[str]): Set of article titles
        dois (Set[str]): Set of DOIs for articles
        department_affiliations (Set[str]): Departments affiliated with
        doi_citation_map (Dict[str, int]): Maps DOIs to citation counts
    """

    _id: str = field(default="")
    name: str = field(default="")
    category: str = field(default="")
    category_url: str = field(default="")
    total_citations: int = 0
    article_count: int = 0
    average_citations: int = 0
    titles: Set[str] = field(default_factory=set)
    dois: Set[str] = field(default_factory=set)
    department_affiliations: Set[str] = field(default_factory=set)
    doi_citation_map: Dict[str, int] = field(default_factory=dict)


@dataclass
@DataClassFactory.register_dataclass(DataClassTypes.FACULTY_STATS)
class FacultyStats(AbstractBaseDataClass):
    """
    A dataclass representing statistics for all faculty members.

    Attributes:
        faculty_stats (Dict[str, FacultyInfo]): Maps faculty names to their info
    """

    faculty_stats: Dict[str, FacultyInfo] = field(default_factory=dict)

    def refine_faculty_stats(
        self, *, faculty_name_unrefined: str, variations: Dict[str, Any]
    ) -> None:
        """
        Refines faculty statistics by updating faculty names based on variations.

        Args:
            faculty_name_unrefined (str): Original faculty name
            name_variations (Dict[str, Any]): Dictionary of name variations
        """
        refined_name = self.get_refined_faculty_name(faculty_name_unrefined, variations)
        if faculty_name_unrefined in self.faculty_stats:
            self.faculty_stats[refined_name] = self.faculty_stats.pop(
                faculty_name_unrefined
            )

    def get_refined_faculty_name(
        self, unrefined_name: str, variations: Dict[str, Any]
    ) -> str:
        """
        Gets the refined name for a faculty member.

        Args:
            unrefined_name (str): Original faculty name
            name_variations (Dict[str, Any]): Dictionary of name variations

        Returns:
            str: Refined faculty name
        """
        for _, variation in variations.items():
            if unrefined_name in variation.variations:
                return variation.most_frequent_variation()
        return unrefined_name

    def set_params(self, params: Dict[str, Any]) -> None:
        """
        Override set_params to handle the nested FacultyInfo dictionary.

        Args:
            params (Dict[str, Any]):
            - Dictionary that can include either a full faculty_stats dictionary or direct updates to individual faculty members.

        Examples:
            Case 1 - Full faculty_stats dictionary:
            >>> faculty_stats = DataClassFactory.get_dataclass(DataClassTypes.FACULTY_STATS)
            >>> faculty_stats.set_params({
            ...     "faculty_stats": {
            ...         "Dr. Smith": {"total_citations": 100, "article_count": 5},
            ...         "Dr. Jones": {"total_citations": 50, "article_count": 3}
            ...     }
            ... })

            Case 2 - Direct faculty member updates:
            >>> faculty_stats = DataClassFactory.get_dataclass(DataClassTypes.FACULTY_STATS)
            >>> faculty_stats.set_params({
            ...     "Dr. Smith": {"total_citations": 100, "article_count": 5}
            ... })
        """
        # Case 1: If params contains a full faculty_stats dictionary
        if "faculty_stats" in params:
            faculty_data = params["faculty_stats"]
        # Case 2: If params is direct faculty member data
        else:
            faculty_data = params

        # Update faculty info for each member
        for name, info in faculty_data.items():
            # Create FacultyInfo if it doesn't exist
            if name not in self.faculty_stats:
                self.faculty_stats[name] = DataClassFactory.get_dataclass(
                    DataClassTypes.FACULTY_INFO
                )

            # Update the faculty info
            if isinstance(info, dict):
                self.faculty_stats[name].set_params(info)
            elif isinstance(info, FacultyInfo):
                self.faculty_stats[name] = info


@dataclass
@DataClassFactory.register_dataclass(DataClassTypes.ARTICLE_DETAILS)
class ArticleDetails(AbstractBaseDataClass):
    """
    A dataclass representing details about an individual article.

    Attributes:
        tc_count (int): Total citation count for the article
        faculty_members (Set[str]): Faculty members associated with article
        faculty_affiliations (Dict[str, List[str]]): Maps faculty to affiliations
        abstract (str): Article abstract
        license_url (str): URL to article license
        date_published_print (str): Print publication date
        date_published_online (str): Online publication date
        journal (str): Journal name
        download_url (str): URL to download article
        doi (str): Digital Object Identifier
    """

    tc_count: int = 0
    faculty_members: Set[str] = field(default_factory=set)
    faculty_affiliations: Dict[str, List[str]] = field(default_factory=dict)
    abstract: str = field(default="")
    license_url: str = field(default="")
    date_published_print: str = field(default="")
    date_published_online: str = field(default="")
    journal: str = field(default="")
    download_url: str = field(default="")
    doi: str = field(default="")


@dataclass
@DataClassFactory.register_dataclass(DataClassTypes.ARTICLE_STATS)
class ArticleStats(AbstractBaseDataClass):
    """
    A dataclass representing statistics for all articles.

    Attributes:
        article_citation_map (Dict[str, ArticleDetails]): Maps article titles to details

    Examples:
        >>> article_stats = DataClassFactory.get_dataclass(DataClassTypes.ARTICLE_STATS)
        >>> article_stats.set_params({
        ...     "article_citation_map": {
        ...         "Article Title": {
        ...             "tc_count": 10,
        ...             "faculty_members": {"Dr. Smith", "Dr. Jones"},
        ...             "journal": "Nature"
        ...         }
        ...     }
        ... })
    """

    article_citation_map: Dict[str, ArticleDetails] = field(default_factory=dict)

    def set_params(self, params: Dict[str, Any]) -> None:
        """
        Override set_params to handle the nested ArticleDetails dictionary.

        Args:
            params (Dict[str, Any]): Dictionary containing article data

        Examples:
            >>> article_stats = DataClassFactory.get_dataclass(DataClassTypes.ARTICLE_STATS)
            >>> article_stats.set_params({
            ...     "Article Title": {
            ...         "tc_count": 10,
            ...         "faculty_members": {"Dr. Smith"},
            ...         "journal": "Nature"
            ...     }
            ... })
        """
        # Case 1: If params contains a full article_citation_map
        if "article_citation_map" in params:
            article_data = params["article_citation_map"]
        # Case 2: If params is direct article data
        else:
            article_data = params

        # Update article details for each article
        for title, details in article_data.items():
            # Create ArticleDetails if it doesn't exist
            if title not in self.article_citation_map:
                self.article_citation_map[title] = DataClassFactory.get_dataclass(
                    DataClassTypes.ARTICLE_DETAILS
                )

            # Update the article details
            if isinstance(details, dict):
                self.article_citation_map[title].set_params(details)
            elif isinstance(details, ArticleDetails):
                self.article_citation_map[title] = details


@dataclass
@DataClassFactory.register_dataclass(DataClassTypes.CROSSREF_ARTICLE_DETAILS)
class CrossrefArticleDetails(AbstractBaseDataClass):
    """
    A dataclass representing details about an individual article from Crossref.

    Attributes:
        _id (str): Unique identifier
        title (str): Article title
        tc_count (int): Total citation count
        faculty_members (Set[str]): Faculty members associated with article
        faculty_affiliations (Dict[str, List[str]]): Maps faculty to affiliations
        abstract (str): Article abstract
        license_url (str): URL to article license
        date_published_print (str): Print publication date
        date_published_online (str): Online publication date
        journal (str): Journal name
        download_url (str): URL to download article
        doi (str): Digital Object Identifier
        themes (Set[str]): Research themes
        categories (Set[str]): Article categories
        category_ids (Set[str]): Category identifiers
        top_level_categories (Set[str]): High-level categories
        mid_level_categories (Set[str]): Mid-level categories
        low_level_categories (Set[str]): Detailed categories
    """

    _id: str = field(default="")
    title: str = field(default="")
    tc_count: int = 0
    faculty_members: Set[str] = field(default_factory=set)
    faculty_affiliations: Dict[str, List[str]] = field(default_factory=dict)
    abstract: str = field(default="")
    license_url: str = field(default="")
    date_published_print: str = field(default="")
    date_published_online: str = field(default="")
    journal: str = field(default="")
    download_url: str = field(default="")
    doi: str = field(default="")
    themes: Set[str] = field(default_factory=set)
    categories: Set[str] = field(default_factory=set)
    category_urls: Set[str] = field(default_factory=set)
    top_level_categories: Set[str] = field(default_factory=set)
    mid_level_categories: Set[str] = field(default_factory=set)
    low_level_categories: Set[str] = field(default_factory=set)
    top_category_urls: Set[str] = field(default_factory=set)
    mid_category_urls: Set[str] = field(default_factory=set)
    low_category_urls: Set[str] = field(default_factory=set)
    url: str = field(default="")


@dataclass
@DataClassFactory.register_dataclass(DataClassTypes.CROSSREF_ARTICLE_STATS)
class CrossrefArticleStats(AbstractBaseDataClass):
    """
    A dataclass representing statistics for all Crossref articles.

    Attributes:
        article_citation_map (Dict[str, CrossrefArticleDetails]): Maps DOIs to article details

    Examples:
        >>> stats = DataClassFactory.get_dataclass(DataClassTypes.CROSSREF_ARTICLE_STATS)
        >>> stats.set_params({
        ...     "article_citation_map": {
        ...         "10.1234/nature12345": {
        ...             "title": "Research Paper",
        ...             "tc_count": 10,
        ...             "faculty_members": {"Dr. Smith"},
        ...             "themes": {"AI", "ML"}
        ...         }
        ...     }
        ... })
    """

    article_citation_map: Dict[str, CrossrefArticleDetails] = field(
        default_factory=dict
    )

    def set_params(self, params: Dict[str, Any], debug: bool = False) -> None:
        """
        Override set_params to handle the nested CrossrefArticleDetails dictionary.

        Args:
            params (Dict[str, Any]): Dictionary containing article data

        Examples:
            >>> stats = DataClassFactory.get_dataclass(DataClassTypes.CROSSREF_ARTICLE_STATS)
            >>> stats.set_params({
            ...     "10.1234/nature12345": {
            ...         "title": "Research Paper",
            ...         "tc_count": 10,
            ...         "faculty_members": {"Dr. Smith"}
            ...     }
            ... })
        """
        if debug:
            print(f"set_params called with params: {params}")
            print(
                f"Current article_citation_map type: {type(self.article_citation_map)}"
            )
            print(f"Current article_citation_map value: {self.article_citation_map}")
        # Case 1: If params contains a full article_citation_map
        if "article_citation_map" in params:
            article_data = params["article_citation_map"]
        # Case 2: If params is direct article data
        else:
            article_data = params

        # Update article details for each DOI
        for doi, details in article_data.items():
            # Create CrossrefArticleDetails if it doesn't exist
            if doi not in self.article_citation_map.keys():
                self.article_citation_map[doi] = DataClassFactory.get_dataclass(
                    DataClassTypes.CROSSREF_ARTICLE_DETAILS
                )

            # Update the article details
            if isinstance(details, dict):
                self.article_citation_map[doi].set_params(details)
            elif isinstance(details, CrossrefArticleDetails):
                self.article_citation_map[doi] = details

            ```

            src/academic_metrics/dataclass_models/string_variation.py:
            ```
from __future__ import annotations
from dataclasses import dataclass, field
from typing import Dict
import logging
from academic_metrics.configs import configure_logging, DEBUG


@dataclass
class StringVariation:
    """
    A data class for tracking different spelling variations of a normalized name and determining the most frequent variation.

    Attributes:
        normalized_name (str): The base form of the name, typically normalized to lower case and stripped of spaces.
        variations (dict[str, int]): A dictionary where keys are variations of the name and values are the counts of how often each variation occurs.

    Methods:
        add_variation(variation: str): Adds a variation of the name to the dictionary or increments its count if it already exists.
        most_frequent_variation(): Returns the variation of the name that occurs most frequently.
    """

    normalized_value: str
    variations: Dict[str, int] = field(default_factory=dict)
    logger: logging.Logger = field(init=False)

    def __post_init__(self):
        """Initialize after dataclass fields are set."""
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="string_variation",
            log_level=DEBUG,
        )

    def add_variation(self, variation: str) -> None:
        """
        Adds a variation of the name to the dictionary or increments its count if it already exists.

        Args:
            variation (str): A specific spelling variation of the name.

        This method modifies the variations dictionary by either adding a new key with a count of 1 or incrementing the count of an existing key.
        """
        # Check if the variation is already in the dictionary and increment its count, otherwise add it with a count of 1
        self.logger.info(
            f"Checking if the variation {variation} is already in the dictionary and incrementing its count..."
        )
        if variation in self.variations:
            self.logger.info(
                f"Variation {variation} already in dictionary, incrementing count..."
            )
            self.variations[variation] += 1
            return
        self.logger.info(
            f"Variation {variation} not in dictionary, adding it with a count of 1..."
        )
        self.variations[variation] = 1

    def most_frequent_variation(self) -> str:
        """
        Returns the variation of the name that occurs most frequently.

        This method finds the key with the highest value in the variations dictionary, which represents the most common spelling variation.
        If the variations dictionary is empty, it raises a ValueError indicating that no variations have been added.

        Returns:
            str: The most frequent variation of the name.

        Raises:
            ValueError: If no variations are found in the dictionary, detailing the normalized name associated with the error.
        """
        # Check if the variations dictionary is empty and raise an error if it is
        self.logger.info(f"Checking if the variations dictionary is empty...")
        if not self.variations:
            self.logger.error(
                f"No variations found for the normalized value: '{self.normalized_value}', raising ValueError..."
            )
            raise ValueError(
                f"No variations found for the normalized value: '{self.normalized_value}'."
            )
        self.logger.info(f"Variations dictionary is not empty.")

        # Return the key (variation) with the maximum value (count) in the variations dictionary
        self.logger.info(
            f"Returning the key (variation) with the maximum value (count) in the variations dictionary..."
        )
        return max(self.variations, key=self.variations.get)

            ```

            src/academic_metrics/enums/__init__.py:
            ```
from .dataclass_enums import DataClassTypes
from .enums import AttributeTypes

            ```

            src/academic_metrics/enums/dataclass_enums.py:
            ```
from enum import Enum


class DataClassTypes(Enum):
    """Enum for the different types of data classes.

    Attributes:
        CATEGORY_INFO (str): Category information.
        GLOBAL_FACULTY_STATS (str): Global faculty statistics.
        FACULTY_INFO (str): Faculty information.
        FACULTY_STATS (str): Faculty statistics.
        ARTICLE_DETAILS (str): Article details.
        ARTICLE_STATS (str): Article statistics.
        CROSSREF_ARTICLE_DETAILS (str): Crossref article details.
        CROSSREF_ARTICLE_STATS (str): Crossref article statistics.
    """

    CATEGORY_INFO = "category_info"
    GLOBAL_FACULTY_STATS = "global_faculty_stats"
    FACULTY_INFO = "faculty_info"
    FACULTY_STATS = "faculty_stats"
    ARTICLE_DETAILS = "article_details"
    ARTICLE_STATS = "article_stats"
    CROSSREF_ARTICLE_DETAILS = "crossref_article_details"
    CROSSREF_ARTICLE_STATS = "crossref_article_stats"

            ```

            src/academic_metrics/enums/enums.py:
            ```
from enum import Enum


class AttributeTypes(Enum):
    """Enum for the different types of attributes.

    Attributes:
        AUTHOR (str): Author.
        TITLE (str): Title.
        ABSTRACT (str): Abstract.
        END_RECORD (str): End record.
        WC_PATTERN (str): Word count pattern.
        DEPARTMENT (str): Department.
        CROSSREF_TITLE (str): Crossref title.
        CROSSREF_ABSTRACT (str): Crossref abstract.
        CROSSREF_AUTHORS (str): Crossref authors.
        CROSSREF_DEPARTMENTS (str): Crossref departments.
        CROSSREF_CATEGORIES (str): Crossref categories.
        CROSSREF_URL (str): Crossref URL.
        CROSSREF_CITATION_COUNT (str): Crossref citation count.
        CROSSREF_LICENSE_URL (str): Crossref license URL.
        CROSSREF_PUBLISHED_PRINT (str): Crossref published print.
        CROSSREF_CREATED_DATE (str): Crossref created date.
        CROSSREF_PUBLISHED_ONLINE (str): Crossref published online.
        CROSSREF_JOURNAL (str): Crossref journal.
        CROSSREF_DOI (str): Crossref DOI.
        CROSSREF_THEMES (str): Crossref themes.
        CROSSREF_EXTRA_CONTEXT (str): Crossref extra context.
    """

    AUTHOR = "author"
    TITLE = "title"
    ABSTRACT = "abstract"
    END_RECORD = "end_record"
    WC_PATTERN = "wc_pattern"
    DEPARTMENT = "department"
    CROSSREF_TITLE = "crossref-title"
    CROSSREF_ABSTRACT = "crossref-abstract"
    CROSSREF_AUTHORS = "crossref-authors"
    CROSSREF_DEPARTMENTS = "crossref-departments"
    CROSSREF_CATEGORIES = "crossref-categories"
    CROSSREF_CITATION_COUNT = "crossref-citation-count"
    CROSSREF_LICENSE_URL = "crossref-license-url"
    CROSSREF_PUBLISHED_PRINT = "crossref-published-print"
    CROSSREF_CREATED_DATE = "crossref-created-date"
    CROSSREF_PUBLISHED_ONLINE = "crossref-published-online"
    CROSSREF_JOURNAL = "crossref-journal"
    CROSSREF_URL = "crossref-url"
    CROSSREF_DOI = "crossref-doi"
    CROSSREF_THEMES = "crossref-themes"
    CROSSREF_EXTRA_CONTEXT = "crossref-extra-context"

            ```

            src/academic_metrics/factories/__init__.py:
            ```
from .dataclass_factory import DataClassFactory
from .abstract_classifier_factory import ClassifierFactory
from .strategy_factory import StrategyFactory

            ```

            src/academic_metrics/factories/abstract_classifier_factory.py:
            ```
# abstract_classifier_factory.py
from __future__ import annotations

import logging
import os
from typing import TYPE_CHECKING, Dict

if TYPE_CHECKING:
    from academic_metrics.utils.taxonomy_util import Taxonomy

from academic_metrics.AI import AbstractClassifier
from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)


class ClassifierFactory:
    """Factory for creating AbstractClassifier instances.

    Attributes:
        logger (logging.Logger): Logger for the factory.
        taxonomy (Taxonomy): Taxonomy for the classifier.
        ai_api_key (str): API key for the classifier.

    Methods:
        abstract_classifier_factory(
            self,
            doi_abstract_dict: Dict[str, str],
            extra_context: dict | None = None,
            pre_classification_model: str | None = None,
            classification_model: str | None = None,
            theme_model: str | None = None,
        ) -> AbstractClassifier:
            Creates an AbstractClassifier instance.
    """

    def __init__(
        self,
        taxonomy: Taxonomy,
        ai_api_key: str,
    ):
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="abstract_classifier_factory",
            log_level=DEBUG,
        )

        self.logger.info("Initializing ClassifierFactory")

        self.taxonomy: Taxonomy = taxonomy
        self.ai_api_key: str = ai_api_key

        self.logger.info("ClassifierFactory initialized successfully")

    def abstract_classifier_factory(
        self,
        doi_abstract_dict: Dict[str, str],
        extra_context: dict | None = None,
        pre_classification_model: str | None = "gpt-4o-mini",
        classification_model: str | None = "gpt-4o-mini",
        theme_model: str | None = "gpt-4o-mini",
    ) -> AbstractClassifier:
        """Creates an AbstractClassifier instance.

        Args:
            doi_abstract_dict (Dict[str, str]): Dictionary of DOIs and abstracts.
            extra_context (dict | None): Extra context for the classifier.
            pre_classification_model (str | None): Pre-classification model for the classifier.
            classification_model (str | None): Classification model for the classifier.
            theme_model (str | None): Theme model for the classifier.

        Returns:
            classifier (AbstractClassifier): An AbstractClassifier instance.
        """
        self.logger.info("Creating AbstractClassifier")
        classifier: AbstractClassifier = AbstractClassifier(
            taxonomy=self.taxonomy,
            doi_to_abstract_dict=doi_abstract_dict,
            api_key=self.ai_api_key,
            extra_context=extra_context,
            pre_classification_model=pre_classification_model,
            classification_model=classification_model,
            theme_model=theme_model,
        )
        self.logger.info("AbstractClassifier created successfully")
        return classifier

            ```

            src/academic_metrics/factories/dataclass_factory.py:
            ```
import logging
import os
from dataclasses import dataclass
from typing import Dict, Type

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)
from academic_metrics.enums import DataClassTypes


class DataClassFactory:
    """Factory for creating and managing dataclass instances.

    This class provides centralized creation, registration, and management of dataclasses
    used throughout the application. It maintains a registry of dataclass types and
    handles their instantiation with proper initialization.

    Attributes:
        _registry (Dict[str, Type[dataclass]]): Internal registry mapping dataclass types to their classes.
        logger (logging.Logger): Logger instance for this class.
        log_file_path (str): Path to the log file.

    Methods:
        register_dataclass: Decorator to register a dataclass with the factory.
        get_dataclass: Creates a new instance of a registered dataclass.
        is_registered: Checks if a dataclass type is registered.
    """

    _registry: Dict[str, Type[dataclass]] = {}

    def __init__(self):
        """Initialize the DataClassFactory with logging configuration.

        Sets up both file and console handlers for logging factory operations.
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="dataclass_factory",
            log_level=DEBUG,
        )

    @classmethod
    def register_dataclass(cls, dataclass_type: DataClassTypes):
        """
        Decorator to register a dataclass with the factory.

        Args:
            dataclass_type (DataClassTypes): The type of dataclass to register.
        """

        def decorator(data_class: Type):
            """Decorator to register a dataclass with the factory.

            Args:
                data_class (Type): The dataclass to register.

            Returns:
                data_class (Type): The dataclass to register.
            """
            # Apply the @dataclass decorator if not already applied
            # if not hasattr(data_class, '__dataclass_fields__'):
            #     data_class = dataclass(data_class)
            # Register the dataclass using the enum value as the key
            cls._registry[dataclass_type.value] = data_class
            return data_class

        return decorator

    @classmethod
    def get_dataclass(
        cls, dataclass_type: DataClassTypes, **init_params
    ) -> Type[dataclass]:
        """
        Creates a new instance of the registered dataclass.

        Args:
            dataclass_type (DataClassTypes): The enum type of the dataclass to create
            init_params (dict): Parameters to initialize the dataclass

        Returns:
            instance (Type[dataclass]): An instance of the requested dataclass

        Raises:
            ValueError: If no dataclass is registered for the given type
        """
        data_class = cls._registry.get(dataclass_type.value)

        if not data_class:
            raise ValueError(f"No dataclass registered for type: {dataclass_type}")

        # First create instance with no params to ensure proper initialization
        instance = data_class()

        # Then set any provided parameters
        if init_params:
            instance.set_params(init_params)

        return instance

    @classmethod
    def is_registered(cls, dataclass_type: DataClassTypes) -> bool:
        """
        Check if a dataclass type is registered.

        Args:
            dataclass_type (DataClassTypes): The enum type to check

        Returns:
            bool: True if the dataclass is registered, False otherwise
        """
        return dataclass_type.value in cls._registry

            ```

            src/academic_metrics/factories/strategy_factory.py:
            ```
from __future__ import annotations

import logging
import os
from typing import TYPE_CHECKING

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)
from academic_metrics.enums import AttributeTypes
from academic_metrics.utils import WarningManager

if TYPE_CHECKING:
    from academic_metrics.strategies import AttributeExtractionStrategy


class StrategyFactory:
    """
    A factory class for managing and retrieving attribute extraction strategies.

    This class provides a mechanism to register and retrieve different strategies for extracting attributes from data entries. It uses a dictionary to map attribute types to their corresponding strategy classes, allowing for flexible and dynamic strategy management.

    Attributes:
        _strategies (dict): A class-level dictionary that maps attribute types to their corresponding strategy classes.

    Methods:
        register_strategy(*attribute_types): Registers a strategy class for one or more attribute types.
        get_strategy(attribute_type, warning_manager): Retrieves the strategy class for a given attribute type and initializes it with a warning manager.

    Usage:
    - Add a strategy to the factory:
    - StrategyFactory.register_strategy(AttributeTypes.TITLE)(TitleExtractionStrategy)
    - Add the enum to enums.py
    - get a strategy from the factory:
    - get_attributes() in utilities.py will then use this factory to get the strategy for a given attribute type.
    """

    _strategies = {}

    def __init__(self):
        """Initializes the StrategyFactory."""
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="strategy_factory",
            log_level=DEBUG,
        )

    @classmethod
    def register_strategy(
        cls,
        *attribute_types: AttributeTypes,
    ):
        """
        Registers a strategy class for one or more attribute types.

        This method is used to associate a strategy class with specific attribute types. The strategy class
        is stored in the _strategies dictionary, allowing it to be retrieved later based on the attribute type.

        Args:
            *attribute_types (AttributeTypes): One or more attribute types to associate with the strategy class.

        Returns:
            function: A decorator function that registers the strategy class.
        """

        def decorator(strategy_class):
            for attribute_type in attribute_types:
                cls._strategies[attribute_type] = strategy_class
            return strategy_class

        return decorator

    @classmethod
    def get_strategy(
        cls, attribute_type: AttributeTypes, warning_manager: WarningManager
    ):
        """
        Retrieves the strategy class for a given attribute type and initializes it with a warning manager.

        This method looks up the strategy class associated with the specified attribute type in the _strategies
        dictionary. If a strategy class is found, it is instantiated with the provided warning manager and returned.

        Args:
            attribute_type (AttributeTypes):
            - The attribute type for which to retrieve the strategy class.

            warning_manager (WarningManager):
            - An instance of WarningManager to be passed to the strategy class.

        Returns:
            strategy (AttributeExtractionStrategy):
            - An instance of the strategy class associated with the specified attribute type.

        Raises:
            ValueError:
            - If no strategy is found for the specified attribute type.
        """
        strategy_class: AttributeExtractionStrategy = cls._strategies.get(
            attribute_type
        )
        if not strategy_class:
            raise ValueError(f"No strategy found for attribute type: {attribute_type}")
        return strategy_class(warning_manager)

            ```

            src/academic_metrics/mapping/_AbstractCategoryMap.py:
            ```
import json
import os

from academic_metrics.enums import AttributeTypes
from academic_metrics.utils import Utilities, WarningManager


class AbstractCategoryMap:
    """
    A class for mapping abstracts to their corresponding categories.

    This class processes a directory of files, extracting abstracts, categories, and titles
    from each file, and creates a mapping between them. The results are stored in a JSON file.

    Attributes:
        utilities (Utilities): An instance of the Utilities class for various utility operations.
        warning_manager (WarningManager): An instance of WarningManager for handling warnings.
        dir_path (str): The directory path containing the files to be processed.
        results (dict): A dictionary storing the mapping of titles to abstracts and categories.

    Methods:
        map_abstract_categories: Processes files in the directory to create the abstract-category mapping.
        write_json: Writes the results to a JSON file.

    Design:
        Uses utility methods to read files and extract attributes.
        Implements a mapping process to associate abstracts with their categories and titles.
        Stores results in a structured dictionary format.

    Summary:
        Provides functionality to create a mapping between abstracts, categories, and titles
        from a set of files, and stores this mapping in a JSON format.
    """

    def __init__(
        self,
        *,
        utilities_obj: Utilities,
        warning_manager: WarningManager,
        dir_path: str,
        crossref_bool: bool,
    ):
        """
        Initializes the AbstractCategoryMap instance.

        This constructor sets up the necessary components and initiates the mapping process.

        Args:
            utilities_obj (Utilities): An instance of the Utilities class.
            warning_manager (WarningManager): An instance of WarningManager for handling warnings.
            dir_path (str): The directory path containing the files to be processed.
            crossref_bool (bool): A boolean indicating whether to use Crossref data (currently unused).

        Returns:
            None

        Design:
            Initializes class attributes with provided arguments.
            Calls map_abstract_categories to process the files and create the mapping.
            Writes the results to a JSON file.

        Summary:
            Sets up the AbstractCategoryMap instance and performs the initial mapping process.
        """
        self.utilities = utilities_obj
        self.warning_manager = warning_manager
        self.dir_path = dir_path
        self.results = self.map_abstract_categories(dir_path=self.dir_path)
        self.write_json("abstracts_to_categories.json", self.results)

    def map_abstract_categories(self, *, dir_path: str):
        """
        Maps abstracts to their corresponding categories and titles.

        This method processes each file in the specified directory, extracting
        abstracts, categories, and titles, and creates a mapping between them.

        Args:
            dir_path (str): The directory path containing the files to be processed.

        Returns:
            dict: A dictionary where keys are titles and values are dictionaries
                  containing the abstract and categories for each title.

        Design:
            Iterates through files in the specified directory.
            Uses utility methods to extract relevant attributes from each file.
            Creates a structured dictionary mapping titles to abstracts and categories.

        Summary:
            Processes files to create a mapping of titles to their abstracts and categories.
        """
        results = {}
        for filename in os.listdir(dir_path):
            file_path = os.path.join(dir_path, filename)
            if not os.path.isfile(file_path):
                continue

            file_content = self.file_ops.read_file(file_path)
            attributes = self.utilities.get_attributes(
                file_content,
                [
                    AttributeTypes.ABSTRACT,
                    AttributeTypes.WC_PATTERN,
                    AttributeTypes.TITLE,
                ],
            )

            abstract = (
                attributes[AttributeTypes.ABSTRACT][1]
                if attributes[AttributeTypes.ABSTRACT][0]
                else None
            )
            categories = (
                attributes[AttributeTypes.WC_PATTERN][1]
                if attributes[AttributeTypes.WC_PATTERN][0]
                else []
            )
            title = (
                attributes[AttributeTypes.TITLE][1]
                if attributes[AttributeTypes.TITLE][0]
                else None
            )

            if abstract:
                results[title] = {"abstract": abstract, "categories": categories}

        return results

    def write_json(self, filename: str, data: dict):
        """
        Writes the provided data to a JSON file.

        This method serializes the given dictionary data into a JSON format
        and writes it to a file with the specified filename.

        Args:
            filename (str): The name of the file to write the JSON data to.
            data (dict): The dictionary data to be written to the JSON file.

        Returns:
            None
        """
        with open(filename, "w") as f:
            json.dump(data, f, indent=4)

            ```

            src/academic_metrics/mapping/__init__.py:
            ```
from ._AbstractCategoryMap import AbstractCategoryMap

            ```

            src/academic_metrics/orchestrators/__init__.py:
            ```
from .category_data_orchestrator import CategoryDataOrchestrator
from .classification_orchestrator import (
    ClassificationOrchestrator,
    ClassificationResultsDict,
    ClassificationResultsTuple,
)

            ```

            src/academic_metrics/orchestrators/category_data_orchestrator.py:
            ```
from __future__ import annotations

import json
import logging
import os
from typing import TYPE_CHECKING, Dict, List, Union

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)
from academic_metrics.dataclass_models import CategoryInfo, FacultyStats

if TYPE_CHECKING:
    from academic_metrics.core import (
        CategoryProcessor,
    )
    from academic_metrics.dataclass_models import (
        CrossrefArticleDetails,
        CrossrefArticleStats,
        StringVariation,
    )
    from academic_metrics.factories import (
        DataClassFactory,
        StrategyFactory,
        Utilities,
        WarningManager,
    )
    from academic_metrics.postprocessing import (
        DepartmentPostprocessor,
        FacultyPostprocessor,
    )


class CategoryDataOrchestrator:
    """Orchestrates the processing and organization of academic publication data.

    This class manages the workflow of processing classified publication data through various stages:
    1. Processing raw data through CategoryProcessor
    2. Managing faculty/department relationships
    3. Generating statistical outputs
    4. Serializing results to JSON files

    Attributes:
        data (List[Dict]): Raw classified publication data to process.
        output_dir_path (str): Directory path for output files.
        extend (bool): Whether to extend existing data files.
        strategy_factory (StrategyFactory): Factory for creating processing strategies.
        warning_manager (WarningManager): System for handling and logging warnings.
        dataclass_factory (DataClassFactory): Factory for creating data model instances.
        utils (Utilities): General utility functions.
        category_processor (CategoryProcessor): Processor for category-related operations.
        faculty_postprocessor (FacultyPostprocessor): Processor for faculty data refinement.
        final_category_data (List[Dict]): Processed category statistics.
        final_faculty_data (List[Dict]): Processed faculty statistics.
        final_article_stats_data (List[Dict]): Processed article statistics.
        final_article_data (List[Dict]): Processed article details.
        final_global_faculty_data (List[Dict]): Processed global faculty statistics.
        logger (logging.Logger): Logger instance for this class.
        log_file_path (str): Path to the log file.

    Methods:
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator.run_orchestrator`: Executes the main data processing workflow.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator.get_final_category_data`: Returns processed category data.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator.get_final_faculty_data`: Returns processed faculty data.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator.get_final_global_faculty_data`: Returns processed global faculty data.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator.get_final_article_stats_data`: Returns processed article statistics.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator.get_final_article_data`: Returns processed article details.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._save_all_results`: Saves all processed data to files.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._refine_faculty_sets`: Refines faculty sets by removing duplicates.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._refine_faculty_stats`: Refines faculty statistics based on name variations.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._clean_category_data`: Prepares category data by removing unwanted keys.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._serialize_and_save_category_data`: Serializes and saves category data.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._serialize_and_save_faculty_stats`: Serializes and saves faculty statistics.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._serialize_and_save_global_faculty_stats`: Serializes and saves global faculty statistics.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._serialize_and_save_category_article_stats`: Serializes and saves article statistics.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._serialize_and_save_articles`: Serializes and saves article details.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._flatten_to_list`: Flattens nested data structures into a list.
        :meth:`~academic_metrics.orchestrators.category_data_orchestrator.CategoryDataOrchestrator._write_to_json`: Writes data to JSON file.
    """

    def __init__(
        self,
        *,
        data: List[Dict],
        output_dir_path: str,
        category_processor: CategoryProcessor,
        faculty_postprocessor: FacultyPostprocessor,
        department_postprocessor: DepartmentPostprocessor,
        strategy_factory: StrategyFactory,
        dataclass_factory: DataClassFactory,
        warning_manager: WarningManager,
        utilities: Utilities,
        extend: bool = False,
    ) -> None:
        """Initialize the CategoryDataOrchestrator with required components and settings.

        Sets up logging configuration with both file and console handlers and
        initializes internal data structures for storing processed results.

        Args:
            data (List[Dict]): Raw classified publication data to process.
            output_dir_path (str): Directory path where output files will be saved.
            category_processor (CategoryProcessor): Processor for handling category-related operations.
            faculty_postprocessor (FacultyPostprocessor): Processor for faculty data refinement.
            strategy_factory (StrategyFactory): Factory for creating processing strategies.
            dataclass_factory (DataClassFactory): Factory for creating data model instances.
            warning_manager (WarningManager): System for handling and logging warnings.
            utilities (Utilities): General utility functions.
            extend (bool, optional): Whether to extend existing data files. Defaults to False.

        Raises:
            ValueError: If output directory path doesn't exist or isn't writable.
            TypeError: If any of the processor or factory arguments are of incorrect type.
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="category_data_orchestrator",
            log_level=DEBUG,
        )

        self.logger.info("Initializing CategoryDataOrchestrator...")
        self.logger.info(f"Data: {data}")
        self.logger.info(f"Output directory path: {output_dir_path}")
        self.logger.info(f"Extend: {extend}")

        self.data: List[Dict] = data
        self.output_dir_path: str = output_dir_path
        self.extend: bool = extend

        self.logger.info("Assigning strategy factory...")
        self.strategy_factory = strategy_factory
        self.logger.info("Strategy factory assigned.")

        self.logger.info("Assigning warning manager...")
        self.warning_manager: WarningManager = warning_manager
        self.logger.info("Warning manager assigned.")

        self.logger.info("Assigning dataclass factory...")
        self.dataclass_factory: DataClassFactory = dataclass_factory
        self.logger.info("Dataclass factory assigned.")

        self.logger.info("Assigning utilities...")
        self.utils: Utilities = utilities
        self.logger.info("Utilities assigned.")

        # Initialize the CategoryProcessor and FacultyDepartmentManager with dependencies
        self.logger.info("Assigning category processor...")
        self.category_processor: CategoryProcessor = category_processor
        self.logger.info("Category processor assigned.")

        # post-processor object
        self.logger.info("Assigning faculty postprocessor...")
        self.faculty_postprocessor: FacultyPostprocessor = faculty_postprocessor
        self.logger.info("Faculty postprocessor assigned.")

        self.logger.info("Assigning department postprocessor...")
        self.department_postprocessor: DepartmentPostprocessor = (
            department_postprocessor
        )
        self.logger.info("Department postprocessor assigned.")

        self.logger.info("Initializing final data structures...")
        self.final_category_data: List[Dict] = []
        self.final_faculty_data: List[Dict] = []
        self.final_article_stats_data: List[Dict] = []
        self.final_article_data: List[Dict] = []
        self.final_global_faculty_data: List[Dict] = []
        self.logger.info("Final data structures initialized.")

    def run_orchestrator(self, category_data: List[Dict] | None = None) -> None:
        """Execute the main data processing workflow.

        Processes the raw publication data through several stages:
        1. Processes data through CategoryProcessor
        2. Gets category data for faculty set refinement
        3. Refines faculty sets to remove duplicates
        4. Refines faculty statistics with name variations
        5. Saves all processed results to files

        Raises:
            ValueError: If category data processing fails.
            IOError: If saving results to files fails.
        """
        self.logger.info("Processing data through category processor...")
        if category_data is None:
            self.category_processor.process_data_list(self.data)

        self.logger.info("Data processed through category processor.")

        # category counts dict to pass to refine faculty sets
        self.logger.info("Getting category data...")
        if category_data is None:
            category_data: dict[str, CategoryInfo] = (
                self.category_processor.get_category_data()
            )
        self.logger.info("Category data retrieved.")

        # Refine faculty sets to remove near duplicates and update counts
        self.logger.info(
            "Refining faculty sets to remove near duplicates and update counts..."
        )
        self._refine_faculty(
            faculty_postprocessor=self.faculty_postprocessor,
            category_dict=category_data,
        )
        self.logger.info("Faculty sets refined.")

        self.logger.info("Refining faculty statistics with name variations...")
        self._refine_faculty_stats(
            faculty_stats=self.category_processor.faculty_stats,
            variations=self.faculty_postprocessor.string_variations,
            category_dict=category_data,
        )
        self.logger.info("Faculty statistics refined.")

        self.logger.info("Processing department sets...")
        self._refine_departments(
            department_postprocessor=self.department_postprocessor,
            category_dict=category_data,
        )
        self.logger.info("Department sets processed.")

        self.logger.info("Saving all processed results to files...")
        self._save_all_results()
        self.logger.info("All processed results saved to files.")

    def get_final_category_data(self) -> List[Dict]:
        """Retrieve the processed category data.

        Returns:
            List[Dict]: List of processed category data dictionaries.

        Raises:
            ValueError: If final category data hasn't been generated yet.
        """
        self.logger.info("Getting final category data...")
        if not hasattr(self, "final_category_data"):
            self.logger.error("Final category data not yet generated")
            raise ValueError("Final category data not yet generated")
        self.logger.info("Final category data retrieved.")
        return self.final_category_data

    def get_final_faculty_data(self) -> List[Dict]:
        """Retrieve the processed faculty data.

        Returns:
            List[Dict]: List of processed faculty data dictionaries.

        Raises:
            ValueError: If final faculty data hasn't been generated yet.
        """
        self.logger.info("Getting final faculty data...")
        if not hasattr(self, "final_faculty_data"):
            self.logger.error("Final faculty data not yet generated")
            raise ValueError("Final faculty data not yet generated")
        self.logger.info("Final faculty data retrieved.")
        return self.final_faculty_data

    def get_final_global_faculty_data(self) -> List[Dict]:
        """Retrieve the processed global faculty data.

        Returns:
            List[Dict]: List of processed global faculty data dictionaries.

        Raises:
            ValueError: If final global faculty data hasn't been generated yet.
        """
        self.logger.info("Getting final global faculty data...")
        if not hasattr(self, "final_global_faculty_data"):
            self.logger.error("Final global faculty data not yet generated")
            raise ValueError("Final global faculty data not yet generated")
        self.logger.info("Final global faculty data retrieved.")
        return self.final_global_faculty_data

    def get_final_article_stats_data(self) -> List[Dict]:
        """Retrieve the processed article statistics data.

        Returns:
            List[Dict]: List of processed article statistics dictionaries.

        Raises:
            ValueError: If final article statistics data hasn't been generated yet.
        """
        self.logger.info("Getting final article stats data...")
        if not hasattr(self, "final_article_stats_data"):
            self.logger.error("Final article stats data not yet generated")
            raise ValueError("Final article stats data not yet generated")
        self.logger.info("Final article stats data retrieved.")
        return self.final_article_stats_data

    def get_final_article_data(self) -> List[Dict]:
        """Retrieve the processed article data.

        Returns:
            List[Dict]: List of processed article data dictionaries.

        Raises:
            ValueError: If final article data hasn't been generated yet.
        """
        self.logger.info("Getting final article data...")
        if not hasattr(self, "final_article_data"):
            self.logger.error("Final article data not yet generated")
            raise ValueError("Final article data not yet generated")
        self.logger.info("Final article data retrieved.")
        return self.final_article_data

    def _save_all_results(self) -> None:
        """Save all processed data to their respective JSON files.

        Serializes and saves:
        1. Category data
        2. Faculty statistics
        3. Article statistics
        4. Article details
        5. Global faculty statistics

        Raises:
            IOError: If any file operations fail.
        """
        self.logger.info("Serializing and saving category data...")

        # Serialize the processed data and save it
        self.logger.info("Serializing and saving category data...")
        self._serialize_and_save_category_data(
            output_path=os.path.join(
                self.output_dir_path, "test_processed_category_data.json"
            ),
            category_data=self.category_processor.get_category_data(),
        )
        self.logger.info("Category data serialized and saved.")

        self.logger.info("Serializing and saving faculty stats...")
        self._serialize_and_save_faculty_stats(
            output_path=os.path.join(
                self.output_dir_path, "test_processed_faculty_stats_data.json"
            ),
            faculty_stats=self.category_processor.get_faculty_stats(),
        )
        self.logger.info("Faculty stats serialized and saved.")

        self.logger.info("Serializing and saving article stats...")
        self._serialize_and_save_category_article_stats(
            output_path=os.path.join(
                self.output_dir_path, "test_processed_article_stats_data.json"
            ),
            article_stats=self.category_processor.get_category_article_stats(),
        )
        self.logger.info("Article stats serialized and saved.")

        self.logger.info("Serializing and saving articles...")
        self._serialize_and_save_articles(
            output_path=os.path.join(
                self.output_dir_path, "test_processed_article_stats_obj_data.json"
            ),
            articles=self.category_processor.get_articles(),
        )
        self.logger.info("Articles serialized and saved.")

        self.logger.info("Serializing and saving global faculty stats...")
        self._serialize_and_save_global_faculty_stats(
            output_path=os.path.join(
                self.output_dir_path, "test_processed_global_faculty_stats_data.json"
            ),
            global_faculty_stats=self.category_processor.get_global_faculty_stats(),
        )
        self.logger.info("Global faculty stats serialized and saved.")

    def _refine_faculty(
        self,
        faculty_postprocessor: FacultyPostprocessor,
        category_dict: dict[str, CategoryInfo],
    ) -> None:
        """Refines faculty sets by removing near duplicates and updating counts.

        Uses FacultyPostprocessor to clean faculty data by removing near-duplicate
        entries and updating all related faculty and department counts.

        Args:
            faculty_postprocessor (FacultyPostprocessor): Postprocessor for faculty data.
                Type: :class:`academic_metrics.postprocessors.faculty_postprocessor.FacultyPostprocessor`
            category_dict (dict): Dictionary of categories and their information.
                Type: Dict[str, :class:`academic_metrics.models.category_info.CategoryInfo`]

        Notes:
            - Removes near-duplicate faculty entries
            - Updates faculty counts per category
            - Updates department counts
            - Maintains faculty-department relationships
            - Ensures data consistency after refinement
        """
        self.logger.info("Refining faculty sets...")
        # Remove near duplicates
        faculty_postprocessor.remove_near_duplicates(category_dict=category_dict)

        # Update counts for each category
        self.logger.info("Updating category counts...")
        for _, info in category_dict.items():
            info.set_params(
                {
                    "faculty_count": len(info.faculty),
                }
            )
        self.logger.info("Faculty sets refined and counts updated.")

    def _refine_faculty_stats(
        self,
        *,
        faculty_stats: Dict[str, FacultyStats],
        variations: Dict[str, StringVariation],
        category_dict: Dict[str, CategoryInfo],
    ) -> None:
        """Refines faculty statistics based on name variations.

        Processes faculty statistics to account for name variations, ensuring accurate
        attribution of publications and metrics across all faculty members.

        Args:
            faculty_stats (Dict): Dictionary of faculty statistics.
                Type: Dict[str, :class:`academic_metrics.models.faculty_stats.FacultyStats`]
            variations (Dict): Dictionary of name variations.
                Type: Dict[str, :class:`academic_metrics.models.string_variation.StringVariation`]
            category_dict (Dict): Dictionary of categories and their information.
                Type: Dict[str, :class:`academic_metrics.models.category_info.CategoryInfo`]

        Notes:
            - Iterates through all categories
            - Processes each faculty member
            - Applies name variation matching
            - Updates publication counts
            - Ensures metric consistency
            - Maintains statistical accuracy
        """
        self.logger.info("Refining faculty statistics with name variations...")
        self.logger.info("Grabbing category_dict keys...")
        categories: List[str] = list(category_dict.keys())
        self.logger.info(f"Grabbed category_dict keys: {categories}")

        self.logger.info("Iterating through categories...")
        for category in categories:
            self.logger.info(f"Processing category: {category}")
            # assigns faculty_stats dict from FacultyStats dataclass to category_faculty_stats
            # category_faculty_stats: Dict[str, FacultyInfo] = faculty_stats[
            #     category
            # ].faculty_stats

            self.logger.info("Grabbing faculty members...")
            faculty_members: List[str] = list(
                faculty_stats[category].faculty_stats.keys()
            )
            self.logger.info(f"Grabbed faculty members: {faculty_members}")

            self.logger.info("Refining faculty stats...")
            for faculty_member in faculty_members:
                faculty_stats[category].refine_faculty_stats(
                    faculty_name_unrefined=faculty_member,
                    variations=variations,
                )
            self.logger.info("Faculty stats refined.")

    def _refine_departments(
        self,
        department_postprocessor: DepartmentPostprocessor,
        category_dict: dict[str, CategoryInfo],
    ) -> None:
        """Processes department sets by removing near duplicates and updates counts.

        Uses DepartmentPostprocessor to clean department data by removing near-duplicate
        entries and updating all related department counts and relationships.

        Args:
            department_postprocessor (DepartmentPostprocessor): Postprocessor for department data.
                Type: :class:`academic_metrics.postprocessors.department_postprocessor.DepartmentPostprocessor`
            category_dict (dict): Dictionary of categories and their information.
                Type: Dict[str, :class:`academic_metrics.models.category_info.CategoryInfo`]

        Notes:
            - Removes near-duplicate department entries
            - Updates department counts per category
            - Maintains faculty-department relationships
            - Ensures naming consistency
            - Preserves hierarchical relationships
            - Updates all related statistics
        """
        self.logger.info("Processing department sets...")
        department_postprocessor.remove_near_duplicates(category_dict=category_dict)

        self.logger.info("Updating department counts...")
        for _, info in category_dict.items():
            info.set_params(
                {
                    "department_count": len(info.departments),
                }
            )
        self.logger.info("Departments refined.")

    def _clean_category_data(
        self, category_data: Dict[str, CategoryInfo]
    ) -> Dict[str, Dict]:
        """Prepare category data by removing unwanted keys.

        Cleans the raw category data by removing specified keys that are not needed
        for further processing or analysis.

        Args:
            category_data (Dict): Raw category data to clean.
                Type: Dict[str, :class:`academic_metrics.models.category_info.CategoryInfo`]

        Returns:
            dict: Cleaned category data with specified keys removed.
                Type: Dict[str, Dict]

        Notes:
            - Identifies and removes unnecessary keys
            - Preserves essential category information
            - Ensures data consistency
            - Prepares data for downstream processing
        """
        self.logger.info("Cleaning category data...")
        # True: Exclude value(s) for key
        # False: Include value(s) for key
        self.logger.info("Defining exclude keys map...")
        exclude_keys_map = {
            "files": True,
            "faculty": False,
            "departments": False,
            "titles": False,
        }
        self.logger.info(f"Exclude keys map: {exclude_keys_map}")

        self.logger.info("Iterating through categories...")
        cleaned_data: Dict[str, Dict] = {
            category: category_info.to_dict(
                # exclude keys is a list of strings matching keys in the dataclass to exclude
                # from the final dict when executing .to_dict()
                # We grab the key out and put that string in the list if it's value is True
                exclude_keys=[
                    key
                    for key, should_exclude in exclude_keys_map.items()
                    if should_exclude
                ]
            )
            for category, category_info in category_data.items()
        }
        self.logger.info("Category data cleaned.")
        return cleaned_data

    def _serialize_and_save_category_data(
        self, *, output_path: str, category_data: Dict[str, Dict]
    ) -> None:
        """Serialize and save category data to JSON file.

        Converts the category data dictionary to JSON format and saves it to the
        specified file path, creating directories if needed.

        Args:
            output_path (str): Path where the JSON file will be saved.
                Type: str
            category_data (Dict): Category data to serialize.
                Type: Dict[str, Dict]

        Raises:
            IOError: If file writing fails or directory creation fails

        Notes:
            - Creates output directory if needed
            - Serializes data to JSON format
            - Handles nested dictionary structures
            - Ensures proper file encoding
            - Validates output before saving
        """
        self.logger.info("Cleaning category data...")
        # Step 1: Clean the data
        cleaned_data: Dict[str, Dict] = self._clean_category_data(category_data)
        self.logger.info("Category data cleaned.")

        # Step 2: Convert to list of category info dicts
        self.logger.info("Converting to list of category info dicts...")
        flattened_data: List[Dict] = list(cleaned_data.values())
        self.logger.info(f"Flattened data: {flattened_data}")

        self.logger.info("Assigning flattened data to final_category_data...")
        self.final_category_data = flattened_data
        self.logger.info("Final category data assigned.")

        self.logger.info("Writing to file...")
        # Step 3: Write to file
        self._write_to_json(flattened_data, output_path)

    def _serialize_and_save_faculty_stats(
        self, *, output_path: str, faculty_stats: Dict[str, FacultyStats]
    ) -> None:
        """Serialize and save faculty statistics to JSON file.

        Converts the faculty statistics dictionary to JSON format and saves it to the
        specified file path, creating directories if needed.

        Args:
            output_path (str): Path where the JSON file will be saved.
                Type: str
            faculty_stats (Dict): Faculty statistics to serialize.
                Type: Dict[str, :class:`academic_metrics.models.faculty_stats.FacultyStats`]

        Raises:
            IOError: If file writing fails or directory creation fails

        Notes:
            - Creates output directory if needed
            - Serializes data to JSON format
            - Handles nested dictionary structures
            - Ensures proper file encoding
            - Validates output before saving
        """
        self.logger.info("Serializing and saving faculty stats...")

        # First .values() gets rid of the category level
        # Second .values() gets rid of the faculty_stats level
        # Third .values() gets rid of the faculty name level
        # We don't lose any data or need to insert it as we do so throughout the processing
        # By not only making them keys but also inserting them into the FacultyInfo objects
        # Define exclude keys map to parse into a list of keys to exclude from final dict
        self.logger.info("Defining exclude keys map...")
        exclude_keys_map = {
            "article_count": False,
            "average_citations": False,
            "doi_citation_map": True,
        }
        self.logger.info(f"Exclude keys map: {exclude_keys_map}")

        self.logger.info("Flattening faculty stats...")
        # List[FacultyInfo.to_dict()]
        flattened_data: List[Dict] = []

        # First level: Categories
        self.logger.info("Iterating through faculty stats...")
        for category_stats in faculty_stats.values():
            self.logger.info(f"Processing category stats: {category_stats}")

            # Second level: faculty_stats dict
            self.logger.info("Converting to dict...")
            faculty_dict = category_stats.to_dict(
                # exclude keys is a list of strings matching keys in the dataclass to exclude
                # from the final dict when executing .to_dict()
                # We grab the key out and put that string in the list if it's value is True
                # True means exclude the value for that key
                exclude_keys=[
                    key
                    for key, should_exclude in exclude_keys_map.items()
                    if should_exclude
                ]
            )
            self.logger.info("Faculty dict converted to dict.")

            self.logger.info("Checking if faculty_stats is in faculty_dict...")
            if "faculty_stats" in faculty_dict:  # Second level: faculty_stats dict
                self.logger.info("Faculty stats found in faculty dict.")

                self.logger.info("Iterating through faculty stats...")
                for faculty_info_obj in faculty_dict["faculty_stats"].values():
                    self.logger.info(f"Processing faculty info obj: {faculty_info_obj}")
                    # Convert FacultyInfo obj to dict and append to flattened_data
                    flattened_data.append(faculty_info_obj)

        self.logger.info("Flattened data appended.")

        self.logger.info("Assigning flattened data to final_faculty_data...")
        self.final_faculty_data = flattened_data
        self.logger.info("Final faculty data assigned.")

        self.logger.info("Writing to file...")
        self._write_to_json(flattened_data, output_path)

    def _serialize_and_save_global_faculty_stats(
        self, *, output_path: str, global_faculty_stats: Dict[str, FacultyStats]
    ) -> None:
        """Serialize and save global faculty statistics to JSON file.

        Converts the global faculty statistics dictionary to JSON format and saves it to the
        specified file path, creating directories if needed.

        Args:
            output_path (str): Path where the JSON file will be saved.
                Type: str
            global_faculty_stats (Dict): Global faculty statistics to serialize.
                Type: Dict[str, :class:`academic_metrics.models.faculty_stats.FacultyStats`]

        Raises:
            IOError: If file writing fails or directory creation fails

        Notes:
            - Creates output directory if needed
            - Serializes data to JSON format
            - Handles nested dictionary structures
            - Ensures proper file encoding
            - Validates output before saving
        """
        self.logger.info("Serializing and saving global faculty stats...")

        # Step 0: Define exclude keys map to parse into a list of keys to exclude from final dict
        self.logger.info("Defining exclude keys map...")
        exclude_keys_map = {
            "article_count": False,
            "average_citations": False,
            "citation_map": True,
        }
        self.logger.info(f"Exclude keys map: {exclude_keys_map}")

        # Step 1: Flatten and convert to list of dicts
        self.logger.info("Flattening and converting to list of dicts...")
        data: List[Dict] = [
            item.to_dict(
                exclude_keys=[
                    key
                    for key, should_exclude in exclude_keys_map.items()
                    if should_exclude
                ]
            )
            for item in global_faculty_stats.values()
        ]
        self.logger.info("Flattened and converted to list of dicts.")

        self.logger.info("Assigning flattened data to final_global_faculty_data...")
        self.final_global_faculty_data = data
        self.logger.info("Final global faculty data assigned.")

        self.logger.info("Writing to file...")
        # Step 2: Write to file
        self._write_to_json(data, output_path)

    def _serialize_and_save_category_article_stats(
        self, *, output_path: str, article_stats: Dict[str, CrossrefArticleStats]
    ) -> None:
        """Serialize and save article statistics to JSON file.

        Converts the category article statistics dictionary to JSON format and saves it to the
        specified file path, creating directories if needed.

        Args:
            output_path (str): Path where the JSON file will be saved.
                Type: str
            article_stats (Dict): Article statistics to serialize.
                Type: Dict[str, :class:`academic_metrics.models.crossref_article_stats.CrossrefArticleStats`]

        Raises:
            IOError: If file writing fails or directory creation fails

        Notes:
            - Creates output directory if needed
            - Serializes data to JSON format
            - Handles nested dictionary structures
            - Ensures proper file encoding
            - Validates output before saving
        """
        self.logger.info("Serializing and saving article stats...")

        self.logger.info("Flattening article stats...")
        flattened_data: List[Dict] = []

        self.logger.info("Iterating through article stats...")
        for category, stats in article_stats.items():
            self.logger.info(f"Processing article stats: {stats}")
            stats_dict = stats.to_dict()

            self.logger.info("Checking if article_citation_map is in stats_dict...")
            if "article_citation_map" in stats_dict:
                self.logger.info("Article citation map found in stats dict.")

                self.logger.info("Iterating through article citation map...")
                for article in stats_dict["article_citation_map"].values():
                    self.logger.info(f"Processing article: {article}")
                    flattened_data.append({**article})
        self.logger.info("Flattened data appended.")

        self.logger.info("Assigning flattened data to final_article_stats_data...")
        self.final_article_stats_data = flattened_data
        self.logger.info("Final article stats data assigned.")

        self.logger.info("Writing to file...")
        self._write_to_json(flattened_data, output_path)

    def _serialize_and_save_articles(
        self, *, output_path: str, articles: List[CrossrefArticleDetails]
    ) -> None:
        """Serialize and save article details to JSON file.

        Converts the list of article details to JSON format and saves it to the
        specified file path, creating directories if needed.

        Args:
            output_path (str): Path where the JSON file will be saved.
                Type: str
            articles (List): Article details to serialize.
                Type: List[:class:`academic_metrics.models.crossref_article_details.CrossrefArticleDetails`]

        Raises:
            IOError: If file writing fails or directory creation fails

        Notes:
            - Creates output directory if needed
            - Serializes data to JSON format
            - Handles complex article objects
            - Ensures proper file encoding
            - Validates output before saving
        """
        self.logger.info("Serializing and saving articles...")

        self.logger.info("Converting to list of dicts...")
        # Convert each CrossrefArticleDetails object to a dict
        article_dicts: List[Dict] = [article.to_dict() for article in articles]
        self.logger.info("Converted to list of dicts.")

        self.logger.info("Assigning to final_article_data...")
        self.final_article_data = article_dicts
        self.logger.info("Final article data assigned.")

        self.logger.info("Writing to file...")
        # Write to file
        self._write_to_json(article_dicts, output_path)

    def _flatten_to_list(self, data: Union[Dict, List]) -> List[Dict]:
        """Recursively flatten nested dictionaries/lists into a flat list.

        Transforms a complex nested structure of dictionaries and lists into a single
        flat list of dictionaries, preserving all data.

        Args:
            data (Union[Dict, List]): Nested structure of dictionaries and lists.
                Type: Union[Dict[str, Any], List[Dict[str, Any]]]

        Returns:
            List: Flattened list of dictionaries.
                Type: List[Dict[str, Any]]

        Notes:
            - Handles arbitrary nesting depth
            - Preserves dictionary contents
            - Maintains data relationships
            - Removes nested structure
            - Keeps all original values

        Examples:
            Input:
                {
                    "cat1": {
                        "article_map": {
                            "doi1": {"title": "Article 1"},
                            "doi2": {"title": "Article 2"}
                        }
                    }
                }
            Output:
                [
                    {"title": "Article 1"},
                    {"title": "Article 2"}
                ]
        """
        self.logger.info("Flattening...")
        flattened: List[Dict] = []

        self.logger.info("Checking if data is a dict...")
        if isinstance(data, dict):
            self.logger.info("Data is a dict. Iterating through values...")
            for value in data.values():
                flattened.extend(self._flatten_to_list(value))
        elif isinstance(data, list):
            self.logger.info("Data is a list. Iterating through items...")
            for item in data:
                flattened.extend(self._flatten_to_list(item))
        else:  # Base case: found a non-dict, non-list value
            self.logger.info("Data is a non-dict, non-list value. Appending...")
            if isinstance(data, dict):  # If it's a dictionary, add it
                flattened.append(data)

        self.logger.info("Flattened.")
        return flattened

    def _write_to_json(self, data: Union[List[Dict], Dict], output_path: str) -> None:
        """Write data to JSON file, handling extend mode.

        Writes the provided data to a JSON file at the specified path, creating
        directories if needed and handling both new files and file extensions.

        Args:
            data (Union[List[Dict], Dict]): Data to write to file.
                Type: Union[List[Dict[str, Any]], Dict[str, Any]]
            output_path (str): Path where the JSON file will be saved.
                Type: str

        Raises:
            IOError: If file operations fail (creation, writing, or directory access)

        Notes:
            - Creates output directory if needed
            - Handles both new and existing files
            - Supports list and dictionary data
            - Ensures proper JSON formatting
            - Validates file permissions
            - Maintains data integrity
        """
        self.logger.info("Checking if extending...")
        if self.extend:
            self.logger.info("Extending...")
            with open(output_path, "r") as json_file:
                existing_data: Union[List[Dict], Dict] = json.load(json_file)
            self.logger.info("Existing data loaded.")

            self.logger.info("Checking if data is a list...")
            if isinstance(data, list):
                self.logger.info("Data is a list. Extending...")
                existing_data.extend(data)
            else:
                self.logger.info("Data is not a list. Updating...")
                existing_data.update(data)
            self.logger.info("Data updated.")
            data: Union[List[Dict], Dict] = existing_data

        self.logger.info("Dumping data to file...")
        with open(output_path, "w") as json_file:
            json.dump(data, json_file, indent=4)


if __name__ == "__main__":
    # import tempfile

    # # Create a test CategoryInfo object
    # test_category = CategoryInfo(
    #     _id="Psychology",
    #     url="Psychology",
    #     category_name="Psychology",
    #     faculty_count=6,
    #     department_count=4,
    #     article_count=3,
    #     faculty={"Sook-Hyun Kim", "Jose I. Juncosa", "Suzanne L. Osman"},
    #     departments={
    #         "Department of Psychology, Salisbury University, Salisbury, MD, USA"
    #     },
    #     titles={"Korean fathers' immigration experience"},
    #     tc_count=0,
    #     citation_average=0.0,
    #     doi_list={"10.1177/10778012241234897"},
    #     themes={"Parenting Challenges", "Cultural Identity"},
    # )

    # # Create the input dictionary format
    # category_data = {"Psychology": test_category}

    # # Create a minimal orchestrator instance
    # with tempfile.TemporaryDirectory() as temp_dir:
    #     orchestrator = CategoryDataOrchestrator(
    #         data=[],
    #         output_dir_path=temp_dir,
    #         category_processor=None,
    #         faculty_postprocessor=None,
    #         strategy_factory=None,
    #         dataclass_factory=None,
    #         warning_manager=None,
    #         utilities=None,
    #     )

    #     # Test the serialization
    #     output_path = os.path.join(temp_dir, "test_categories.json")
    #     print("\nTesting category serialization:")
    #     print(f"Input category data: {category_data}")

    #     orchestrator._serialize_and_save_category_data(
    #         output_path=output_path, category_data=category_data
    #     )

    #     print("\nAfter serialization:")
    #     print(
    #         f"Has final_category_data: {hasattr(orchestrator, 'final_category_data')}"
    #     )
    #     if hasattr(orchestrator, "final_category_data"):
    #         print(
    #             f"Length of final_category_data: {len(orchestrator.final_category_data)}"
    #         )
    #         print(f"Content: {orchestrator.final_category_data}")

    #     # Test retrieval
    #     try:
    #         retrieved_data = orchestrator.get_final_category_data()
    #         print("\nSuccessfully retrieved data:")
    #         print(f"Length: {len(retrieved_data)}")
    #         print(f"Content: {retrieved_data}")

    #         # Also check the JSON file
    #         print("\nJSON file content:")
    #         with open(output_path, "r") as f:
    #             json_content = json.load(f)
    #             print(f"JSON length: {len(json_content)}")
    #             print(f"JSON content: {json_content}")

    #     except ValueError as e:
    #         print(f"\nError retrieving data: {e}")
    from academic_metrics.utils import (
        MinHashUtility,
        Utilities,
        WarningManager,
        Taxonomy,
    )

    from academic_metrics.core import CategoryProcessor
    from academic_metrics.postprocessing import (
        DepartmentPostprocessor,
        FacultyPostprocessor,
    )
    from academic_metrics.factories import StrategyFactory, DataClassFactory

    from academic_metrics.constants import (
        INPUT_FILES_DIR_PATH,
        LOG_DIR_PATH,
        OUTPUT_FILES_DIR_PATH,
        SPLIT_FILES_DIR_PATH,
    )

    taxonomy = Taxonomy()
    minhash_util = MinHashUtility(num_hashes=100)
    warning_manager = WarningManager()
    strategy_factory = StrategyFactory()
    dataclass_factory = DataClassFactory()
    utilities = Utilities(
        strategy_factory=strategy_factory,
        warning_manager=warning_manager,
    )
    category_processor = CategoryProcessor(
        utils=utilities,
        dataclass_factory=dataclass_factory,
        warning_manager=warning_manager,
        taxonomy_util=taxonomy,
    )
    faculty_postprocessor = FacultyPostprocessor(minhash_util=minhash_util)
    department_postprocessor = DepartmentPostprocessor(minhash_util=minhash_util)

    with open("test_processed_category_data.json", "r") as f:
        data = json.load(f)

    orchestrator = CategoryDataOrchestrator(
        data=data,
        output_dir_path=OUTPUT_FILES_DIR_PATH,
        category_processor=category_processor,
        faculty_postprocessor=faculty_postprocessor,
        department_postprocessor=department_postprocessor,
        strategy_factory=strategy_factory,
        dataclass_factory=dataclass_factory,
        warning_manager=warning_manager,
        utilities=utilities,
    )

    orchestrator.run_orchestrator(data)

            ```

            src/academic_metrics/orchestrators/classification_orchestrator.py:
            ```
from __future__ import annotations

import logging
import os
from typing import TYPE_CHECKING, Callable, Dict, List, Tuple, TypeAlias

from pylatexenc.latex2text import LatexNodes2Text
from unidecode import unidecode

from academic_metrics.AI import AbstractClassifier
from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)
from academic_metrics.enums import AttributeTypes

if TYPE_CHECKING:
    from academic_metrics.utils import Utilities

ClassificationResultsDict: TypeAlias = Dict[str, List[str]]
"""Type alias for a dictionary mapping DOIs to lists of classification results.

This type alias is used to represent the return type of the 
:meth:`~academic_metrics.AI.AbstractClassifier.get_classification_results_by_doi` method.
"""

ClassificationResultsTuple: TypeAlias = Tuple[
    List[str], List[str], List[str], List[str]
]
"""Type alias for a tuple containing lists of classification results.

This type alias is used to represent the return type of the 
:meth:`~academic_metrics.AI.AbstractClassifier.get_classification_results_by_doi` method.

Notes:
    - Format of the tuple is (top_categories, mid_categories, low_categories, themes)
"""


class ClassificationOrchestrator:
    """Manages the classification process for research abstracts.

    Orchestrates the process of extracting DOIs and abstracts from research metadata,
    classifying them using AbstractClassifier, and integrating results back into
    the original data. Tracks unclassified items for monitoring.

    Attributes:
        abstract_classifier_factory (Callable[..., AbstractClassifier]): Factory function for AbstractClassifier instances.
        taxonomy (Taxonomy): Classification hierarchy for AbstractClassifier.
        utilities (Utilities): Utilities for attribute extraction.
        ai_api_key (str): API key for AI service access.
        unclassified_item_count (int): Count of unclassified items.
            Type: int
        unclassified_dois (List): DOIs of unclassified items.
            Type: List[str]
        unclassified_abstracts (List): Abstracts of unclassified items.
            Type: List[str]
        unclassified_doi_abstract_dict (Dict): Maps unclassified DOIs to abstracts.
            Type: Dict[str, str]
        unclassified_items (List): Complete metadata of unclassified items.
            Type: List[Dict[str, Any]]
        unclassified_details (Dict): Organized unclassified data.
            Type: Dict[str, Union[List[str], List[Dict[str, Any]]]]
            Contains:
            - dois: List of unclassified DOIs
            - abstracts: List of unclassified abstracts
            - items: List of unclassified metadata items

    Methods:
        run_classification() -> List[Dict]: Processes and classifies a list of research metadata dictionaries.
        get_unclassified_item_count() -> int: Returns the number of unclassified items.
        get_unclassified_dois() -> List[str]: Returns the DOIs of unclassified items.
        get_unclassified_abstracts() -> List[str]: Returns the abstracts of unclassified items.
        get_unclassified_doi_abstract_dict() -> Dict[str, str]: Returns the DOI to abstract mapping dictionary for unclassified items.
        get_unclassified_items() -> List[Dict]: Returns the unclassified items.
        get_unclassified_details_dict() -> Dict: Returns the details of unclassified items.
        _classification_orchestrator() -> List[Dict]: Core classification logic for processing research metadata.
        _inject_categories() -> None: Adds classification results to a research metadata dictionary.
        _extract_categories() -> ClassificationResultsDict | ClassificationResultsTuple: Gets classification results for a specific DOI.
        _make_doi_abstract_dict() -> Dict[str, str]: Creates a DOI to abstract mapping dictionary.
        _retrieve_doi_abstract() -> Tuple[str, str]: Extracts DOI and abstract from a research metadata dictionary.
        _update_classified_instance_variables() -> None: Updates tracking variables for unclassified items.
        _set_classification_ran_true() -> None: Sets the classification ran flag to true.
        _has_ran_classification() -> bool: Checks if classification has been run.
        _validate_classification_ran() -> None: Validates if classification has been run.
        _normalize_abstract() -> str: Normalizes an abstract by removing LaTeX and converting any resulting unicode to ASCII.
    """

    def __init__(
        self,
        abstract_classifier_factory: Callable[[Dict[str, str]], AbstractClassifier],
        utilities: Utilities,
    ):
        """Initialize the ClassificationOrchestrator.

        Sets up the orchestrator with required dependencies for classifying research
        abstracts and managing the classification process.

        Args:
            abstract_classifier_factory (Callable): Factory function for AbstractClassifier.
                Type: Callable[[Dict[str, str]], :class:`~academic_metrics.AI.abstract_classifier.AbstractClassifier`]
            utilities (Utilities): Utilities instance for attribute extraction.
                Type: :class:`~academic_metrics.utils.utilities.Utilities`

        Returns:
            None

        Notes:
            - Initializes tracking variables for unclassified items
            - Sets up classification status flags
            - Prepares data structures for results
            - Validates factory function compatibility
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="classification_orchestrator",
            log_level=DEBUG,
        )

        self.abstract_classifier_factory = abstract_classifier_factory
        self.utilities = utilities

        # flag to check if classification has been run to provide a method for which to prevent retrieval
        # of unclassified attributes before classification has been ran
        self._classification_ran: bool = False

        self.unclassified_item_count: int = 0
        self.unclassified_dois: List[str] = []
        self.unclassified_abstracts: List[str] = []
        self.unclassified_doi_abstract_dict: Dict[str, str] = {}
        self.unclassified_items: List[Dict] = []
        self.unclassified_details_dict: Dict = {
            "dois": [],
            "abstracts": [],
            "items": [],
        }

    def run_classification(
        self,
        data: List[Dict],
        pre_classification_model: str | None = "gpt-4o-mini",
        classification_model: str | None = "gpt-4o-mini",
        theme_model: str | None = "gpt-4o-mini",
    ) -> List[Dict]:
        """Processes and classifies a list of research metadata dictionaries.

        Extracts abstracts from research metadata, classifies them using specified
        AI models, and injects the classification results back into the original data.

        Args:
            data (list): List of dictionaries containing research metadata.
                Type: List[Dict[str, Any]]
            pre_classification_model (str | None): Model for pre-classification processing.
                Type: str | None
                Defaults to "gpt-4o-mini"
            classification_model (str | None): Model for main classification.
                Type: str | None
                Defaults to "gpt-4o-mini"
            theme_model (str | None): Model for theme extraction.
                Type: str | None
                Defaults to "gpt-4o-mini"

        Returns:
            List: Modified data with classifications injected.
                Type: List[Dict[str, Any]]
                Includes:
                - Original metadata
                - Classification results
                - Theme information
                - Processing status

        Notes:
            - Processes each item sequentially
            - Tracks unclassified items
            - Handles missing abstracts
            - Updates internal statistics
            - Maintains original data structure
        """
        classified_data = self._classification_orchestrator(
            data,
            pre_classification_model=pre_classification_model,
            classification_model=classification_model,
            theme_model=theme_model,
        )
        self._set_classification_ran_true()
        return classified_data

    def get_unclassified_item_count(self) -> int:
        """Gets the number of unclassified items.

        Retrieves the count of items that could not be classified during the
        classification process.

        Returns:
            int: Number of unclassified items.
                Type: int

        Raises:
            RuntimeError: If classification has not been run yet

        Notes:
            - Validates classification status
            - Returns current count
            - Includes all unclassified types
            - Requires prior classification run
        """
        self._validate_classification_ran(self._has_ran_classification())
        return self.unclassified_item_count

    def get_unclassified_dois(self) -> List[str]:
        """Gets the DOIs of unclassified items.

        Retrieves the list of Digital Object Identifiers (DOIs) for items that
        could not be classified during the classification process.

        Returns:
            List: List of unclassified DOIs.
                Type: List[str]
                Empty list if all items were classified.

        Raises:
            RuntimeError: If classification has not been run yet

        Notes:
            - Validates classification status
            - Returns unique DOIs only
            - Maintains original DOI format
            - Requires prior classification run
        """
        self._validate_classification_ran(self._has_ran_classification())
        return self.unclassified_dois

    def get_unclassified_abstracts(self) -> List[str]:
        """Gets the abstracts of unclassified items.

        Retrieves the list of research abstracts for items that could not be
        classified during the classification process.

        Returns:
            List: List of unclassified abstracts.
                Type: List[str]
                Empty list if all items were classified.

        Raises:
            RuntimeError: If classification has not been run yet

        Notes:
            - Validates classification status
            - Returns normalized abstracts
            - Maintains text formatting
            - Requires prior classification run
            - May include empty abstracts
        """
        self._validate_classification_ran(self._has_ran_classification())
        return self.unclassified_abstracts

    def get_unclassified_doi_abstract_dict(self) -> Dict[str, str]:
        """Gets the DOI to abstract mapping dictionary for unclassified items.

        Retrieves a dictionary that maps Digital Object Identifiers (DOIs) to their
        corresponding abstracts for items that could not be classified.

        Returns:
            Dict: Dictionary mapping unclassified DOIs to abstracts.
                Type: Dict[str, str]
                Keys: DOIs (str)
                Values: Abstracts (str)
                Empty dict if all items were classified.

        Raises:
            RuntimeError: If classification has not been run yet

        Notes:
            - Validates classification status
            - Maintains DOI-abstract relationships
            - Contains normalized abstracts
            - Requires prior classification run
            - Preserves original DOI format
        """
        self._validate_classification_ran(self._has_ran_classification())
        return self.unclassified_doi_abstract_dict

    def get_unclassified_items(self) -> List[Dict]:
        """Gets the unclassified items.

        Retrieves the complete list of research items that could not be classified,
        including all their original metadata.

        Returns:
            List: List of unclassified items with full metadata.
                Type: List[Dict[str, Any]]
                Empty list if all items were classified.
                Each dict contains complete item metadata.

        Raises:
            RuntimeError: If classification has not been run yet

        Notes:
            - Validates classification status
            - Returns complete metadata
            - Preserves original structure
            - Requires prior classification run
            - Maintains all item attributes
        """
        self._validate_classification_ran(self._has_ran_classification())
        return self.unclassified_items

    def get_unclassified_details_dict(self) -> Dict:
        """Gets the details of unclassified items.

        Retrieves a comprehensive dictionary containing organized information about
        all unclassified items, including DOIs, abstracts, and complete metadata.

        Returns:
            Dict: Organized details of unclassified items.
                Type: Dict[str, Union[List[str], List[Dict[str, Any]]]]
                Contains:
                - dois: List[str] - Unclassified DOIs
                - abstracts: List[str] - Unclassified abstracts
                - items: List[Dict] - Complete metadata

        Raises:
            RuntimeError: If classification has not been run yet

        Notes:
            - Validates classification status
            - Provides structured access
            - Groups related information
            - Requires prior classification run
            - Maintains data relationships
        """
        self._validate_classification_ran(self._has_ran_classification())
        return self.unclassified_details_dict

    def _classification_orchestrator(
        self,
        data: List[Dict],
        pre_classification_model: str | None = "gpt-4o-mini",
        classification_model: str | None = "gpt-4o-mini",
        theme_model: str | None = "gpt-4o-mini",
    ) -> List[Dict]:
        """Core classification logic for processing research metadata.

        Implements the main classification workflow, processing research metadata
        through multiple stages of classification and theme extraction.

        Args:
            data (List): List of dictionaries containing research metadata.
                Type: List[Dict[str, Any]]
            pre_classification_model (str | None): Model for pre-classification processing.
                Type: str | None
                Defaults to "gpt-4o-mini"
            classification_model (str | None): Model for main classification.
                Type: str | None
                Defaults to "gpt-4o-mini"
            theme_model (str | None): Model for theme extraction.
                Type: str | None
                Defaults to "gpt-4o-mini"

        Returns:
            List: Modified data with classifications and themes injected.
                Type: List[Dict[str, Any]]
                Includes:
                - Original metadata
                - Classification results
                - Theme information
                - Processing status

        Notes:
            - Processes items sequentially
            - Handles classification failures
            - Tracks unclassified items
            - Updates internal statistics
            - Maintains data integrity
            - Manages model selection
        """
        i = 0

        # This must be a `while` loop because the list `data` is modified during iteration.
        # Specifically, items may be removed (via `pop`) when an error occurs. A `for` loop
        # uses an iterator tied to the list's initial state and does not account for changes
        # to the list structure. Modifying the list while using a `for` loop would cause
        # skipped items, incorrect indexing, or runtime errors.
        #
        # With a `while` loop, we have full control over the index (`i`). If an item is
        # removed, the remaining items shift, and the loop naturally rechecks the current
        # index without skipping. This ensures every item is processed exactly once, and
        # the logic remains robust despite the dynamic list modifications.
        #
        # Do not attempt to use a `for` loop hereâ€”it will not handle these modifications safely.
        while i < len(data):
            item = data[i]

            try:
                doi, abstract, extra_context = self._get_classification_dependencies(
                    item
                )
                normalized_abstract: str = self._normalize_abstract(abstract)
                doi_abstract_dict: Dict[str, str] = self._make_doi_abstract_dict(
                    doi, normalized_abstract
                )
                if not doi_abstract_dict:
                    self._update_classified_instance_variables(
                        item=item, doi=doi, abstract=abstract
                    )
                    i += 1
                    continue

                classifier: AbstractClassifier = self.abstract_classifier_factory(
                    doi_abstract_dict=doi_abstract_dict,
                    extra_context=extra_context,
                    pre_classification_model=pre_classification_model,
                    classification_model=classification_model,
                    theme_model=theme_model,
                )

                classifier.classify()

                self._inject_categories(
                    data=item, categories=self._extract_categories(doi, classifier)
                )

                i += 1

            except Exception as e:
                self.logger.error(f"Error processing item {i}: {e}")
                self.logger.error(f"Popping the item at index {i} from data")
                self.logger.error(f"Full error traceback:", exc_info=True)
                self.logger.error(f"Problem item: {item}")
                data.pop(i)

        return data

    def _inject_categories(
        self,
        data: Dict,
        categories: ClassificationResultsDict | ClassificationResultsTuple,
    ) -> None:
        """Adds classification results to a research metadata dictionary.

        Injects classification categories and themes into the provided metadata
        dictionary, handling both dictionary and tuple result formats.

        Args:
            data (Dict): Research metadata dictionary.
                Type: Dict[str, Any]
            categories (Union): Classification results including categories and themes.
                Type: Union[ClassificationResultsDict, ClassificationResultsTuple]
                Where:
                - ClassificationResultsDict: Dict[str, List[str]]
                    Format: {
                        "top_categories": List[str],
                        "mid_categories": List[str],
                        "low_categories": List[str],
                        "themes": List[str]
                    }
                - ClassificationResultsTuple: Tuple[List[str], List[str], List[str], List[str]]
                    Format: (
                        top_level_categories: List[str],
                        mid_level_categories: List[str],
                        low_level_categories: List[str],
                        themes: List[str]
                    )

        Raises:
            ValueError: If categories is neither a dict nor a tuple

        Notes:
            - Modifies input dictionary in-place
            - Handles both result formats
            - Preserves existing metadata
            - Validates category structure
            - Maintains hierarchical relationships
            - Provides default empty lists for missing dictionary keys
        """
        # Check if it's a dictionary (Dict[str, List[str]])
        # Format: {
        #     "top_categories": List[str],
        #     "mid_categories": List[str],
        #     "low_categories": List[str],
        #     "themes": List[str]
        # }
        if isinstance(categories, dict):
            data["categories"] = {}
            data["categories"]["top"] = categories.get("top_categories", [])
            data["categories"]["mid"] = categories.get("mid_categories", [])
            data["categories"]["low"] = categories.get("low_categories", [])
            data["themes"] = categories.get("themes", [])
        # Otherwise it must be a tuple (Tuple[List[str], List[str], List[str], List[str]])
        # Format: (
        #     top_level_categories: List[str],
        #     mid_level_categories: List[str],
        #     low_level_categories: List[str],
        #     themes: List[str]
        # )
        elif isinstance(categories, tuple):
            data["categories"] = {}
            data["categories"]["top"] = categories[0]
            data["categories"]["mid"] = categories[1]
            data["categories"]["low"] = categories[2]
            data["themes"] = categories[3]
        else:
            raise ValueError("Invalid categories format")

    def _extract_categories(
        self, doi: str, classifier: AbstractClassifier
    ) -> ClassificationResultsDict | ClassificationResultsTuple:
        """Gets classification results for a specific DOI.

        Retrieves the classification categories and themes for a given DOI using
        the provided classifier instance. Supports both dictionary and tuple result formats.

        Args:
            doi (str): DOI identifier for the research item.
                Type: str
            classifier (AbstractClassifier): Classifier instance that performed classification.
                Type: :class:`~academic_metrics.AI.abstract_classifier.AbstractClassifier`

        Returns:
            Union: Classification results including categories and themes.
                Type: Union[:data:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationResultsDict`,
                          :data:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationResultsTuple`]
                Where:
                - :data:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationResultsDict`:
                    Format: {
                        "top_categories": List[str],
                        "mid_categories": List[str],
                        "low_categories": List[str],
                        "themes": List[str]
                    }
                - :data:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationResultsTuple`:
                    Format: (
                        top_level_categories: List[str],
                        mid_level_categories: List[str],
                        low_level_categories: List[str],
                        themes: List[str]
                    )

        Notes:
            - Utilizes classifier to obtain results
            - Supports multiple result formats
            - Ensures DOI is valid and classified
            - Handles missing classification gracefully
        """
        # .get_classification_results_by_doi() has a argument of return_type that can be set to either a dictionary or a tuple
        # By default it returns a dict, it you want a tuple you can do return_type=tuple
        return classifier.get_classification_results_by_doi(doi)

    def _make_doi_abstract_dict(self, doi: str, abstract: str) -> Dict[str, str]:
        """Creates a DOI to abstract mapping dictionary.

        Constructs a dictionary that maps a given DOI to its corresponding abstract,
        ensuring both values are provided.

        Args:
            doi (str): DOI identifier for the research item.
                Type: str
            abstract (str): Research abstract text.
                Type: str

        Returns:
            dict: Dictionary mapping DOI to abstract.
                Type: Dict[str, str]
                Format: {doi: abstract}

        Raises:
            ValueError: If either DOI or abstract is missing

        Notes:
            - Ensures both DOI and abstract are non-empty
            - Provides a simple mapping structure
            - Validates input before mapping
        """
        if not doi or not abstract:
            raise ValueError("Both DOI and abstract must be provided and non-empty.")
        return {doi: abstract}

    def _get_classification_dependencies(self, item: Dict) -> Tuple[str, str, dict]:
        """Extracts DOI, abstract, and extra context from a research metadata dictionary.

        Uses the utilities module to safely extract required attributes from the
        research metadata, handling missing or invalid values.

        Args:
            item (dict): Research metadata dictionary.
                Type: Dict[str, Any]

        Returns:
            tuple: DOI, abstract, and extra context.
                Type: Tuple[str, str, dict]
                Format: (
                    doi: str | None,
                    abstract: str | None,
                    extra_context: dict | None
                )

        Notes:
            - Uses :class:`~academic_metrics.utils.utilities.Utilities` for extraction
            - Extracts attributes:
                - :data:`~academic_metrics.enums.enums.AttributeTypes.CROSSREF_DOI`
                - :data:`~academic_metrics.enums.enums.AttributeTypes.CROSSREF_ABSTRACT`
                - :data:`~academic_metrics.enums.enums.AttributeTypes.CROSSREF_EXTRA_CONTEXT`
            - Returns None for any missing attributes
            - Preserves original attribute values
            - Handles missing or malformed data gracefully
        """
        result: Dict[AttributeTypes, Tuple[bool, str]] = self.utilities.get_attributes(
            item,
            [
                AttributeTypes.CROSSREF_DOI,
                AttributeTypes.CROSSREF_ABSTRACT,
                AttributeTypes.CROSSREF_EXTRA_CONTEXT,
            ],
        )
        doi: str = (
            result[AttributeTypes.CROSSREF_DOI][1]
            if result[AttributeTypes.CROSSREF_DOI][0]
            else None
        )
        abstract: str = (
            result[AttributeTypes.CROSSREF_ABSTRACT][1]
            if result[AttributeTypes.CROSSREF_ABSTRACT][0]
            else None
        )
        extra_context: dict = (
            result[AttributeTypes.CROSSREF_EXTRA_CONTEXT][1]
            if result[AttributeTypes.CROSSREF_EXTRA_CONTEXT][0]
            else None
        )
        return doi, abstract, extra_context

    def _update_classified_instance_variables(
        self, item: Dict, doi: str, abstract: str
    ) -> None:
        """Updates tracking variables for unclassified items.

        Maintains multiple tracking collections for items that couldn't be classified,
        ensuring consistent record-keeping across different data structures.

        Args:
            item (dict): Research metadata dictionary.
                Type: Dict[str, Any]
            doi (str): DOI identifier.
                Type: str | None
            abstract (str): Research abstract text.
                Type: str | None

        Returns:
            None

        Notes:
            - Updates instance variables:
                - :attr:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationOrchestrator.unclassified_item_count`
                - :attr:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationOrchestrator.unclassified_dois`
                - :attr:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationOrchestrator.unclassified_abstracts`
                - :attr:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationOrchestrator.unclassified_doi_abstract_dict`
                - :attr:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationOrchestrator.unclassified_items`
                - :attr:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationOrchestrator.unclassified_details_dict`
            - Handles missing values by using "NULL" placeholder
            - Maintains parallel data structures for different access patterns
            - Preserves original metadata in unclassified items list
            - Increments unclassified item counter
        """
        self.unclassified_item_count += 1
        (
            self.unclassified_dois.append(doi)
            if doi
            else self.unclassified_dois.append("NULL")
        )
        (
            self.unclassified_abstracts.append(abstract)
            if abstract
            else self.unclassified_abstracts.append("NULL")
        )
        self.unclassified_doi_abstract_dict[doi] = abstract
        self.unclassified_items.append(item)
        (
            self.unclassified_details_dict["dois"].append(doi)
            if doi
            else self.unclassified_details_dict["dois"].append("NULL")
        )
        (
            self.unclassified_details_dict["abstracts"].append(abstract)
            if abstract
            else self.unclassified_details_dict["abstracts"].append("NULL")
        )
        self.unclassified_details_dict["items"].append(item)

    def _set_classification_ran_true(self) -> None:
        """Sets the classification ran flag to true.

        Updates the internal state to indicate that classification process
        has been executed.

        Args:
            None

        Returns:
            None

        Notes:
            - Updates :attr:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationOrchestrator._classification_ran`
            - Used for validation checks
            - State cannot be reset to false
        """
        self._classification_ran: bool = True

    def _has_ran_classification(self) -> bool:
        """Checks if classification has been run.

        Returns
        """
        return self._classification_ran

    def _validate_classification_ran(self, classification_ran: bool) -> None:
        """Checks if classification has been run.

        Verifies whether the classification process has been executed by
        checking the internal state flag.

        Args:
            None

        Returns:
            bool: True if classification has been run, False otherwise.
                Type: bool

        Notes:
            - Reads :attr:`~academic_metrics.orchestrators.classification_orchestrator.ClassificationOrchestrator._classification_ran`
            - Used for validation before accessing results
            - Cannot detect if classification is currently running
        """
        if not classification_ran:
            raise RuntimeError(
                "Classification has not been run yet. "
                "Call run_classification() on your data before attempting to retrieve unclassified attributes. "
                "Data should be a list of loaded crossref JSON objects."
            )

    def _normalize_abstract(self, abstract: str) -> str:
        """Normalizes an abstract by removing LaTeX and converting any resulting unicode to ASCII.

        Processes research abstract text through two stages:
        1. Converts LaTeX notation to unicode text
        2. Converts unicode characters to ASCII equivalents

        Args:
            abstract (str): Research abstract text.
                Type: str
                May contain LaTeX math notation and unicode characters.

        Returns:
            str: Normalized abstract text.
                Type: str
                Contains only ASCII characters.

        Notes:
            - Uses :class:`~pylatexenc.latex2text.LatexNodes2Text` for LaTeX conversion
            - Uses :pypi:`Unidecode` for unicode to ASCII conversion
            - Handles mathematical notation
            - Preserves text structure
            - Removes special characters
            - Math mode set to "text" for consistent conversion
        """
        converter: LatexNodes2Text = LatexNodes2Text(math_mode="text")
        unicode_abstract: str = converter.latex_to_text(abstract)
        ascii_abstract: str = unidecode(unicode_abstract)
        return ascii_abstract

            ```

            src/academic_metrics/other/__init__.py:
            ```
from .in_memory_taxonomy import TAXONOMY_AS_STRING

            ```

            src/academic_metrics/other/in_memory_taxonomy.py:
            ```
TAXONOMY_AS_STRING: str = """
{
    "Agricultural sciences and natural resources": {
        "Agricultural, animal, plant, and veterinary sciences": [
            "Agronomy and crop science",
            "Animal sciences",
            "Food science and technology",
            "Plant sciences",
            "Soil sciences",
            "Veterinary biomedical and clinical sciences",
            "Agricultural, animal, plant, and veterinary sciences nec"
        ],
        "Natural resources and conservation": [
            "Environmental science",
            "Environmental natural resources management and policy",
            "Forestry",
            "Natural resources conservation and research",
            "Natural resources and conservation nec"
        ]
    },
    "Biological and biomedical sciences": {
        "Biochemistry, biophysics, and molecular biology": [
            "Biochemistry",
            "Biochemistry and molecular biology",
            "Biophysics",
            "Molecular biology",
            "Biochemistry, biophysics, and molecular biology nec"
        ],
        "Bioinformatics, biostatistics, and computational biology": [
            "Bioinformatics",
            "Biostatistics",
            "Computational biology",
            "Bioinformatics, biostatistics, and computational biology nec"
        ],
        "Cell cellular biology and anatomy": [
            "Cell cellular and molecular biology",
            "Developmental biology and embryology",
            "Cell cellular biology and anatomy nec"
        ],
        "Ecology, evolutionary biology, and epidemiology": [
            "Ecology",
            "Ecology and evolutionary biology",
            "Epidemiology",
            "Epidemiology and biostatistics",
            "Evolutionary biology",
            "Ecology, evolutionary biology, and epidemiology nec"
        ],
        "Genetics and genomics": [
            "Genetics, general",
            "Genome sciences and genomics",
            "Human medical genetics",
            "Molecular genetics",
            "Genetics nec"
        ],
        "Microbiology and immunology": [
            "Immunology",
            "Microbiology and immunology",
            "Microbiology, general",
            "Virology",
            "Microbiology and immunology nec"
        ],
        "Neurobiology and neurosciences": [
            "Cognitive neuroscience",
            "Neurobiology and anatomy",
            "Neuroscience",
            "Neurobiology and neurosciences nec"
        ],
        "Pharmacology and toxicology": [
            "Pharmacology",
            "Toxicology",
            "Pharmacology and toxicology nec"
        ],
        "Physiology, oncology and cancer biology": [
            "Biomechanics",
            "Exercise physiology and kinesiology",
            "Oncology and cancer biology",
            "Physiology, general",
            "Physiology, oncology and cancer biology nec"
        ],
        "Biological and biomedical sciences, other": [
            "Biology biological sciences, general",
            "Biomedical sciences, general",
            "Botany and plant biology",
            "Entomology",
            "Plant pathology and phytopathology",
            "Plant physiology and biology nec",
            "Zoology and animal biology",
            "Biological and biomedical sciences nec"
        ]
    },
    "Computer and information sciences": {
        "Computer science": [
            "Computer science"
        ],
        "Computer and information sciences, other": [
            "Artificial intelligence",
            "Computer and information sciences, general",
            "Computer systems networking and telecommunications",
            "Informatics and information technology ",
            "Information science studies",
            "Computer and information sciences nec"
        ]
    },
    "Engineering": {
        "Biological, biomedical, and biosystems engineering": [
            "Bioengineering and biomedical engineering",
            "Biological and biosystems engineering and biomedical technology"
        ],
        "Chemical and petroleum engineering": [
            "Chemical and biomolecular engineering",
            "Chemical engineering",
            "Petroleum engineering",
            "Chemical and petroleum engineering nec"
        ],
        "Civil, environmental, and transportation engineering": [
            "Civil engineering",
            "Environmental environmental health engineering",
            "Geotechnical and geoenvironmental engineering",
            "Structural engineering",
            "Transportation and highway engineering",
            "Civil, environmental, and transportation engineering nec"
        ],
        "Electrical and computer engineering": [
            "Computer engineering",
            "Electrical and computer engineering",
            "Electrical and electronics engineering",
            "Electrical and computer engineering nec"
        ],
        "Engineering technologies": [
            "Electrical and electronic engineering technologies",
            "Electromechanical technologies",
            "Environmental control technologies",
            "Engineering technologies nec"
        ],
        "Industrial engineering and operations research": [
            "Industrial engineering",
            "Operations research",
            "Systems and manufacturing engineering"
        ],
        "Materials and mining engineering": [
            "Materials engineering",
            "Materials science and engineering",
            "Materials and mining engineering nec"
        ],
        "Mechanical engineering": [
            "Mechanical engineering, general"
        ],
        "Engineering, other": [
            "Aerospace, aeronautical, astronautical, and space engineering",
            "Engineering mechanics, physics, and science",
            "Nanotechnology",
            "Nuclear engineering",
            "Engineering nec"
        ]
    },
    "Geosciences, atmospheric, and ocean sciences": {
        "Geological and earth sciences": [
            "Geochemistry",
            "Geology",
            "Geology earth science, general",
            "Geophysics and seismology",
            "Hydrology and water resources science",
            "Geological and earth sciences nec"
        ],
        "Ocean marine sciences and atmospheric science": [
            "Atmospheric sciences and meteorology, general",
            "Climatology, atmospheric chemistry and physics",
            "Marine biology and biological oceanography",
            "Marine sciences",
            "Oceanography, chemical and physical",
            "Atmospheric sciences and meteorology nec"
        ]
    },
    "Health sciences": {
        "Nursing and nursing science": [
            "Nursing education",
            "Nursing science",
            "Nursing specialties and practice"
        ],
        "Pharmacy and pharmaceutical sciences": [
            "Medicinal and pharmaceutical chemistry",
            "Pharmaceutical sciences",
            "Pharmacy, pharmaceutical sciences, and administration nec"
        ],
        "Public health": [
            "Environmental health",
            "Health services research",
            "Health medical physics",
            "Public health education and promotion",
            "Public health, general",
            "Public health nec"
        ],
        "Health sciences, other": [
            "Communication disorders sciences",
            "Exercise science and kinesiology",
            "Health sciences, general",
            "Marriage and family therapy counseling",
            "Medical clinical science",
            "Medical, biomedical, and health informatics",
            "Mental health, counseling, and therapy services and sciences",
            "Rehabilitation and therapeutic sciences",
            "Health sciences nec"
        ]
    },
    "Mathematics and statistics": {
        "Applied mathematics": [
            "Applied mathematics, general",
            "Computational and applied mathematics",
            "Applied mathematics nec"
        ],
        "Mathematics": [
            "Algebra and number theory",
            "Mathematics, general",
            "Mathematics nec"
        ],
        "Statistics": [
            "Applied statistics, general",
            "Mathematics and statistics",
            "Statistics",
            "Statistics nec"
        ]
    },
    "Multidisciplinary interdisciplinary sciences": {
        "Interdisciplinary computer sciences": [
            "Computer science and engineering",
            "Electrical engineering and computer science",
            "Interdisciplinary computer sciences nec"
        ],
        "Multidisciplinary interdisciplinary sciences, other": [
            "Behavioral and cognitive sciences",
            "Computational science and engineering",
            "Data science and data analytics",
            "History philosophy of science, technology and society",
            "Nanoscience nanoscale science",
            "Nutrition sciences",
            "Multidisciplinary interdisciplinary sciences nec"
        ]
    },
    "Physical sciences": {
        "Astronomy and astrophysics": [
            "Astronomy",
            "Astrophysics",
            "Astronomy and astrophysics nec"
        ],
        "Chemistry": [
            "Analytical chemistry",
            "Chemical biology",
            "Chemistry, general",
            "Inorganic chemistry",
            "Organic chemistry",
            "Physical chemistry",
            "Polymer chemistry",
            "Theoretical chemistry",
            "Chemistry nec"
        ],
        "Materials sciences": [
            "Materials science",
            "Materials chemistry and materials science nec"
        ],
        "Physics": [
            "Applied physics",
            "Atomic molecular physics",
            "Condensed matter and materials physics",
            "Elementary particle physics",
            "Nuclear physics",
            "Optics optical sciences",
            "Physics and astronomy",
            "Physics, general",
            "Plasma and high-temperature physics",
            "Theoretical and mathematical physics",
            "Physics and physical sciences nec"
        ]
    },
    "Psychology": {
        "Clinical psychology": [
            "Clinical child psychology",
            "Clinical psychology"
        ],
        "Counseling and applied psychology": [
            "Applied behavior analysis",
            "Counseling psychology",
            "Educational psychology",
            "Industrial and organizational psychology",
            "School psychology",
            "Counseling and applied psychology nec"
        ],
        "Research and experimental psychology": [
            "Behavioral neuroscience",
            "Cognitive psychology and psycholinguistics",
            "Developmental and child psychology",
            "Experimental psychology",
            "Social psychology",
            "Research and experimental psychology nec"
        ],
        "Psychology, other": [
            "Human development",
            "Psychology, general",
            "Psychology nec"
        ]
    },
    "Social sciences": {
        "Anthropology": [
            "Anthropology, general",
            "Cultural anthropology",
            "Physical and biological anthropology",
            "Anthropology nec"
        ],
        "Area, ethnic, cultural, gender, and group studies": [
            "Area studies",
            "Ethnic studies",
            "Area, ethnic, cultural, gender, and group studies nec"
        ],
        "Economics": [
            "Agricultural economics",
            "Applied economics",
            "Development economics and international development",
            "Econometrics and quantitative economics",
            "Economics, general",
            "Environmental natural resource economics",
            "Economics nec"
        ],
        "Political science and government": [
            "Political science and government, general",
            "Political science and government nec"
        ],
        "Public policy analysis": [
            "Education policy analysis",
            "Health policy analysis",
            "Public policy analysis, general",
            "Public policy nec"
        ],
        "Sociology, demography, and population studies": [
            "Sociology, general",
            "Sociology, demography, and population studies nec"
        ],
        "Social sciences, other": [
            "Applied linguistics",
            "Archeology",
            "Criminal justice and corrections",
            "Criminology",
            "Geography and cartography",
            "International relations and national security studies",
            "Linguistics",
            "Social sciences nec"
        ]
    },
    "Business": {
        "Business administration and management": [
            "Business management and administration",
            "Organizational leadership",
            "Business administration and management nec"
        ],
        "Business, other": [
            "Accounting and accounting-related",
            "Finance and financial management",
            "Management information systems",
            "Management sciences",
            "Marketing",
            "Organizational behavior studies",
            "Business nec"
        ]
    },
    "Education": {
        "Education leadership and administration": [
            "Educational leadership and administration, general",
            "Higher education and community college administration",
            "Education leadership and administration nec"
        ],
        "Education research": [
            "Curriculum and instruction",
            "Educational assessment, evaluation, and research methods",
            "Educational instructional technology and media design",
            "Higher education evaluation and research",
            "Student counseling and personnel services",
            "Education research nec"
        ],
        "Teacher education": [
            "Adult, continuing, and workforce education and development",
            "Bilingual, multilingual, and multicultural education",
            "Mathematics teacher education",
            "Music teacher education",
            "Special education and teaching",
            "STEM educational methods",
            "Teacher education, science and engineering",
            "Teacher education, specific levels and methods",
            "Teacher education, specific subject areas"
        ],
        "Education, other": [
            "Education, general",
            "Education nec"
        ]
    },
    "Humanities": {
        "English language and literature, letters": [
            "American literature (United States)",
            "Creative writing",
            "English language and literature, general",
            "English literature (British and commonwealth)",
            "Rhetoric and composition, and writing studies",
            "English language and literature nec"
        ],
        "Foreign languages, literatures, and linguistics": [
            "Comparative literature",
            "Hispanic Latin American languages, literatures, and linguistics",
            "Romance languages, literatures, and linguistics",
            "Spanish language and literature",
            "Foreign languages, literatures, and linguistics nec"
        ],
        "History": [
            "American history (United States)",
            "European history",
            "History, general",
            "History, regional focus",
            "History nec"
        ],
        "Philosophy and religious studies": [
            "Philosophy",
            "Religion religious studies",
            "Philosophy and religious studies nec"
        ],
        "Humanities, other": [
            "Bible biblical studies",
            "Humanities and humanistic studies",
            "Theological and ministerial studies"
        ]
    },
    "Visual and performing arts": {
        "Performing arts": [
            "Dance, drama, theatre arts and stagecraft",
            "Music performance",
            "Music theory and composition",
            "Musicology and ethnomusicology",
            "Music nec"
        ],
        "Visual arts, media studies, and design": [
            "Art history, criticism and conservation",
            "Film, cinema, and media studies",
            "Visual arts, media studies design, and arts management nec"
        ]
    },
    "Other non-science and engineering": {
        "Communications and journalism": [
            "Applied communication, advertising, and public relations",
            "Communication and media studies",
            "Communication, general",
            "Mass communication media studies",
            "Communications and journalism nec"
        ],
        "Multidisciplinary interdisciplinary studies": [
            "Classical and ancient studies",
            "Multidisciplinary interdisciplinary studies nec"
        ],
        "Public administration and social services": [
            "Public administration",
            "Social work and human services"
        ],
        "Non-science and engineering, other": [
            "Architecture and architectural studies ",
            "City urban, community, and regional planning",
            "Family, consumer sciences and human sciences",
            "Homeland security and protective services",
            "Law, legal studies and research",
            "Parks, recreation, leisure, fitness, and sport studies and management",
            "Other non-science and engineering nec"
        ]
    }
}
"""

            ```

            src/academic_metrics/postprocessing/BasePostprocessor.py:
            ```
from __future__ import annotations

import logging
from typing import Dict, TYPE_CHECKING, List, Set

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)

from academic_metrics.dataclass_models import StringVariation

if TYPE_CHECKING:
    from academic_metrics.dataclass_models import CategoryInfo
    from academic_metrics.utils import MinHashUtility


class BasePostprocessor:
    """
    A class responsible for processing and standardizing attribute data across different categories.

    This class provides methods to extract attribute sets from category data, remove near-duplicate values,
    and standardize attribute values to ensure consistency across categories. It utilizes MinHash for estimating
    the similarity between values to effectively identify and remove duplicates. Additionally, it maintains
    a dictionary of value variations to track the most frequent spelling variations of each value.

    Attributes:
        processed_sets_list (list): Stores processed attribute sets after deduplication and standardization.
        minhash_util (MinHashUtility): Utility for generating MinHash signatures and comparing them.
        value_variations (dict): Stores NameVariation objects for each normalized attribute value.

    Methods:
        extract_sets(category_dict): Extracts the specified attribute from each CategoryInfo object in the provided dictionary.
        remove_near_duplicates(category_dict): Processes each CategoryInfo object to remove near-duplicate attribute values and standardize them.
        standardized_data_update(category_dict, standardized_sets): Updates CategoryInfo objects with standardized attribute sets.
        standardize_attribute(category_dict): Standardizes attribute values across all categories based on the most frequent spelling variations.
        remove_update_attribute(category_dict, attribute_sets_list): Removes near-duplicate attribute values within each attribute set.
        duplicate_postprocessor(attribute_set, attribute_sets, similarity_threshold): Processes a set of attribute values to remove near-duplicates.
        process_value_pair(similarity_threshold, most_frequent_variation, value_signatures, to_remove, v1, v2): Compares two attribute values and determines which to keep.
        value_to_remove(most_frequent_variation, v1, v2, v1_normalized, v2_normalized): Determines which of two attribute values to remove based on their variations.
        get_duplicate_utilities(attribute_set, attribute_sets): Generates utilities needed for duplicate removal.
        generate_signatures(attribute_set): Generates MinHash signatures for each value in an attribute set.
        get_most_frequent_value_variation(attribute_sets_list): Maps each normalized value to its most frequent spelling variation.
        standardize_values_across_sets(attribute_sets_list): Standardizes values in attribute sets based on the most frequent value variation.
    """

    def __init__(
        self, attribute_name: str, minhash_util: MinHashUtility, threshold: float = 0.5
    ):
        self.logger: logging.Logger = configure_logging(
            module_name=__name__,
            log_file_name=f"{attribute_name}_postprocessor",
            log_level=DEBUG,
        )

        self.logger.info(f"Initializing {attribute_name}Postprocessor...")
        self.attribute_name: str = attribute_name
        self.processed_sets_list: List[Set[str]] = (
            []
        )  # List to store processed attribute sets

        self.minhash_util: MinHashUtility = minhash_util
        self.similarity_threshold: float = threshold

        self.logger.info("Initializing StringVariation dictionary...")
        self.string_variations: Dict[str, StringVariation] = (
            {}
        )  # Dictionary to store StringVariation objects for each normalized value

    def extract_sets(self, category_dict: Dict[str, CategoryInfo]) -> List[Set[str]]:
        """
        Extracts the specified attribute from each CategoryInfo object in the provided dictionary.

        This method iterates over a dictionary of CategoryInfo objects and collects the set
        from the attribute specified by self.attribute_name from each object.
        These sets are typically used for further processing, such as deduplication or analysis.

        Args:
            category_dict (Dict[str, CategoryInfo]): A dictionary where the keys are category identifiers
                                 and the values are CategoryInfo objects.

        Returns:
            List[Set[str]]: A list containing the attribute set from each CategoryInfo object.
        """
        self.logger.info(f"Extracting {self.attribute_name} sets...")
        list_of_sets: List[Set[str]] = [
            getattr(category_info, self.attribute_name)
            for category_info in category_dict.values()
        ]
        self.logger.info(f"{self.attribute_name} sets extracted.")
        return list_of_sets

    def remove_near_duplicates(
        self, *, category_dict: Dict[str, CategoryInfo]
    ) -> Dict[str, CategoryInfo]:
        """
        Processes each CategoryInfo object to remove near-duplicate values and standardize
        them across categories.

        This method orchestrates several steps to enhance the integrity and consistency
        of the specified attribute:
        1. Extract sets from each category
        2. Remove near-duplicate values within each set
        3. Standardize values across all categories
        4. Update the category data with cleaned and standardized sets

        Args:
            category_dict (Dict[str, CategoryInfo]): A dictionary where the keys are category
                                                    identifiers and the values are CategoryInfo
                                                    objects.

        Returns:
            Dict[str, CategoryInfo]: The updated dictionary with cleaned and standardized
                                    values across all CategoryInfo objects.
        """
        self.logger.info(f"Removing near-duplicates for {self.attribute_name}...")
        # Step 1
        attribute_sets_list: List[Set[str]] = self.extract_sets(
            category_dict=category_dict
        )
        self.logger.info(f"{self.attribute_name} sets extracted.")

        # Step 2
        self.logger.info(f"Removing near-duplicate {self.attribute_name} values...")
        self.remove_update_attribute(category_dict, attribute_sets_list)
        self.logger.info(f"Near-duplicate {self.attribute_name} values removed.")

        # Step 3
        self.logger.info(f"Standardizing {self.attribute_name} values...")
        standardized_sets: List[Set[str]] = self.standardize_attribute(category_dict)
        self.logger.info(f"{self.attribute_name} values standardized.")

        # Step 4
        self.logger.info(f"Updating category data...")
        self.standardized_data_update(category_dict, standardized_sets)
        self.logger.info(f"Category data updated.")
        return category_dict

    def standardized_data_update(
        self, category_dict: Dict[str, CategoryInfo], standardized_sets: List[Set[str]]
    ) -> None:
        """
        Updates the CategoryInfo objects in the dictionary with standardized sets.

        Args:
            category_dict (Dict[str, CategoryInfo]):
                - A dictionary where the keys are category identifiers
                - and the values are CategoryInfo objects.
            standardized_sets (List[Set[str]]): A list of sets containing standardized values.

        This method iterates over the category dictionary and updates each CategoryInfo
        object with the corresponding standardized set for the specified attribute.
        """
        self.logger.info(f"Updating {self.attribute_name} data...")
        for (_, category_info), standardized_set in zip(
            category_dict.items(), standardized_sets
        ):
            setattr(category_info, self.attribute_name, standardized_set)
        self.logger.info(f"{self.attribute_name} data updated.")

    def standardize_attribute(
        self, category_dict: Dict[str, CategoryInfo]
    ) -> List[Set[str]]:
        """
        Standardizes attribute values across all categories based on the most frequent spelling variations.

        Args:
            category_dict (Dict[str, CategoryInfo]): A dictionary where the keys are category identifiers and the values are CategoryInfo objects.

        Returns:
            List[Set[str]]: A list of sets containing the standardized attribute values across all categories.

        This method extracts updated attribute sets after duplicate removal and standardizes values across all sets based on the most frequent global variation.
        """
        self.logger.info(f"Standardizing {self.attribute_name} values...")
        updated_attribute_sets: List[Set[str]] = self.extract_sets(
            category_dict=category_dict
        )
        self.logger.info(f"{self.attribute_name} sets extracted.")

        self.logger.info(
            f"Standardizing {self.attribute_name} values across all sets..."
        )
        standardized_sets: List[Set[str]] = self.standardize_values_across_sets(
            updated_attribute_sets
        )
        self.logger.info(f"{self.attribute_name} values standardized.")

        return standardized_sets

    def remove_update_attribute(
        self,
        category_dict: Dict[str, CategoryInfo],
        attribute_sets_list: List[Set[str]],
    ) -> None:
        """
        Removes near-duplicate values within each set based on MinHash similarity.

        Args:
            category_dict (Dict[str, CategoryInfo]): A dictionary where the keys are category identifiers
                                                    and the values are CategoryInfo objects.
            attribute_sets_list (List[Set[str]]): A list containing the attribute set
                                           from each CategoryInfo object.

        This method iterates over each category and processes the attribute set to
        remove near-duplicates, updating the specified attribute of each CategoryInfo object.
        """
        self.logger.info(
            f"Removing near-duplicate {self.attribute_name} values within each set..."
        )
        for _, category_info in category_dict.items():
            current_set = getattr(category_info, self.attribute_name)
            final_set: Set[str] = self.duplicate_postprocessor(
                current_set, attribute_sets_list, self.similarity_threshold
            )
            setattr(category_info, self.attribute_name, final_set)
        self.logger.info(f"Near-duplicate {self.attribute_name} values removed.")

    def duplicate_postprocessor(
        self,
        attribute_set: Set[str] | List[str],
        attribute_sets: List[Set[str]],
        similarity_threshold: float = 0.5,
    ) -> Set[str]:
        """
        Processes a set of values to remove near-duplicates based on MinHash similarity and most frequent variations.

        This method first generates the necessary utilities for comparison and removal.
        It then compares each value against all others in the set for near duplicates.
        If a value is deemed to be a duplicate based on MinHash similarity and the most
        frequent variation, it is added to the set of values to be removed.
        Finally, the refined set is returned, excluding any values deemed to be duplicates.

        Args:
            attribute_set (Set[str] | List[str]): A set or list of values to be processed.
            attribute_sets (List[Set[str]]): A list of sets, where each set contains values
                                           from a different category.
            similarity_threshold (float): The threshold for considering values as duplicates
                                        based on MinHash similarity.

        Returns:
            Set[str]: The refined set, excluding any values deemed to be duplicates.
        """
        # Generate needed utilities
        # most_frequent_variation is a dictionary that maps each normalized value to its most frequent variation
        # value_signatures is a dictionary that maps each value to its MinHash signature
        # to_remove is a set of values to be removed from the attribute set
        self.logger.info(
            f"Generating needed utilities for {self.attribute_name} processing..."
        )
        if not isinstance(attribute_set, set):
            attribute_set = set(attribute_set)

        (
            most_frequent_variation,
            value_signatures,
            to_remove,
        ) = self.get_duplicate_utilities(attribute_set, attribute_sets)
        self.logger.info("Needed utilities generated.")

        # Compare each value against all others in the set for near duplicates
        self.logger.info(
            f"Comparing each {self.attribute_name} value against all others..."
        )
        for v1 in attribute_set:
            self.logger.info(f"Comparing {v1} against all others in the set...")
            for v2 in attribute_set:
                self.logger.info(f"Comparing {v1} against {v2}...")
                if v1 == v2:
                    self.logger.info(f"{v1} is the same as {v2}, skipping...")
                    continue
                self.process_value_pair(
                    similarity_threshold,
                    most_frequent_variation,
                    value_signatures,
                    to_remove,
                    v1,
                    v2,
                )

        self.logger.info(f"Refining {self.attribute_name} set...")
        refined_attribute_set: Set[str] = attribute_set - to_remove
        self.logger.info(f"{self.attribute_name} set refined.")
        return refined_attribute_set

    def process_value_pair(
        self,
        similarity_threshold: float,
        most_frequent_variation: Dict[str, str],
        value_signatures: Dict[str, List[int]],
        to_remove: Set[str],
        v1: str,
        v2: str,
    ) -> None:
        """
        Compares two values and determines which one to keep based on MinHash similarity
        and most frequent variation.

        This method first compares the MinHash signatures of the two values to determine
        their similarity. If the similarity exceeds the specified threshold, it then
        determines which value to remove based on the most frequent variation. The value
        not chosen as the most frequent variation is added to the set of values to be removed.

        Args:
            similarity_threshold (float): Threshold for considering values as duplicates.
            most_frequent_variation (Dict[str, str]): Dictionary mapping normalized values
                                                     to their most frequent variations.
            value_signatures (Dict[str, List[int]]): Dictionary of MinHash signatures.
            to_remove (Set[str]): Set of values to be removed.
            v1, v2 (str): Values to compare.
        """
        self.logger.info(f"Comparing {self.attribute_name} values {v1} and {v2}...")

        self.logger.info(f"Getting MinHash signatures for {v1} and {v2}...")
        signature1: List[int] = value_signatures[v1]
        signature2: List[int] = value_signatures[v2]
        self.logger.info(f"MinHash signatures retrieved.")

        self.logger.info(f"Comparing MinHash signatures...")
        similarity: float = self.minhash_util.compare_signatures(signature1, signature2)
        self.logger.info(f"MinHash signatures compared. Similarity: {similarity}")

        # Early exit if the similarity is below the threshold
        if similarity <= similarity_threshold:
            self.logger.info(
                f"Similarity is below threshold ({similarity_threshold}), skipping..."
            )
            return

        self.logger.info(f"Normalizing values...")
        v1_normalized: str = v1.lower().replace(" ", "")
        v2_normalized: str = v2.lower().replace(" ", "")
        self.logger.info(f"Values normalized.")

        # Decide which value to keep
        self.logger.info(f"Deciding which value to keep...")
        to_remove.add(
            self.value_to_remove(
                most_frequent_variation, v1, v2, v1_normalized, v2_normalized
            )
        )
        self.logger.info(f"Value to remove decided.")

    def value_to_remove(
        self,
        most_frequent_variation: Dict[str, str],
        v1: str,
        v2: str,
        v1_normalized: str,
        v2_normalized: str,
    ) -> str:
        """
        Determines which of two values to remove based on their normalized forms and most frequent variations.

        This method checks if the normalized form of each value matches its most frequent variation.
        If one value matches its most frequent variation and the other does not, the non-matching
        value is chosen for removal. If neither or both values match their most frequent variations,
        the lexicographically greater value is chosen for removal.

        Args:
            most_frequent_variation (Dict[str, str]): Dictionary mapping normalized values
                                                     to their most frequent variations.
            v1, v2 (str): Original values to compare.
            v1_normalized, v2_normalized (str): Normalized forms of the values.

        Returns:
            str: The value to be removed.
        """
        self.logger.info(f"Deciding which {self.attribute_name} value to remove...")
        if (
            most_frequent_variation[v1_normalized] == v1
            and most_frequent_variation[v2_normalized] != v2
        ):
            self.logger.info(f"{v2} is the value to remove.")
            return v2
        elif (
            most_frequent_variation[v2_normalized] == v2
            and most_frequent_variation[v1_normalized] != v1
        ):
            self.logger.info(f"{v1} is the value to remove.")
            return v1
        # If none of the above conditions hold, return the lexicographically greater value
        self.logger.info(f"Returning the lexicographically greater value...")

        return v2 if v1 < v2 else v1

    def get_duplicate_utilities(
        self, attribute_set: set[str], attribute_sets: list[set[str]]
    ) -> tuple[Dict[str, str], Dict[str, List[int]], Set[str]]:
        """
        Generates utilities needed for duplicate removal.

        Args:
            attribute_set (set[str]): A set of values for which to generate MinHash signatures.
            attribute_sets (list[set[str]]): A list of sets, where each set contains values
                                           from a different category.

        Returns:
            tuple[Dict[str, str], Dict[str, List[int]], Set[str]]: A tuple containing:
                - most_frequent_variation: Dictionary mapping normalized values to their most frequent variations
                - value_signatures: Dictionary mapping each value to its MinHash signature
                - to_remove: Empty set for collecting values to be removed
        """
        self.logger.info(
            f"Generating utilities needed for {self.attribute_name} processing..."
        )
        most_frequent_variation: Dict[str, str] = self.get_most_frequent_variation(
            attribute_sets
        )
        self.logger.info("Most frequent variation dictionary generated.")

        value_signatures: Dict[str, List[int]] = self.generate_signatures(attribute_set)
        self.logger.info("Value signatures dictionary generated.")

        to_remove: Set[str] = set()
        self.logger.info("Set of values to remove initialized.")

        return most_frequent_variation, value_signatures, to_remove

    def generate_signatures(self, attribute_set: Set[str]) -> Dict[str, List[int]]:
        """
        Generates MinHash signatures for each value in the given set.

        This method tokenizes each value into n-grams, computes a MinHash signature for
        these n-grams, and stores the result. A MinHash signature is a compact representation
        of the set of n-grams and is used to estimate the similarity between sets of values.

        Args:
            attribute_set (set[str]): A set of values for which to generate MinHash signatures.

        Returns:
            dict[str, list[int]]: A dictionary mapping each value to its corresponding
                                 MinHash signature.
        """
        self.logger.info(
            f"Generating MinHash signatures for each {self.attribute_name} value..."
        )
        # Dictionary comprehension to generate a MinHash signature for each value
        value_signatures: Dict[str, List[int]] = {
            value: self.minhash_util.compute_signature(
                self.minhash_util.tokenize(value)
            )
            for value in attribute_set
        }
        self.logger.info(
            f"MinHash signatures for {self.attribute_name} values generated."
        )

        return value_signatures

    def get_most_frequent_variation(
        self, attribute_sets_list: List[Set[str]]
    ) -> Dict[str, str]:
        """
        Creates a dictionary that maps each unique normalized value to its most commonly
        occurring spelling variation across all provided sets.

        A 'normalized value' is derived by converting the original value to lowercase and
        removing all spaces, which helps in identifying different spellings of the same value
        as equivalent. The 'most frequent variation' refers to the spelling of the value that
        appears most often in the data, maintaining the original case and spaces.

        Args:
            attribute_sets_list (List[Set[str]]): A list where each set contains values from
                                                 a specific category. Each set may include
                                                 various spelling variations.

        Returns:
            Dict[str, str]: A dictionary with normalized values as keys and their most
                           frequent original spelling variations as values.
        """
        # Dictionary to store StringVariation objects for each normalized value
        self.logger.info(
            f"Creating dictionary to store {self.attribute_name} variations..."
        )
        value_variations: Dict[str, StringVariation] = self.string_variations
        self.logger.info("Variation dictionary initialized.")

        # Iterate over each set of values
        self.logger.info(f"Processing {self.attribute_name} sets...")
        for attribute_set in attribute_sets_list:
            self.logger.info(f"Processing set: {attribute_set}...")
            for value in attribute_set:
                self.logger.info(f"Processing value: {value}...")
                # Normalize the value
                normalized_value: str = value.lower().replace(" ", "")
                self.logger.info(f"Normalized value: {normalized_value}")

                # Create new StringVariation object if needed
                if normalized_value not in value_variations:
                    self.logger.info(
                        f"Creating new variation tracker for: {normalized_value}"
                    )
                    value_variations[normalized_value] = StringVariation(
                        normalized_value
                    )

                # Track this variation
                value_variations[normalized_value].add_variation(value)
                self.logger.info(f"Variation tracked: {value}")

        # Create dictionary of most frequent variations
        self.logger.info(
            f"Determining most frequent {self.attribute_name} variations..."
        )
        most_frequent_variation: Dict[str, str] = {
            normalized_value: variation.most_frequent_variation()
            for normalized_value, variation in value_variations.items()
        }
        self.logger.info("Most frequent variations determined.")

        return most_frequent_variation

    def standardize_values_across_sets(
        self, attribute_sets_list: List[Set[str]]
    ) -> List[Set[str]]:
        """
        Standardizes values across all sets by mapping each value to its most frequent
        variation across all sets.

        This method first generates a mapping of the most frequent variations for all values,
        then uses this mapping to standardize each value in each set. If a value has no
        recorded frequent variation, it remains unchanged.

        Args:
            attribute_sets_list (List[Set[str]]): A list of sets, where each set contains
                                                 values from a different category.

        Returns:
            List[Set[str]]: A list of sets containing the standardized values across
                           all categories.
        """
        # Generate the most frequent variation mapping across all sets
        self.logger.info(
            f"Generating the most frequent {self.attribute_name} variation mapping..."
        )
        most_frequent_variation: Dict[str, str] = self.get_most_frequent_variation(
            attribute_sets_list
        )
        self.logger.info("Most frequent variation mapping generated.")

        # Iterate through each set and standardize values based on the global mapping
        standardized_sets: List[Set[str]] = []
        self.logger.info(f"Standardizing {self.attribute_name} values across sets...")
        for attribute_set in attribute_sets_list:
            self.logger.info(f"Processing set: {attribute_set}...")
            standardized_set: Set[str] = set()
            for value in attribute_set:
                normalized_value: str = value.lower().replace(" ", "")
                self.logger.info(f"Normalized value: {normalized_value}")
                # Replace the value with its most frequent variation, if available
                standardized_value: str = most_frequent_variation.get(
                    normalized_value, value
                )
                self.logger.info(f"Standardized to: {standardized_value}")
                standardized_set.add(standardized_value)
            standardized_sets.append(standardized_set)
            self.logger.info("Set standardization complete.")

        self.logger.info(f"All {self.attribute_name} sets standardized.")
        return standardized_sets

            ```

            src/academic_metrics/postprocessing/DepartmentPostprocessor.py:
            ```
from __future__ import annotations

from typing import Dict, TYPE_CHECKING

from .BasePostprocessor import BasePostprocessor
from academic_metrics.configs import configure_logging, DEBUG
from academic_metrics.dataclass_models import CategoryInfo

if TYPE_CHECKING:
    from academic_metrics.utils import MinHashUtility


class DepartmentPostprocessor(BasePostprocessor):
    def __init__(self, minhash_util: MinHashUtility, threshold: float = 0.7):
        """
        Initialize the DepartmentPostprocessor with a MinHashUtility instance.

        Args:
            minhash_util (MinHashUtility): A MinHashUtility instance for minhash operations.
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="department_postprocessor",
            log_level=DEBUG,
        )

        self.logger.info("Initializing DepartmentPostprocessor...")
        super().__init__(
            attribute_name="departments", minhash_util=minhash_util, threshold=threshold
        )

    def remove_near_duplicates(
        self,
        *,
        category_dict: Dict[str, CategoryInfo],
    ) -> Dict[str, CategoryInfo]:
        """
        Remove near-duplicate department names from the category dictionary.

        Args:
            category_dict (Dict[str, CategoryInfo]): A dictionary mapping category names
                                                    to CategoryInfo objects.

        Returns:
            Dict[str, CategoryInfo]: A dictionary mapping category names to CategoryInfo
                                    objects with near-duplicate department names removed.
        """
        self.logger.info("Processing department names across categories...")
        processed_dict = super().remove_near_duplicates(category_dict=category_dict)
        self.logger.info("Department names processed.")
        return processed_dict

            ```

            src/academic_metrics/postprocessing/FacultyPostprocessor.py:
            ```
from __future__ import annotations

from typing import Dict, TYPE_CHECKING

from .BasePostprocessor import BasePostprocessor
from academic_metrics.configs import configure_logging, DEBUG
from academic_metrics.dataclass_models import CategoryInfo

if TYPE_CHECKING:
    from academic_metrics.utils import MinHashUtility


class FacultyPostprocessor(BasePostprocessor):
    def __init__(self, minhash_util: MinHashUtility, threshold: float = 0.5):
        """
        Initialize the FacultyPostprocessor with a MinHashUtility instance.

        Args:
            minhash_util (MinHashUtility): A MinHashUtility instance for minhash operations.
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="faculty_postprocessor",
            log_level=DEBUG,
        )

        self.logger.info("Initializing FacultyPostprocessor...")
        super().__init__(
            attribute_name="faculty", minhash_util=minhash_util, threshold=threshold
        )

    def remove_near_duplicates(
        self,
        *,
        category_dict: Dict[str, CategoryInfo],
    ) -> Dict[str, CategoryInfo]:
        """
        Remove near-duplicate faculty names from the category dictionary.

        Args:
            category_dict (Dict[str, CategoryInfo]): A dictionary mapping category names
                                                    to CategoryInfo objects.

        Returns:
            Dict[str, CategoryInfo]: A dictionary mapping category names to CategoryInfo
                                    objects with near-duplicate faculty names removed.
        """
        self.logger.info("Processing faculty names across categories...")
        processed_dict = super().remove_near_duplicates(category_dict=category_dict)
        self.logger.info("Faculty names processed.")
        return processed_dict

            ```

            src/academic_metrics/postprocessing/__init__.py:
            ```
from .BasePostprocessor import BasePostprocessor
from .FacultyPostprocessor import FacultyPostprocessor
from .DepartmentPostprocessor import DepartmentPostprocessor

            ```

            src/academic_metrics/runners/__init__.py:
            ```
from .pipeline import PipelineRunner, get_excel_report, command_line_runner

            ```

            src/academic_metrics/runners/pipeline.py:
            ```
import json
import logging
import os
from collections import defaultdict
from typing import Any, Callable, Dict, List, TypedDict
from urllib.parse import quote, unquote

from academic_metrics.constants import (
    INPUT_FILES_DIR_PATH,
    LOG_DIR_PATH,
    OUTPUT_FILES_DIR_PATH,
    SPLIT_FILES_DIR_PATH,
)
from academic_metrics.core import CategoryProcessor
from academic_metrics.postprocessing import (
    FacultyPostprocessor,
    DepartmentPostprocessor,
)
from academic_metrics.data_collection import CrossrefWrapper, Scraper
from academic_metrics.DB import DatabaseWrapper
from academic_metrics.enums import AttributeTypes
from academic_metrics.factories import (
    DataClassFactory,
    ClassifierFactory,
    StrategyFactory,
)
from academic_metrics.orchestrators import (
    CategoryDataOrchestrator,
    ClassificationOrchestrator,
)
from academic_metrics.utils import (
    APIKeyValidator,
    Taxonomy,
    Utilities,
    WarningManager,
    MinHashUtility,
)
from academic_metrics.configs import configure_logging, DEBUG


class SaveOfflineKwargs(TypedDict):
    offline: bool
    run_crossref_before_file_load: bool
    make_files: bool
    extend: bool


class PipelineRunner:
    """Orchestrates the academic metrics data processing pipeline.

    This class manages the end-to-end process of collecting, processing, and storing
    academic publication data. It handles data collection from Crossref, classification
    of publications, generation of statistics, and storage in MongoDB.

    Attributes:
        SAVE_OFFLINE_KWARGS (SaveOfflineKwargs): Default configuration for offline processing.
        logger (logging.Logger): Pipeline-wide logger instance.
        ai_api_key (str): API key for AI services.
        db_name (str): Name of the MongoDB database.
        mongodb_uri (str): URI for MongoDB connection.
        db (DatabaseWrapper): Database interface instance.
        scraper (Scraper): Web scraping utility instance.
        crossref_wrapper (CrossrefWrapper): Crossref API interface instance.
        taxonomy (Taxonomy): Publication taxonomy utility.
        warning_manager (WarningManager): Warning logging utility.
        strategy_factory (StrategyFactory): Strategy pattern factory.
        utilities (Utilities): General utility functions.
        classification_orchestrator (ClassificationOrchestrator): Publication classifier.
        dataclass_factory (DataClassFactory): Data class creation utility.
        category_processor (CategoryProcessor): Category statistics processor.
        faculty_postprocessor (FacultyPostprocessor): Faculty data processor.
        department_postprocessor (DepartmentPostprocessor): Department data processor.
        debug (bool): Debug mode flag.

    Methods:
        run_pipeline: Executes the main data processing pipeline.
        _create_taxonomy: Creates a new Taxonomy instance.
        _create_classifier_factory: Creates a new ClassifierFactory instance.
        _create_warning_manager: Creates a new WarningManager instance.
        _create_strategy_factory: Creates a new StrategyFactory instance.
        _create_utilities_instance: Creates a new Utilities instance.
        _create_classification_orchestrator: Creates a new ClassificationOrchestrator.
        _create_orchestrator: Creates a new CategoryDataOrchestrator.
        _get_acf_func: Returns the abstract classifier factory function.
        _validate_api_key: Validates the provided API key.
        _make_files: Creates split files from input files.
        _load_files: Loads and returns data from split files.
        _create_dataclass_factory: Creates a new DataClassFactory instance.
        _create_crossref_wrapper: Creates a new CrossrefWrapper instance.
        _create_category_processor: Creates a new CategoryProcessor instance.
        _create_faculty_postprocessor: Creates a new FacultyPostprocessor instance.
        _create_scraper: Creates a new Scraper instance.
        _create_db: Creates a new DatabaseWrapper instance.
        _encode_affiliation: URL encodes an affiliation string.
    """

    SAVE_OFFLINE_KWARGS: SaveOfflineKwargs = {
        "offline": False,
        "run_crossref_before_file_load": False,
        "make_files": False,
        "extend": False,
    }

    def __init__(
        self,
        ai_api_key: str,
        crossref_affiliation: str,
        data_from_month: int,
        data_to_month: int,
        data_from_year: int,
        data_to_year: int,
        mongodb_uri: str,
        db_name: str | None = "Site_Data",
        debug: bool | None = False,
        pre_classification_model: str | None = "gpt-4o-mini",
        classification_model: str | None = "gpt-4o-mini",
        theme_model: str | None = "gpt-4o-mini",
    ):
        """Initialize the PipelineRunner with necessary configurations and dependencies.

        Args:
            ai_api_key (str): API key for AI services (e.g., OpenAI).
            crossref_affiliation (str): Institution name to search for in Crossref.
            data_from_year (int): Start year for publication data collection.
            data_to_year (int): End year for publication data collection.
            mongodb_uri (str): Connection URL for MongoDB instance.
            db_name (str, optional): Name of the MongoDB database. Defaults to "Site_Data".
            debug (bool, optional): Enable debug mode for additional logging and controls. Defaults to False.

        Raises:
            Exception: If logger setup fails or required dependencies cannot be initialized.
        """
        self.logger: logging.Logger = configure_logging(__name__, "pipeline", DEBUG)
        self.logger.info("Initializing PipelineRunner...")
        self.logger.info("PipelineRunner logger initialized successfully")

        self.logger.info("Initializing PipelineRunner dependencies...")
        self.ai_api_key: str = ai_api_key
        self.db_name: str = db_name
        self.mongodb_uri: str = mongodb_uri

        self.logger.info("Creating DatabaseWrapper instance...")
        self.db: DatabaseWrapper = self._create_db()
        self.logger.info("DatabaseWrapper instance created successfully")

        self.logger.info("Creating Scraper instance...")
        self.scraper: Scraper = self._create_scraper()
        self.logger.info("Scraper instance created successfully")

        self.logger.info("Creating CrossrefWrapper instance...")
        self.crossref_wrapper: CrossrefWrapper = self._create_crossref_wrapper(
            affiliation=self._encode_affiliation(crossref_affiliation),
            from_month=data_from_month,
            to_month=data_to_month,
            from_year=data_from_year,
            to_year=data_to_year,
        )
        self.logger.info("CrossrefWrapper instance created successfully")

        self.logger.info("Creating Taxonomy instance...")
        self.taxonomy: Taxonomy = self._create_taxonomy()
        self.logger.info("Taxonomy instance created successfully")

        self.logger.info("Creating WarningManager instance...")
        self.warning_manager: WarningManager = self._create_warning_manager()
        self.logger.info("WarningManager instance created successfully")

        self.logger.info("Creating StrategyFactory instance...")
        self.strategy_factory: StrategyFactory = self._create_strategy_factory()
        self.logger.info("StrategyFactory instance created successfully")

        self.logger.info("Creating Utilities instance...")
        self.utilities: Utilities = self._create_utilities_instance()
        self.logger.info("Utilities instance created successfully")

        self.logger.info("Creating ClassificationOrchestrator instance...")
        self.classification_orchestrator: ClassificationOrchestrator = (
            self._create_classification_orchestrator()
        )
        self.logger.info("ClassificationOrchestrator instance created successfully")

        self.logger.info("Creating DataClassFactory instance...")
        self.dataclass_factory: DataClassFactory = self._create_dataclass_factory()
        self.logger.info("DataClassFactory instance created successfully")

        self.logger.info("Creating CategoryProcessor instance...")
        self.category_processor: CategoryProcessor = self._create_category_processor()
        self.logger.info("CategoryProcessor instance created successfully")

        self.logger.info("Creating MinHashUtility instance...")
        self.minhash_util: MinHashUtility = self._create_minhash_util()
        self.logger.info("MinHashUtility instance created successfully")

        self.logger.info("Creating FacultyPostprocessor instance...")
        self.faculty_postprocessor: FacultyPostprocessor = (
            self._create_faculty_postprocessor(minhash_util=self.minhash_util)
        )
        self.logger.info("FacultyPostprocessor instance created successfully")

        self.logger.info("Creating DepartmentPostprocessor instance...")
        self.department_postprocessor: DepartmentPostprocessor = (
            self._create_department_postprocessor(minhash_util=self.minhash_util)
        )
        self.logger.info("DepartmentPostprocessor instance created successfully")

        self.logger.info("Setting debug mode...")
        self.debug: bool = debug
        self.logger.info("Debug mode set successfully")

        self.logger.info("Setting pre-classification-model...")
        self.pre_classification_model: str | None = pre_classification_model
        self.logger.info("Pre-classification-model set successfully")

        self.logger.info("Setting classification-model...")
        self.classification_model: str | None = classification_model
        self.logger.info("Classification-model set successfully")

        self.logger.info("Setting theme-model...")
        self.theme_model: str | None = theme_model
        self.logger.info("Theme-model set successfully")

        self.logger.info("PipelineRunner initialized successfully")

    def run_pipeline(
        self,
        save_offline_kwargs: SaveOfflineKwargs = SAVE_OFFLINE_KWARGS,
        test_filtering: bool | None = False,
        save_to_db: bool | None = True,
    ):
        """Execute the main data processing pipeline.

        This method orchestrates the entire pipeline process:
        1. Retrieves existing DOIs from database
        2. Collects new publication data from Crossref
        3. Filters out duplicate articles
        4. Runs AI classification on publications
        5. Processes and generates category statistics
        6. Saves processed data to MongoDB

        Args:
            save_offline_kwargs (SaveOfflineKwargs, optional): Configuration for offline processing.
                Defaults to SAVE_OFFLINE_KWARGS.
                - offline: Whether to run in offline mode
                - run_crossref_before_file_load: Run Crossref before loading files
                - make_files: Generate new split files
                - extend: Extend existing data

        Raises:
            Exception: If there are errors in data processing or database operations.
        """
        self.logger.info("Running pipeline...")

        # Get the existing DOIs from the database
        # so that we don't process duplicates
        self.logger.info("Getting existing DOIs from database...")
        existing_dois: List[str] = []
        if save_to_db:
            existing_dois: List[str] = self.db.get_dois()
        self.logger.info(f"Found {len(existing_dois)} existing DOIs in database")

        # Get data from crossref for the school and date range
        self.logger.info("Getting data from Crossref...")
        data: List[Dict[str, Any]] = []
        if save_offline_kwargs["offline"]:
            if save_offline_kwargs["run_crossref_before_file_load"]:
                data: List[Dict[str, Any]] = self.crossref_wrapper.run_all_process()
            if save_offline_kwargs["make_files"]:
                self._make_files()
            data: List[Dict[str, Any]] = self._load_files()
        else:
            # Fetch raw data from Crossref api for the year range
            # and get out the result list containing the raw data.
            data: List[Dict[str, Any]] = (
                self.crossref_wrapper.run_afetch_yrange().get_result_list()
            )
        self.logger.info(
            "Filtering out articles whose DOIs are already in the db or those that are not found..."
        )
        # Then filter out articles whose DOIs are already
        # in the db or those that are not found.
        already_existing_count: int = 0
        filtered_data: List[Dict[str, Any]] = []
        for article in data:
            # Get the DOI out of the article item
            attribute_results: List[str] = self.utilities.get_attributes(
                article, [AttributeTypes.CROSSREF_DOI]
            )
            # Unpack the DOI from the dict returned by get_attributes
            doi = (
                attribute_results[AttributeTypes.CROSSREF_DOI][1]
                if attribute_results[AttributeTypes.CROSSREF_DOI][0]
                else None
            )
            # Only keep articles that have a DOI and aren't already in the database
            if doi is not None:
                if doi not in existing_dois:
                    filtered_data.append(article)
                else:
                    already_existing_count += 1
            else:
                self.logger.warning(f"Article with no DOI: {article}")
                continue

        self.logger.info(f"Filtered out {already_existing_count}/{len(data)} articles")
        self.logger.info(f"Articles to process: {len(filtered_data)}")
        self.logger.info("Initial filtering complete")

        if len(filtered_data) == 0:
            self.logger.info("No articles to process")
            return

        # Then set data to filtered data so we don't
        # keep the raw data floating in memory.
        data: List[Dict[str, Any]] = filtered_data

        # Now run final processing to have `Scraper` fetch missing abstracts.
        # Reset the result list in `CrossrefWrapper` so it doesn't
        # run on the original raw data, and instead runs on the filtered data.
        self.logger.info("Resetting CrossrefWrapper result list...")
        self.crossref_wrapper.result = data
        self.logger.info("CrossrefWrapper result list reset successfully")

        # Run the final processing to fetch missing abstracts
        # and get out the final data.
        # Again, we don't want to keep the raw data floating in memory,
        # so we reassign `data` to the the result list returned by `.get_result_list()`.
        self.logger.info("Running final processing to fetch missing abstracts...")
        data = self.crossref_wrapper.final_data_process().get_result_list()
        self.logger.info("Final processing complete")

        if len(data) == 0:
            self.logger.info(
                "None of the remaining articles have abstracts or none could be retrieved"
            )
            return

        if test_filtering:
            print(f"\n\nFiltered out {already_existing_count} articles\n\n")
            print(
                f"\n\nFILTERED DATA VAR CONTENTS:\n{json.dumps(filtered_data, indent=4)}\n\n"
            )
            print(f"\n\nDATA VAR CONTENTS:\n{data}\n\n")
            return

        self.logger.info(f"\n\nDATA: {data}\n\n")

        if self.debug:
            print(f"There are {len(data)} articles to process.")
            response: str = input("Would you like to slice the data? (y/n)")
            if response == "y":
                res: str = input("How many articles would you like to process?")
                data = data[: int(res)]
                self.logger.info(f"\n\nSLICED DATA:\n{data}\n\n")

        # Run classification on all data
        # comment out to run without AI for testing
        self.logger.info("Running classification...")
        data = self.classification_orchestrator.run_classification(
            data,
            pre_classification_model=self.pre_classification_model,
            classification_model=self.classification_model,
            theme_model=self.theme_model,
        )
        self.logger.info("Classification complete")

        with open("classified_data.json", "w") as file:
            json.dump(data, file, indent=4)

        # Process classified data and generate category statistics
        self.logger.info(
            "Processing classified data and generating category statistics..."
        )
        category_orchestrator: CategoryDataOrchestrator = self._create_orchestrator(
            data=data,
            extend=save_offline_kwargs["extend"],
        )
        category_orchestrator.run_orchestrator()
        self.logger.info("Category statistics processing complete")

        # Get all the processed data from CategoryDataOrchestrator
        self.logger.info("Getting final data...")

        self.logger.info("Getting final category data...")
        category_data: List[Dict[str, Any]] = (
            category_orchestrator.get_final_category_data()
        )
        self.logger.info("Final category data retrieved successfully")

        self.logger.info("Getting final faculty data...")
        # faculty_data = self.category_orchestrator.get_final_faculty_data()
        article_data: List[Dict[str, Any]] = (
            category_orchestrator.get_final_article_data()
        )
        self.logger.info("Final article data retrieved successfully")

        self.logger.info("Getting final global faculty data...")
        global_faculty_data: List[Dict[str, Any]] = (
            category_orchestrator.get_final_global_faculty_data()
        )
        self.logger.info("Final global faculty data retrieved successfully")

        if save_to_db:
            self.logger.info("Attempting to save data to database...")
            try:
                self.db.insert_categories(category_data)
                self.logger.info(
                    f"""Successfully inserted {len(category_data)} categories into database"""
                )
            except Exception as e:
                self.logger.error(f"Error saving to database: {e}")

            try:
                self.db.insert_articles(article_data)
                self.logger.info(
                    f"""Successfully inserted {len(article_data)} articles into database"""
                )
            except Exception as e:
                self.logger.error(f"Error saving to database: {e}")

            try:
                self.db.insert_faculty(global_faculty_data)
                self.logger.info(
                    f"""Successfully inserted {len(global_faculty_data)} faculty into database"""
                )
            except Exception as e:
                self.logger.error(f"Error saving to database: {e}")

    def test_run(self):
        with open("test_processed_category_data.json", "r") as file:
            category_data: List[Dict[str, Any]] = json.load(file)

        try:
            self.db.insert_categories(category_data)
            self.logger.info(
                f"""Successfully inserted {len(category_data)} categories into database"""
            )
        except Exception as e:
            self.logger.error(f"Error saving to database: {e}")

        with open("test_processed_article_stats_obj_data.json", "r") as file:
            article_data: List[Dict[str, Any]] = json.load(file)

        try:
            self.db.insert_articles(article_data)
            self.logger.info(
                f"""Successfully inserted {len(article_data)} articles into database"""
            )
        except Exception as e:
            self.logger.error(f"Error saving to database: {e}")

        with open("test_processed_global_faculty_stats_data.json", "r") as file:
            global_faculty_data: List[Dict[str, Any]] = json.load(file)

        try:
            self.db.insert_faculty(global_faculty_data)
            self.logger.info(
                f"""Successfully inserted {len(global_faculty_data)} faculty into database"""
            )
        except Exception as e:
            self.logger.error(f"Error saving to database: {e}")

    def _create_taxonomy(self) -> Taxonomy:
        """Create a new Taxonomy instance for publication classification.

        Returns:
            Taxonomy: A new instance of the Taxonomy utility class.
        """
        return Taxonomy()

    def _create_classifier_factory(self) -> ClassifierFactory:
        """Create a new ClassifierFactory for generating publication classifiers.

        Returns:
            ClassifierFactory: A factory instance configured with taxonomy and AI API key.
        """
        return ClassifierFactory(
            taxonomy=self.taxonomy,
            ai_api_key=self.ai_api_key,
        )

    def _create_warning_manager(self) -> WarningManager:
        """Create a new WarningManager for handling pipeline warnings.

        Returns:
            WarningManager: A new instance of the warning management utility.
        """
        return WarningManager()

    def _create_strategy_factory(self) -> StrategyFactory:
        """Create a new StrategyFactory for generating processing strategies.

        Returns:
            StrategyFactory: A new instance of the strategy factory.
        """
        return StrategyFactory()

    def _create_utilities_instance(self) -> Utilities:
        """Create a new Utilities instance with required dependencies.

        Returns:
            Utilities: A utility instance configured with strategy factory and warning manager.
        """
        return Utilities(
            strategy_factory=self.strategy_factory,
            warning_manager=self.warning_manager,
        )

    def _create_classification_orchestrator(self) -> ClassificationOrchestrator:
        """Create a new ClassificationOrchestrator for managing publication classification.

        Returns:
            ClassificationOrchestrator: An orchestrator instance configured with classifier factory and utilities.
        """
        return ClassificationOrchestrator(
            abstract_classifier_factory=self._get_acf_func(),
            utilities=self.utilities,
        )

    def _create_orchestrator(
        self, data: List[Dict[str, Any]], extend: bool
    ) -> CategoryDataOrchestrator:
        """Create a new CategoryDataOrchestrator for managing category data processing.

        Args:
            data (List[Dict[str, Any]]): List of publication data to process.
            extend (bool): Whether to extend existing data.

        Returns:
            CategoryDataOrchestrator: An orchestrator instance configured with all necessary processors and utilities.
        """
        return CategoryDataOrchestrator(
            data=data,
            output_dir_path=OUTPUT_FILES_DIR_PATH,
            category_processor=self.category_processor,
            faculty_postprocessor=self.faculty_postprocessor,
            department_postprocessor=self.department_postprocessor,
            warning_manager=self.warning_manager,
            strategy_factory=self.strategy_factory,
            utilities=self.utilities,
            dataclass_factory=self.dataclass_factory,
            extend=extend,
        )

    def _get_acf_func(self) -> Callable[[Dict[str, str]], ClassifierFactory]:
        """Get the abstract classifier factory function.

        Returns:
            Callable[[Dict[str, str]], ClassifierFactory]: A function that creates an AbstractClassifier
                given a dictionary of DOIs and abstracts.
        """
        return self._create_classifier_factory().abstract_classifier_factory

    def _validate_api_key(self, validator: APIKeyValidator, api_key: str) -> None:
        """Validate the provided API key.

        Args:
            validator (APIKeyValidator): Validator instance to check the API key.
            api_key (str): API key to validate.

        Raises:
            ValueError: If the API key is invalid.
        """
        if not validator.is_valid(api_key=api_key):
            raise ValueError(
                "Invalid API key. Please check your API key and try again."
            )

    def _make_files(self) -> None:
        """Create split files from input files for offline processing.

        Raises:
            Exception: If input directory contains no files to process.
        """
        if not os.listdir(INPUT_FILES_DIR_PATH):
            raise Exception(
                f"Input directory: {INPUT_FILES_DIR_PATH} contains no files to process."
            )

        files_to_split = [
            os.path.join(INPUT_FILES_DIR_PATH, file)
            for file in os.listdir(INPUT_FILES_DIR_PATH)
            if file.endswith(".json")
        ]

        for file_path in files_to_split:
            self.utilities.make_files(
                path_to_file=file_path,
                split_files_dir_path=SPLIT_FILES_DIR_PATH,
            )

    def _load_files(self) -> List[Dict[str, Any]]:
        """Load all split files into a list of dictionaries.

        Returns:
            List[Dict[str, Any]]: List of loaded data from split files.

        Notes:
            Warnings are logged for any files that fail to load.
        """
        data_list: List[Dict[str, Any]] = []
        for file_name in os.listdir(SPLIT_FILES_DIR_PATH):
            file_path: str = os.path.join(SPLIT_FILES_DIR_PATH, file_name)
            if not os.path.isfile(file_path):
                continue

            try:
                with open(file_path, "r") as file:
                    data: Dict[str, Any] = json.load(file)
                    data_list.append(data)
            except Exception as e:
                self.warning_manager.log_warning(
                    "File Loading", f"Error loading file: {file_path}. Error: {e}"
                )
                raise e

        return data_list

    def _create_dataclass_factory(self) -> DataClassFactory:
        """Create a new DataClassFactory for generating data classes.

        Returns:
            DataClassFactory: A new instance of the data class factory.
        """
        return DataClassFactory()

    def _create_crossref_wrapper(self, **kwargs) -> CrossrefWrapper:
        """Create a new CrossrefWrapper for interacting with the Crossref API.

        Args:
            **kwargs: Keyword arguments for CrossrefWrapper configuration.

        Returns:
            CrossrefWrapper: A configured CrossrefWrapper instance.
        """
        if "scraper" not in kwargs:
            kwargs["scraper"] = self.scraper if self.scraper else self._create_scraper()
        return CrossrefWrapper(**kwargs)

    def _create_category_processor(self) -> CategoryProcessor:
        """Create a new CategoryProcessor for processing publication categories.

        Returns:
            CategoryProcessor: A processor instance configured with utilities and factories.
        """
        return CategoryProcessor(
            utils=self.utilities,
            dataclass_factory=self.dataclass_factory,
            warning_manager=self.warning_manager,
            taxonomy_util=self.taxonomy,
        )

    def _create_minhash_util(self) -> MinHashUtility:
        """Create a new MinHashUtility instance for minhash operations.

        Returns:
            MinHashUtility: A new instance of the minhash utility.
        """
        return MinHashUtility(
            num_hashes=100,
        )

    def _create_faculty_postprocessor(
        self, minhash_util: MinHashUtility
    ) -> FacultyPostprocessor:
        """Create a new FacultyPostprocessor for processing faculty data.

        Returns:
            FacultyPostprocessor: A new instance of the faculty post-processor.
        """
        return FacultyPostprocessor(minhash_util=minhash_util)

    def _create_department_postprocessor(
        self, minhash_util: MinHashUtility
    ) -> DepartmentPostprocessor:
        """Create a new DepartmentPostprocessor for processing department data.

        Returns:
            DepartmentPostprocessor: A new instance of the department post-processor.
        """
        return DepartmentPostprocessor(minhash_util=minhash_util, threshold=0.5)

    def _create_scraper(self) -> Scraper:
        """Create a new Scraper instance for web scraping.

        Returns:
            Scraper: A scraper instance configured with the AI API key.
        """
        return Scraper(api_key=self.ai_api_key)

    def _create_db(self) -> DatabaseWrapper:
        """Create a new DatabaseWrapper for database operations.

        Returns:
            DatabaseWrapper: A database wrapper configured with connection details.
        """
        return DatabaseWrapper(db_name=self.db_name, mongo_uri=self.mongodb_uri)

    @staticmethod
    def _encode_affiliation(affiliation: str) -> str:
        """URL encode an affiliation string if it's not already encoded.

        Checks if the string is already properly URL-encoded by:
        1. Decoding it with unquote()
        2. Re-encoding it with quote()
        3. Comparing to original - if they match, it was already encoded

        Args:
            affiliation (str): Institution name to encode (e.g. "Salisbury University"
                or "Salisbury%20University")

        Returns:
            str: URL-encoded string (e.g. "Salisbury%20University")
        """
        return (
            affiliation
            if quote(unquote(affiliation)) == affiliation
            else quote(affiliation)
        )


def get_excel_report(db: DatabaseWrapper):
    """Save all data from database to Excel files.

    Args:
        db (DatabaseWrapper): The database wrapper to get data from.
    """
    articles, categories, faculty = db.get_all_data()

    import pandas as pd

    pd.DataFrame(articles).to_excel("article_data.xlsx", index=False)
    pd.DataFrame(categories).to_excel("category_data.xlsx", index=False)
    pd.DataFrame(faculty).to_excel("faculty_data.xlsx", index=False)


def main(
    openai_api_key_env_var_name: str | None = "OPENAI_API_KEY",
    mongodb_uri_env_var_name: str | None = "MONGODB_URI",
):
    if openai_api_key_env_var_name is None:
        raise ValueError("openai_api_key_env_var_name cannot be None")

    if mongodb_uri_env_var_name is None:
        raise ValueError("mongodb_uri_env_var_name cannot be None")

    import argparse
    from dotenv import load_dotenv

    load_dotenv()
    ai_api_key = os.getenv(openai_api_key_env_var_name)
    mongodb_uri = os.getenv(mongodb_uri_env_var_name)

    if ai_api_key is None:
        raise ValueError(
            f"\n\nError: {openai_api_key_env_var_name} environment variable not found."
            "\n\nPlease set the environment variable and try again."
            "\nIf you are unsure how to set an environment variable, or you do not have an OpenAI API key,"
            "\nplease refer to the README.md file for more information:"
            "\nhttps://github.com/spencerpresley/COSC425-DATA"
        )

    if mongodb_uri is None:
        raise ValueError(
            f"\n\nError: {mongodb_uri_env_var_name} environment variable not found."
            "\n\nPlease set the environment variable and try again."
            "\nIf you are unsure how to set an environment variable, or you do not have a MongoDB URI,"
            "\nplease refer to the README.md file for more information:"
            "\nhttps://github.com/spencerpresley/COSC425-DATA"
        )

    # Create argument parser
    parser = argparse.ArgumentParser(description="Run the academic metrics pipeline")

    parser.add_argument(
        "--test-run",
        action="store_true",
        help="Run in test mode using local MongoDB",
    )
    parser.add_argument(
        "--pre-classification-model",
        type=str,
        default="gpt-4o-mini",
        choices=["gpt-4o-mini", "gpt-4o"],
        help="Valid pre-classification-model's are 'gpt-4o-mini' or 'gpt-4o'",
    )
    parser.add_argument(
        "--classification-model",
        type=str,
        default="gpt-4o-mini",
        choices=["gpt-4o-mini", "gpt-4o"],
        help="Valid classification-model's are 'gpt-4o-mini' or 'gpt-4o'",
    )
    parser.add_argument(
        "--theme-model",
        type=str,
        default="gpt-4o-mini",
        choices=["gpt-4o-mini", "gpt-4o"],
        help="Valid theme-model's are 'gpt-4o-mini' or 'gpt-4o'",
    )

    parser.add_argument(
        "--from-year",
        type=int,
        default=2024,
        required=True,
        help="Starting year for data collection (e.g., 2019)",
    )

    parser.add_argument(
        "--to-year",
        type=int,
        default=2024,
        required=True,
        help="Ending year for data collection (e.g., 2024)",
    )

    parser.add_argument(
        "--from-month",
        type=int,
        default=1,
        choices=range(1, 13),
        help="Starting month (1-12, default: 1)",
    )

    parser.add_argument(
        "--to-month",
        type=int,
        default=12,
        choices=range(1, 13),
        help="Ending month (1-12, default: 12)",
    )

    parser.add_argument(
        "--as-excel",
        action="store_true",
        help="Save data to excel files. This is an additional action, it doesn't remove other data output types.",
    )

    parser.add_argument(
        "--db-name",
        type=str,
        default="Site_Data",
        help="Name of the database to use",
    )

    parser.add_argument(
        "--crossref-affiliation",
        type=str,
        required=True,
        help="The affiliation to use for the Crossref API",
    )

    args = parser.parse_args()

    # Configure logging
    logger = configure_logging(__name__, "main", log_level=logging.DEBUG)

    pre_classification_model = args.pre_classification_model
    classification_model = args.classification_model
    theme_model = args.theme_model

    if args.test_run:
        # Load local mongodb url
        logger.info("Running in test mode using local MongoDB...")
        mongodb_uri = os.getenv("LOCAL_MONGODB_URL")
        pipeline = PipelineRunner(
            ai_api_key=ai_api_key,
            crossref_affiliation="Salisbury University",
            data_from_year=2024,
            data_to_year=2024,
            mongodb_uri=mongodb_uri,
        )

        # Execute test run
        pipeline.test_run()
    else:
        # Normal pipeline execution
        logger.info(f"Running in production mode...")
        mongodb_uri = os.getenv(mongodb_uri_env_var_name)
        years = [str(year) for year in range(args.from_year, args.to_year + 1)]
        months = [str(month) for month in range(args.from_month, args.to_month + 1)]

        processed_dict = defaultdict(list)

        for year in years:
            for month in months:
                pipeline_runner = PipelineRunner(
                    ai_api_key=ai_api_key,
                    crossref_affiliation=args.crossref_affiliation,
                    data_from_month=int(month),
                    data_to_month=int(month),
                    data_from_year=int(year),
                    data_to_year=int(year),
                    mongodb_uri=mongodb_uri,
                    db_name=args.db_name,
                    pre_classification_model=pre_classification_model,
                    classification_model=classification_model,
                    theme_model=theme_model,
                )
                pipeline_runner.run_pipeline()
                processed_dict[year].append(month)
                logger.info(f"Processed year: {year}, month: {month}")

    logger.info(f"Processed data: {json.dumps(processed_dict, indent=4)}")

    if args.as_excel:
        logger.info("Creating Excel files...")
        db = DatabaseWrapper(db_name=args.db_name, mongo_uri=mongodb_uri)

        get_excel_report(db)

        logger.info("Excel files created successfully")


def command_line_runner(
    openai_api_key_env_var_name: str | None = "OPENAI_API_KEY",
    mongodb_uri_env_var_name: str | None = "MONGODB_URI",
):
    main(openai_api_key_env_var_name, mongodb_uri_env_var_name)


if __name__ == "__main__":
    command_line_runner(mongodb_uri_env_var_name="LOCAL_MONGODB_URL")

            ```

            src/academic_metrics/strategies/AttributeExtractionStrategies.py:
            ```
from __future__ import annotations

import json
import logging
import os
import re
import uuid
from abc import ABC, abstractmethod
from html import unescape
from typing import Any, Dict, List

from bs4 import BeautifulSoup

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)
from academic_metrics.enums import AttributeTypes
from academic_metrics.utils import WarningManager

from academic_metrics.factories import StrategyFactory


class AttributeExtractionStrategy(ABC):
    """
    Abstract base class for attribute extraction strategies.

    This class provides a framework for extracting various attributes from academic publication data.
    It defines common methods and properties that all specific extraction strategies should implement or utilize.

    It implements the Strategy pattern, allowing for flexible implementation of different extraction methods for different types of attributes or data sources. See more on the Strategy pattern here: https://en.wikipedia.org/wiki/Strategy_pattern

    Attributes:
        logger (logging.Logger): Logger for recording extraction-related events.
        abstract_pattern (re.Pattern): Regular expression pattern for extracting abstracts.
        missing_abstracts_file (str): File path for storing information about missing abstracts.
        warning_manager (WarningManager): Manages and logs warnings during extraction.
        unknown_authors_dict (dict): Stores information about unidentified authors.
        unknown_authors_file (str): File path for storing information about unknown authors.
        crossref_author_key (str): Key used to access author information in Crossref data.

    Methods:
        extract_attribute: Abstract method to be implemented by subclasses for specific attribute extraction.
        html_to_markdown: Converts HTML content to Markdown format.
        get_crossref_author_affils: Retrieves author affiliations from Crossref data.
        get_author_obj: Extracts author information from Crossref JSON data.
        set_author_sequence_dict: Organizes author information into a structured dictionary.
        write_missing_authors_file: Writes information about unknown authors to a file.
        create_author_sequence_dict: Creates a template dictionary for author sequence information.
        create_unknown_authors_dict: Creates a template dictionary for unknown authors.
        log_extraction_warning: Logs warnings encountered during attribute extraction.
        generate_error_id: Generates a unique identifier for error tracking.
        get_authors_as_list: Converts the author sequence dictionary to a list of author names.

    Design:
        This class is designed as an abstract base class, providing a common interface and shared functionality
        for various attribute extraction strategies. It uses the Strategy pattern, allowing for flexible
        implementation of different extraction methods for different types of attributes or data sources.

    Summary:
        The AttributeExtractionStrategy class serves as a foundation for creating specific attribute extraction
        strategies in an academic publication data processing system. It provides utility methods for handling
        common tasks such as HTML parsing, author information processing, and error logging, while defining
        an interface for implementing specific extraction logic in subclasses.
    """

    def __init__(
        self,
        warning_manager: WarningManager,
        missing_abstracts_file="missing_abstracts.txt",
    ):
        """
        Initializes the AttributeExtractionStrategy with necessary components and configurations.

        This constructor sets up the basic infrastructure needed for attribute extraction,
        including logging, file paths for storing missing data, and utilities for managing
        warnings and unknown author information.

        Args:
            warning_manager (WarningManager): An instance of WarningManager for handling and logging warnings
                                              encountered during the extraction process.
            missing_abstracts_file (str, optional): The file path where information about missing abstracts
                                                    will be stored. Defaults to "missing_abstracts.txt".

        Returns:
            None

        Design:
            The constructor initializes various attributes of the class, setting up the logger,
            compiling regular expressions, and preparing data structures for handling unknown
            authors and missing abstracts. It's designed to provide a consistent starting point
            for all subclasses of AttributeExtractionStrategy.

        Summary:
            This method prepares an instance of AttributeExtractionStrategy (or its subclass)
            for operation by setting up necessary tools and configurations for attribute extraction.
            It ensures that each strategy has access to logging, warning management, and file storage
            for handling edge cases and errors in the extraction process.
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="attribute_extraction_strategies",
            log_level=DEBUG,
        )

        self.abstract_pattern: re.Pattern = re.compile(r"AB\s(.+?)(?=\nC1)", re.DOTALL)
        self.missing_abstracts_file: str = missing_abstracts_file
        self.warning_manager: WarningManager = warning_manager
        self.unknown_authors_dict: dict = self.create_unknown_authors_dict()
        self.unknown_authors_file: str = "crossref_unknown_authors.json"
        self.crossref_author_key: str = "author"

    @abstractmethod
    def extract_attribute(self, entry_text: str) -> Any:
        """
        Abstract method to extract a specific attribute from the entry text.

        This method should be implemented by subclasses to define the specific
        extraction logic for each attribute type.

        Args:
            entry_text: The text or data from which to extract the attribute.
            entry_text applies to both WoS and Crossref data.

        Returns:
            The extracted attribute value.

        Raises:
            NotImplementedError: If the method is not implemented in a subclass.

        Design:
            This abstract method enforces a common interface for all attribute
            extraction strategies, allowing for polymorphic use of different
            strategies.

        Summary:
            Defines the contract for attribute extraction methods in subclasses.
        """
        raise NotImplementedError("This method must be implemented in a subclass")

    def html_to_markdown(self, html_content: str) -> str:
        """
        Converts HTML content to Markdown format.

        This method takes HTML content, particularly JATS XML, and converts it
        to a simplified Markdown format.

        Args:
            html_content (str): The HTML content to be converted.

        Returns:
            str: The converted content in Markdown format.

        Design:
            Uses BeautifulSoup for parsing HTML and custom logic to generate Markdown.
            Handles both sectioned and non-sectioned content.

        Summary:
            Transforms complex HTML/XML content into more readable Markdown format.
        """
        # Use BeautifulSoup to parse the HTML content
        soup: BeautifulSoup = BeautifulSoup(html_content, "lxml")

        markdown_content: list[str] = []

        # Check if there are any <jats:sec> sections
        sections: list[BeautifulSoup] = soup.find_all(["jats:sec", "section"])
        if sections:
            for section in sections:
                title = section.find("jats:title")
                if title:
                    string: str = f"## {title.get_text(strip=True)}:"
                    if title.get_text(strip=True).endswith(":"):
                        string = string[:-1]
                    markdown_content.append(string)

                paragraphs: list[BeautifulSoup] = section.find_all(["jats:p", "p"])
                for paragraph in paragraphs:
                    markdown_content.append(paragraph.get_text(strip=True) + "\n")
        else:
            # If no sections, combine all paragraphs
            paragraphs: list[BeautifulSoup] = soup.find_all(["jats:p", "p"])
            for paragraph in paragraphs:
                markdown_content.append(paragraph.get_text(strip=True) + "\n")

        return "\n".join(markdown_content)

    def get_crossref_author_affils(self, author_item: dict) -> list[str]:
        """
        Retrieves author affiliations from Crossref data.

        This method extracts the affiliation information for an author
        from the Crossref author data structure.

        It is designed to be used with Crossref data, not WoS data. It is NOT compatible with WoS data.

        This is implemented in the base class as it is used by more than a single subclass such as CrossrefAuthorExtractionStrategy and CrossrefDepartmentExtractionStrategy.

        Args:
            author_item (dict): A dictionary containing author information from Crossref.

        Returns:
            list[str]: A list of affiliation names for the author.

        Design:
            Directly accesses the 'affiliation' key in the author dictionary.
            Extracts the 'name' field from each affiliation entry.

        Summary:
            Extracts and returns a list of affiliation names for a given author.
        """
        raw_affils: list[str] = author_item["affiliation"]
        affils: list[str] = []
        for affil in raw_affils:
            affils.append(affil["name"])
        return affils

    def get_author_obj(self, *, crossref_json: dict) -> list[dict]:
        """
        Extracts the author object from Crossref JSON data.

        This method retrieves the author information from the Crossref JSON structure.
        It is designed to be used with Crossref data, not WoS data.

        Args:
            crossref_json (dict): The Crossref JSON data containing author information.

        Returns:
            list[dict]: A list of dictionaries, each containing information about an author.

        Design:
            Uses the class attribute 'crossref_author_key' to access author information.

        Summary:
            Extracts and returns the author object from Crossref JSON data.
        """
        authors: list[dict] = crossref_json.get(self.crossref_author_key, None)
        return authors

    def write_missing_authors_file(self, unknown_authors: dict) -> None:
        """
        Writes information about unknown authors to a file.

        This method saves the information about authors that couldn't be properly
        processed to a JSON file for later analysis.

        Args:
            unknown_authors (dict): A dictionary containing information about unknown authors.
            unknown_authors_file (str): The file path where the information will be saved.

        Returns:
            None

        Design:
            Uses JSON format to store the unknown authors information.

        Summary:
            Saves information about unknown or problematic authors to a file.
        """
        with open(self.missing_authors_file, "w") as unknown_authors_file:
            json.dump(unknown_authors, unknown_authors_file, indent=4)

    def create_author_sequence_dict(self) -> dict:
        """
        Creates a template dictionary for author sequence information.

        This method initializes a dictionary structure to store information
        about the first author and additional authors.

        Returns:
            dict: A dictionary with keys for 'first' author and 'additional' authors.

        Design:
            Provides a consistent structure for storing author information.

        Summary:
            Creates and returns a template dictionary for organizing author information.
        """
        return {"first": {"author_name": "", "affiliations": []}, "additional": []}

    def create_unknown_authors_dict(self) -> dict:
        """
        Creates a template dictionary for unknown authors.

        This method initializes a dictionary to store information about
        authors that couldn't be properly processed.

        Returns:
            dict: A dictionary with a key for 'unknown_authors'.

        Design:
            Provides a consistent structure for storing information about problematic authors.

        Summary:
            Creates and returns a template dictionary for tracking unknown authors.
        """
        return {"unknown_authors": []}

    def log_extraction_warning(
        self,
        attribute_class_name: str,
        warning_message: str,
        entry_id: str = None,
        line_prefix: str = None,
    ):
        """
        Logs warnings encountered during attribute extraction.

        This method creates a standardized log message for extraction warnings
        and can optionally include specific entry information.

        Args:
            attribute_class_name (str): The name of the attribute class where the warning occurred.
            warning_message (str): The specific warning message.
            entry_id (str, optional): An identifier for the entry causing the warning.
            line_prefix (str, optional): A prefix to identify specific lines in the entry.

        Returns:
            None

        Design:
            Generates a unique error ID for each warning.
            Commented-out code shows potential for more detailed logging.

        Summary:
            Creates and logs a standardized warning message for attribute extraction issues.
        """
        log_message = f"Failed to extract {attribute_class_name}. Error ID: {self.generate_error_id()}"
        self.logger.warning(log_message)
        self.warning_manager.log_warning(attribute_class_name, log_message, entry_id)

        # * Commented out code is potential for more detailed logging.
        # if type(entry_id) == str:
        #     for line in entry_id.splitlines():
        #         if line.startswith(line_prefix):
        #             log_message += f" - Line: {line}"
        # else:
        #     log_message += f" - Entry ID: {entry_id[:25]}"
        # log_message += f" - {warning_message}"
        # self.logger.warning(log_message)
        # self.warning_manager.log_warning(attribute_class_name, log_message, entry_id)

    def generate_error_id(self) -> str:
        """
        Generates a unique identifier for error tracking.

        This method creates a UUID to uniquely identify each error or warning instance.

        Returns:
            str: A unique UUID string.

        Design:
            Uses Python's uuid module to generate a version 4 UUID.

        Summary:
            Generates and returns a unique identifier string for error tracking purposes.
        """
        return str(uuid.uuid4())

    def write_missing_authors_file(
        self, unknown_authors: dict, unknown_authors_file: str
    ) -> None:
        """
        Converts the author sequence dictionary to a list of author names.

        This method extracts author names from the structured author sequence dictionary
        and returns them as a simple list.

        Args:
            author_sequence_dict (dict): A dictionary containing structured author information.

        Returns:
            list[str]: A list of author names in the order they appear in the publication.

        Design:
            Handles both the first author and additional authors from the dictionary.

        Summary:
            Extracts and returns a list of author names from the structured author dictionary.
        """
        with open(unknown_authors_file, "w") as unknown_authors_file:
            json.dump(unknown_authors, unknown_authors_file, indent=4)

    def set_author_sequence_dict(
        self, *, author_items: list[dict], author_sequence_dict: dict
    ) -> None:
        """
        Organizes author information into a structured dictionary.

        This method processes a list of author items and organizes them into a dictionary
        based on their sequence (first author or additional authors).
        It is designed to work with Crossref data, not WoS data.

        Args:
            author_items (list[dict]): A list of dictionaries containing author information.
            author_sequence_dict (dict): A dictionary to store the organized author information.

        Returns:
            None

        Design:
            Processes each author item, extracting name and affiliation information.
            Handles cases for first author and additional authors separately.
            Logs warnings for missing or incomplete author information.

        Summary:
            Organizes author information into a structured dictionary format.
        """
        for author_item in author_items:
            sequence: str = author_item.get("sequence", "")
            author_given_name: str = author_item.get("given", "")
            author_family_name: str = author_item.get("family", "")

            author_name: str = ""
            if author_given_name and author_family_name:
                author_name = f"{author_given_name} {author_family_name}"
            else:
                self.log_extraction_warning(
                    attribute_class_name=self.__class__.__name__,
                    warning_message="Attribute: 'Crossref_Author' was not found in the entry",
                    entry_id=author_item,
                )
                self.unknown_authors_dict["unknown_authors"].append(author_item)
                continue

            author_affiliations: list[str] = self.get_crossref_author_affils(
                author_item
            )

            if not sequence:
                self.log_extraction_warning(
                    attribute_class_name=self.__class__.__name__,
                    warning_message="Attribute: 'Crossref_Author' was not found in the entry",
                    entry_id=author_item,
                )
                self.unknown_authors_dict["unknown_authors"].append(
                    f"Error ID: {self.generate_error_id()} - {author_item}"
                )
                continue

            if sequence == "first":
                author_sequence_dict[sequence]["author_name"] = author_name
                for affiliation in author_affiliations:
                    author_sequence_dict[sequence]["affiliations"].append(affiliation)

            elif sequence == "additional":
                additional_author_dict: dict = {}
                additional_author_dict["author_name"] = author_name
                additional_author_dict["affiliations"] = []
                for affiliation in author_affiliations:
                    additional_author_dict["affiliations"].append(affiliation)
                author_sequence_dict["additional"].append(additional_author_dict)

        self.write_missing_authors_file(
            self.unknown_authors_dict, self.unknown_authors_file
        )

    def get_authors_as_list(self, *, author_sequence_dict: dict) -> list[str]:
        """
        Converts the author sequence dictionary to a list of author names.

        This method extracts author names from the structured author sequence dictionary
        and returns them as a simple list.

        Args:
            author_sequence_dict (dict): A dictionary containing structured author information.

        Returns:
            list[str]: A list of author names in the order they appear in the publication.

        Design:
            Handles both the first author and additional authors from the dictionary.

        Summary:
            Extracts and returns a list of author names from the structured author dictionary.
        """
        authors: list[str] = []
        authors.append(author_sequence_dict["first"]["author_name"])
        for item in author_sequence_dict["additional"]:
            authors.append(item["author_name"])
        return authors


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_TITLE)
class CrossrefTitleExtractionStrategy(AttributeExtractionStrategy):
    """
    A strategy for extracting title information from Crossref data.

    This class implements the AttributeExtractionStrategy for title extraction
    specifically from Crossref JSON data. It focuses on extracting and cleaning
    the title(s) of a publication.

    Attributes:
        title_key (str): The key used to access title information in the Crossref JSON.

    Methods:
        clean_title: Removes HTML tags from a title string.
        extract_attribute: Extracts and cleans the title(s) from the Crossref entry.

    Design:
        Utilizes BeautifulSoup for HTML tag removal and handles potential multiple titles.
        Implements the Strategy pattern for title extraction from Crossref data.

    Summary:
        Provides a specialized strategy for extracting and cleaning publication titles
        from Crossref data entries, handling potential HTML content and multiple titles.
    """

    def __init__(self, warning_manager: WarningManager):
        """
        Initializes the CrossrefTitleExtractionStrategy.

        This constructor sets up the strategy with a warning manager and defines
        the key for accessing title information in Crossref data.

        Args:
            warning_manager (WarningManager): An instance of WarningManager for handling extraction warnings.

        Returns:
            None

        Design:
            Calls the superclass constructor and sets up the title key for Crossref data.

        Summary:
            Prepares the strategy instance for title extraction from Crossref data.
        """
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_title_extraction_strategy",
            log_level=DEBUG,
        )
        self.title_key: str = "title"

    def clean_title(self, title: str) -> str:
        """
        Removes HTML tags from a title string using BeautifulSoup.

        This method uses BeautifulSoup to parse and remove any HTML tags
        present in the title string.

        Args:
            title (str): The title string potentially containing HTML tags.

        Returns:
            str: The cleaned title string with HTML tags removed.

        Design:
            Uses BeautifulSoup with 'html.parser' to safely remove HTML tags.

        Summary:
            Cleans a title string by removing any HTML tags it may contain.
        """
        soup: BeautifulSoup = BeautifulSoup(title, "html.parser")
        return soup.get_text()

    def extract_attribute(self, entry_text: dict) -> tuple[bool, list[str]]:
        """
        Extracts and cleans the title(s) from the Crossref entry.

        This method retrieves the title(s) from the Crossref JSON data,
        handles potential multiple titles, and cleans each title by removing HTML tags.

        Args:
            entry_text (dict): The Crossref JSON data containing the publication information.

        Returns:
            tuple[bool, list[str]]: A tuple containing:
                - A boolean indicating success (True) or failure (False) of the extraction.
                - A list of cleaned title strings, or None if no titles were found.

        Design:
            Retrieves titles using the predefined title_key.
            Handles cases where a single title or multiple titles may be present.
            Cleans each title using the clean_title method.
            Logs a warning if no titles are found.

        Summary:
            Extracts, cleans, and returns the publication title(s) from Crossref JSON data.
        """
        titles: list[str] = entry_text.get(self.title_key, [])
        if not isinstance(titles, list):
            titles = [titles]
        cleaned_titles: list[str] = [self.clean_title(title) for title in titles]
        if cleaned_titles:
            return (True, cleaned_titles)
        else:
            self.log_extraction_warning(
                attribute_class_name=self.__class__.__name__,
                warning_message="Attribute: 'Crossref_Title' was not found in the entry",
                entry_id=entry_text,
            )
            return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_ABSTRACT)
class CrossrefAbstractExtractionStrategy(AttributeExtractionStrategy):
    """
    A strategy for extracting abstract information from Crossref data.

    This class implements the AttributeExtractionStrategy for abstract extraction
    specifically from Crossref JSON data. It focuses on extracting and cleaning
    the abstract of a publication.

    Attributes:
        abstract_key (str): The key used to access abstract information in the Crossref JSON.

    Methods:
        clean_abstract: Converts HTML content in the abstract to Markdown format.
        extract_attribute: Extracts and cleans the abstract from the Crossref entry.

    Design:
        Utilizes the html_to_markdown method for cleaning HTML content in abstracts.
        Implements the Strategy pattern for abstract extraction from Crossref data.

    Summary:
        Provides a specialized strategy for extracting and cleaning publication abstracts
        from Crossref data entries, handling potential HTML content.
    """

    def __init__(self, warning_manager: WarningManager):
        """
        Initializes the CrossrefAbstractExtractionStrategy.

        This constructor sets up the strategy with a warning manager and defines
        the key for accessing abstract information in Crossref data.

        Args:
            warning_manager (WarningManager): An instance of WarningManager for handling extraction warnings.

        Returns:
            None

        Design:
            Calls the superclass constructor and sets up the abstract key for Crossref data.

        Summary:
            Prepares the strategy instance for abstract extraction from Crossref data.
        """
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_abstract_extraction_strategy",
            log_level=DEBUG,
        )
        self.abstract_key: str = "abstract"

    def clean_abstract(self, abstract: str) -> str:
        """
        Cleans the abstract by converting HTML content to Markdown format.

        This method uses the html_to_markdown method to convert any HTML content
        in the abstract to a more readable Markdown format.

        Args:
            abstract (str): The abstract string potentially containing HTML content.

        Returns:
            str: The cleaned abstract string in Markdown format.

        Design:
            Utilizes the html_to_markdown method inherited from the parent class.

        Summary:
            Converts HTML content in the abstract to Markdown for improved readability.
        """
        return self.html_to_markdown(abstract)

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        """
        Extracts and cleans the abstract from the Crossref entry.

        This method retrieves the abstract from the Crossref JSON data,
        cleans it by converting HTML to Markdown, and returns the result.

        Args:
            entry_text (dict): The Crossref JSON data containing the publication information.

        Returns:
            tuple[bool, str]: A tuple containing:
                - A boolean indicating success (True) or failure (False) of the extraction.
                - The cleaned abstract string, or None if no abstract was found.

        Design:
            Retrieves the abstract using the predefined abstract_key.
            Cleans the abstract using the clean_abstract method.
            Logs a warning if no abstract is found.

        Summary:
            Extracts, cleans, and returns the publication abstract from Crossref JSON data.
        """
        abstract: str = entry_text.get(self.abstract_key, None)
        if abstract:
            abstract = self.clean_abstract(abstract)
            return (True, abstract)
        else:
            self.log_extraction_warning(
                attribute_class_name=self.__class__.__name__,
                warning_message="Attribute: 'Crossref_Abstract' was not found in the entry",
                entry_id=entry_text,
            )
            return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_AUTHORS)
class CrossrefAuthorExtractionStrategy(AttributeExtractionStrategy):
    """
    A strategy for extracting author information from Crossref data.

    This class implements the AttributeExtractionStrategy for author extraction
    specifically from Crossref JSON data. It focuses on extracting and organizing
    author names and handling cases where author information might be incomplete.

    Attributes:
        unknown_authors (dict): A dictionary to store information about authors with incomplete data.
        missing_authors_file (str): The file path to store information about unknown authors.

    Methods:
        get_author_name: Constructs a full author name from given and family name components.
        extract_attribute: Extracts and organizes author information from the Crossref entry.

    Design:
        Utilizes helper methods to process individual author items and organize them.
        Implements the Strategy pattern for author extraction from Crossref data.

    Summary:
        Provides a specialized strategy for extracting and organizing author information
        from Crossref data entries, handling potential incomplete author data.
    """

    def __init__(self, warning_manager: WarningManager):
        """
        Initializes the CrossrefAuthorExtractionStrategy.

        This constructor sets up the strategy with a warning manager and initializes
        structures for handling unknown authors.

        Args:
            warning_manager (WarningManager): An instance of WarningManager for handling extraction warnings.

        Returns:
            None

        Design:
            Calls the superclass constructor and sets up data structures for unknown authors.

        Summary:
            Prepares the strategy instance for author extraction from Crossref data.
        """
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_author_extraction_strategy",
            log_level=DEBUG,
        )
        self.unknown_authors: dict = self.create_unknown_authors_dict()
        self.missing_authors_file: str = "unknown_authors.json"

    def get_author_name(self, author_item: dict) -> str:
        """
        Constructs a full author name from given and family name components.

        This method attempts to create a full author name from the given and family
        name fields in the author item. If either component is missing, it logs a warning.

        Args:
            author_item (dict): A dictionary containing author information from Crossref.

        Returns:
            str: The full author name if both components are present, None otherwise.

        Design:
            Extracts given and family names from the author item.
            Logs a warning if either component is missing.

        Summary:
            Constructs and returns a full author name, or None if information is incomplete.
        """
        given_name: str = author_item.get("given", "")
        family_name: str = author_item.get("family", "")
        if given_name and family_name:
            return f"{given_name} {family_name}"
        else:
            self.log_extraction_warning(
                attribute_class_name=self.__class__.__name__,
                warning_message="Attribute: 'Crossref_Author' was not found in the entry",
                entry_id=author_item,
            )
            return None

    def extract_attribute(self, crossref_json: dict) -> tuple[bool, list[str]]:
        """
        Extracts and organizes author information from the Crossref entry.

        This method processes the Crossref JSON data to extract author information,
        organizes it into a structured format, and returns a list of author names.

        Args:
            crossref_json (dict): The Crossref JSON data containing the publication information.

        Returns:
            tuple[bool, list[str]]: A tuple containing:
                - A boolean indicating success (True) or failure (False) of the extraction.
                - A list of author names extracted from the Crossref data.

        Design:
            Uses helper methods to extract author objects and organize them into a sequence.
            Converts the organized author data into a simple list of names.

        Summary:
            Extracts, organizes, and returns a list of author names from Crossref JSON data.
        """
        author_items: list[dict] = self.get_author_obj(crossref_json=crossref_json)
        author_sequence_dict: dict = self.create_author_sequence_dict()
        self.set_author_sequence_dict(
            author_items=author_items,
            author_sequence_dict=author_sequence_dict,
        )
        return (
            True,
            self.get_authors_as_list(author_sequence_dict=author_sequence_dict),
        )


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_DEPARTMENTS)
class CrossrefDepartmentExtractionStrategy(AttributeExtractionStrategy):
    """
    A strategy for extracting department information from Crossref data.

    This class implements the AttributeExtractionStrategy for department extraction
    specifically from Crossref JSON data. It focuses on extracting and organizing
    department affiliations for each author in a publication.

    Methods:
        extract_attribute: Extracts and organizes department affiliations from the Crossref entry.

    Design:
        Utilizes helper methods to process author information and extract department affiliations.
        Implements the Strategy pattern for department extraction from Crossref data.

    Summary:
        Provides a specialized strategy for extracting and organizing department affiliations
        for authors from Crossref data entries.
    """

    def __init__(self, warning_manager: WarningManager):
        """
        Initializes the CrossrefDepartmentExtractionStrategy.

        This constructor sets up the strategy with a warning manager.

        Args:
            warning_manager (WarningManager): An instance of WarningManager for handling extraction warnings.

        Returns:
            None

        Design:
            Calls the superclass constructor to set up the warning manager.

        Summary:
            Prepares the strategy instance for department extraction from Crossref data.
        """
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_department_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, crossref_json: dict) -> tuple[bool, list[str]]:
        """
        Extracts and organizes department affiliations from the Crossref entry.

        This method processes the Crossref JSON data to extract department affiliations
        for each author, organizing them into a dictionary structure.

        Args:
            crossref_json (dict): The Crossref JSON data containing the publication information.

        Returns:
            tuple[bool, dict[str, list[str]]]: A tuple containing:
                - A boolean indicating success (True) or failure (False) of the extraction.
                - A dictionary where keys are author names and values are lists of their affiliations.

        Design:
            Uses helper methods to extract author objects and organize them into a sequence.
            Processes both the first author and additional authors separately.
            Creates a dictionary mapping author names to their department affiliations.
            Logs a warning if no department affiliations are found.

        Summary:
            Extracts and returns a dictionary of author names mapped to their department affiliations
            from Crossref JSON data.
        """
        author_items: list[dict] = self.get_author_obj(crossref_json=crossref_json)
        sequence_dict: dict = self.create_author_sequence_dict()
        self.set_author_sequence_dict(
            author_items=author_items,
            author_sequence_dict=sequence_dict,
        )

        # keys are authors, values are their affiliation
        department_affiliations: dict[str, str] = {}
        first_author = sequence_dict.get("first", "")
        if first_author:
            department_affiliations[first_author.get("author_name", "Unknown")] = (
                first_author.get("affiliations", [])
            )

        additional_authors = sequence_dict.get("additional", [])
        if additional_authors:
            for author in additional_authors:
                department_affiliations[author.get("author_name", "")] = author.get(
                    "affiliations", []
                )
        if department_affiliations:
            return (True, department_affiliations)
        else:
            self.log_extraction_warning(
                attribute_class_name=self.__class__.__name__,
                warning_message="Attribute: 'Crossref_Department' was not found in the entry",
                entry_id=crossref_json,
            )
            return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_CATEGORIES)
class CrossrefCategoriesExtractionStrategy(AttributeExtractionStrategy):
    """
    A strategy for extracting category information from Crossref data.

    This class implements the AttributeExtractionStrategy for category extraction
    specifically from Crossref JSON data. It focuses on retrieving the categories
    associated with a publication.

    Methods:
        extract_attribute: Extracts the categories from the Crossref entry.

    Design:
        Implements the Strategy pattern for category extraction from Crossref data.
        Uses a simple dictionary lookup to retrieve category information.

    Summary:
        Provides a specialized strategy for extracting publication categories
        from Crossref data entries.
    """

    def __init__(self, warning_manager: WarningManager):
        """
        Initializes the CrossrefCategoriesExtractionStrategy.

        This constructor sets up the strategy with a warning manager.

        Args:
            warning_manager (WarningManager): An instance of WarningManager for handling extraction warnings.

        Returns:
            None

        Design:
            Calls the superclass constructor to set up the warning manager.

        Summary:
            Prepares the strategy instance for category extraction from Crossref data.
        """
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_categories_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, crossref_json: dict) -> tuple[bool, list[str]]:
        """
        Extracts the categories from the Crossref entry.

        This method retrieves the categories associated with a publication
        from the Crossref JSON data.

        Args:
            crossref_json (dict): The Crossref JSON data containing the publication information.

        Returns:
            tuple[bool, list[str]]: A tuple containing:
                - A boolean indicating success (True) if categories are found, False otherwise.
                - A list of category strings, or None if no categories are found.

        Design:
            Uses a simple dictionary get method to retrieve the categories.
            Returns True only if categories are present.

        Summary:
            Extracts and returns the categories associated with a publication from Crossref JSON data.
        """
        top_level_categories = crossref_json.get("categories", {}).get("top", [])
        mid_level_categories = crossref_json.get("categories", {}).get("mid", [])
        low_level_categories = crossref_json.get("categories", {}).get("low", [])
        categories = {
            "top": top_level_categories,
            "mid": mid_level_categories,
            "low": low_level_categories,
        }

        return (True if categories else False, categories)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_CITATION_COUNT)
class CrossrefCitationCountExtractionStrategy(AttributeExtractionStrategy):
    """
    A strategy for extracting citation count information from Crossref data.

    This class implements the AttributeExtractionStrategy for citation count extraction
    specifically from Crossref JSON data. It focuses on retrieving the number of times
    a publication has been cited.

    Methods:
        extract_attribute: Extracts the citation count from the Crossref entry.

    Design:
        Implements the Strategy pattern for citation count extraction from Crossref data.
        Uses a simple dictionary lookup to retrieve citation count information.

    Summary:
        Provides a specialized strategy for extracting publication citation counts
        from Crossref data entries.
    """

    def __init__(self, warning_manager: WarningManager):
        """
        Initializes the CrossrefCitationCountExtractionStrategy.

        This constructor sets up the strategy with a warning manager.

        Args:
            warning_manager (WarningManager): An instance of WarningManager for handling extraction warnings.

        Returns:
            None

        Design:
            Calls the superclass constructor to set up the warning manager.

        Summary:
            Prepares the strategy instance for citation count extraction from Crossref data.
        """
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_citation_count_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, crossref_json: dict) -> tuple[bool, int]:
        """
        Extracts the citation count from the Crossref entry.

        This method retrieves the number of times a publication has been cited
        from the Crossref JSON data.

        Args:
            crossref_json (dict): The Crossref JSON data containing the publication information.

        Returns:
            tuple[bool, int]: A tuple containing:
                - A boolean always set to True (as the method always returns a count, even if it's 0).
                - An integer representing the citation count.

        Design:
            Uses a dictionary get method to retrieve the citation count, defaulting to 0 if not found.
            Always returns True as the first element of the tuple, as a count is always available.

        Summary:
            Extracts and returns the citation count for a publication from Crossref JSON data.
        """
        citation_count = crossref_json.get("is-referenced-by-count", 0)
        return (True, citation_count)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_LICENSE_URL)
class CrossrefLicenseURLExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_license_url_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        license_info = entry_text.get("license", [])
        if license_info and isinstance(license_info, list):
            url = license_info[0].get("URL")
            if url:
                return (True, url)
        self.log_extraction_warning(
            attribute_class_name=self.__class__.__name__,
            warning_message="Attribute: 'Crossref_License_URL' was not found in the entry",
            entry_id=entry_text,
        )
        return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_PUBLISHED_PRINT)
class CrossrefPublishedPrintExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_published_print_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        published_print = entry_text.get("published-print", {}).get("date-parts", [[]])[
            0
        ]
        if published_print:
            date_str = "-".join(map(str, published_print))
            return (True, date_str)
        self.log_extraction_warning(
            attribute_class_name=self.__class__.__name__,
            warning_message="Attribute: 'Crossref_Published_Print' was not found in the entry",
            entry_id=entry_text,
        )
        return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_CREATED_DATE)
class CrossrefCreatedDateExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_created_date_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        created = entry_text.get("created", {}).get("date-time")
        if created:
            return (True, created)
        self.log_extraction_warning(
            attribute_class_name=self.__class__.__name__,
            warning_message="Attribute: 'Crossref_Created_Date' was not found in the entry",
            entry_id=entry_text,
        )
        return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_PUBLISHED_ONLINE)
class CrossrefPublishedOnlineExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_published_online_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        published_online = entry_text.get("published-online", {}).get(
            "date-parts", [[]]
        )[0]
        if published_online:
            date_str = "-".join(map(str, published_online))
            return (True, date_str)
        self.log_extraction_warning(
            attribute_class_name=self.__class__.__name__,
            warning_message="Attribute: 'Crossref_Published_Online' was not found in the entry",
            entry_id=entry_text,
        )
        return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_JOURNAL)
class CrossrefJournalExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_journal_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        journal = entry_text.get("container-title", [])
        if journal and isinstance(journal, list):
            return (True, journal[0])
        self.log_extraction_warning(
            attribute_class_name=self.__class__.__name__,
            warning_message="Attribute: 'Crossref_Journal' was not found in the entry",
            entry_id=entry_text,
        )
        return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_URL)
class CrossrefURLExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_url_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        url = entry_text.get("URL")
        if url:
            return (True, url)
        self.log_extraction_warning(
            attribute_class_name=self.__class__.__name__,
            warning_message="Attribute: 'Crossref_URL' was not found in the entry",
            entry_id=entry_text,
        )
        return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_DOI)
class CrossrefDOIExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_doi_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        doi = entry_text.get("DOI")
        if doi:
            return (True, doi)
        self.log_extraction_warning(
            attribute_class_name=self.__class__.__name__,
            warning_message="Attribute: 'Crossref_DOI' was not found in the entry",
            entry_id=entry_text,
        )
        return (False, None)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_THEMES)
class CrossrefThemesExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_themes_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, list[str]]:
        themes = entry_text.get("themes", [])
        return (True, themes)


@StrategyFactory.register_strategy(AttributeTypes.CROSSREF_EXTRA_CONTEXT)
class CrossrefExtraContextExtractionStrategy(AttributeExtractionStrategy):
    def __init__(self, warning_manager: WarningManager):
        super().__init__(warning_manager=warning_manager)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="crossref_extra_context_extraction_strategy",
            log_level=DEBUG,
        )

    def extract_attribute(self, entry_text: dict) -> tuple[bool, str]:
        extra_context = entry_text.get("extra_context", None)
        if extra_context is not None:
            return (True, extra_context)
        self.log_extraction_warning(
            attribute_class_name=self.__class__.__name__,
            warning_message="Attribute: 'Crossref_Extra_Context' was not found in the entry",
            entry_id=entry_text,
        )
        return (False, None)

            ```

            src/academic_metrics/strategies/__init__.py:
            ```
from .AttributeExtractionStrategies import AttributeExtractionStrategy

            ```

            src/academic_metrics/utils/__init__.py:
            ```
from .api_key_validator import APIKeyValidator
from .taxonomy_util import Taxonomy
from .utilities import Utilities
from .warning_manager import WarningManager
from .minhash_util import MinHashUtility

            ```

            src/academic_metrics/utils/api_key_validator.py:
            ```
import logging
import os
from dataclasses import dataclass
from typing import TYPE_CHECKING, Dict, Optional

if TYPE_CHECKING:
    from langchain.schema.runnable import Runnable

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)


@dataclass
class ValidationResult:
    openai: bool = False
    anthropic: bool = False
    google: bool = False


class APIKeyValidator:
    """
    Validator for LLM API keys across different services.

    Example:
        >>> validator = APIKeyValidator(api_key="sk-...")
        >>> if validator.is_valid():
        >>>     print("Key is valid!")
        >>>     validator.print_results()  # See which services work
    """

    def __init__(self):
        # Dict to track api keys which have been validated already
        # Key = api_key, Value = bool (True if valid, False if not)
        self._validated_already: Dict[str, bool] = {}
        # self.log_file_path: str = os.path.join(LOG_DIR_PATH, "api_key_validator.log")
        # self.logger: logging.Logger = logging.getLogger(__name__)
        # self.logger.setLevel(logging.DEBUG)
        # self.logger.handlers = []

        # if not self.logger.handlers:
        #     handler: logging.FileHandler = logging.FileHandler(self.log_file_path)
        #     handler.setLevel(logging.DEBUG)
        #     formatter: logging.Formatter = logging.Formatter(
        #         "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        #     )
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="api_key_validator",
            log_level=DEBUG,
        )

    def _validate(self, api_key: str, model: Optional[str] = None) -> None:
        """Run validation tests for each service."""
        from langchain.prompts import (
            ChatPromptTemplate,
            HumanMessagePromptTemplate,
            PromptTemplate,
            SystemMessagePromptTemplate,
        )

        results: ValidationResult = ValidationResult()
        system_prompt_template: PromptTemplate = PromptTemplate(template="test")
        human_prompt_template: PromptTemplate = PromptTemplate(template="test")

        prompt: ChatPromptTemplate = ChatPromptTemplate.from_messages(
            [
                SystemMessagePromptTemplate.from_template(
                    system_prompt_template.template
                ),
                HumanMessagePromptTemplate.from_template(
                    human_prompt_template.template
                ),
            ]
        )

        # Test OpenAI
        try:
            from langchain_openai import ChatOpenAI

            llm: ChatOpenAI = ChatOpenAI(api_key=api_key, model=model or "gpt-4o-mini")

            chain: Runnable = prompt | llm
            chain.invoke({})
            results.openai = True

        except Exception:
            pass

        # Test Anthropic
        try:
            from langchain_anthropic import ChatAnthropic

            llm: ChatAnthropic = ChatAnthropic(
                api_key=api_key, model=model or "claude-3.5-haiku"
            )
            chain: Runnable = prompt | llm
            chain.invoke({})
            results.anthropic = True
        except Exception:
            pass

        # Test Google
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI

            llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(
                api_key=api_key, model=model or "gemini-1.5-pro"
            )
            chain: Runnable = prompt | llm
            chain.invoke({})
            results.google = True
        except Exception:
            pass

        self._validated_already[api_key] = results

    def _check_attr(self) -> None:
        """Check if the API key is valid for any service."""
        if not hasattr(self, "_current_key"):
            raise RuntimeError(
                "Must call is_valid() before checking validity. "
                "Example usage: "
                ">>> validator = APIKeyValidator() "
                ">>> if validator.is_valid(api_key='...'): "
                ">>>     print('Key is valid!')"
                ">>> else: "
                ">>>     print('Key is invalid!')"
            )

    def is_valid(self, api_key: str, model: Optional[str] = None) -> bool:
        """Check if the API key is valid for any service. Validates if not already done."""
        if api_key not in self._validated_already:
            self._validate(api_key=api_key, model=model)

        results = self._validated_already[api_key]
        return any([results.openai, results.anthropic, results.google])

    def get_results_for_api_key(self, api_key: str) -> Dict[str, bool]:
        """Get detailed validation results. Validates if not already done."""
        if api_key not in self._validated_already:
            self._validate(api_key=api_key)

        results = self._validated_already[api_key]
        return {
            "openai": results.openai,
            "anthropic": results.anthropic,
            "google": results.google,
        }

    def get_full_results(self) -> Dict[str, Dict[str, bool]]:
        """Get detailed validation results for all keys."""
        return self._validated_already

    def print_results_for_api_key(self, api_key: str) -> None:
        """Print formatted validation results for a given API key."""
        from academic_metrics.utils.unicode_chars_dict import unicode_chars_dict

        results: Dict[str, bool] = self.get_results_for_api_key(api_key)
        print(f"API Key: {api_key}")
        for service, valid in results.items():
            status = (
                f"{unicode_chars_dict.get('boxed_checkmark', '')} Valid"
                if valid
                else f"{unicode_chars_dict.get('boxed_x', '')} Invalid"
            )
            print(f"{service.title()}: {status}")
        print("-" * 25)

    def print_full_results(self) -> None:
        """Print formatted validation results for all keys."""
        print("\nAPI Key Validation Results:")
        print("-" * 25)
        api_keys = list(self._validated_already.keys())
        for api_key in api_keys:
            self.print_results_for_api_key(api_key)

            ```

            src/academic_metrics/utils/minhash_util.py:
            ```
from __future__ import annotations
import random

from typing import List, Tuple, Set, TYPE_CHECKING
from academic_metrics.configs import configure_logging, DEBUG

if TYPE_CHECKING:
    import logging


class MinHashUtility:
    """
    A utility class for performing MinHash calculations to estimate the similarity between sets of data.

    This class provides methods for generating hash functions, tokenizing strings into n-grams, computing MinHash signatures,
    and comparing these signatures to estimate the similarity between sets. The MinHash technique is particularly useful
    in applications where exact matches are not necessary, but approximate matches are sufficient, such as duplicate
    detection, document similarity, and clustering.

    Attributes:
        num_hashes (int): The number of hash functions to use in MinHash calculations, affecting the accuracy and
                          performance of the similarity estimation.
        large_prime (int): A large prime number used as the modulus in hash functions to minimize collisions.
        hash_fns (list[callable]): A list of pre-generated hash functions used for computing MinHash signatures.

    Methods:
        tokenize(string, n): Tokenizes a string into n-grams.
        generate_coefficients(): Generates random coefficients for hash functions.
        generate_hash_functions(): Creates a list of hash functions based on generated coefficients.
        compute_signature(tokens): Computes the MinHash signature for a set of tokens.
        compare_signatures(signature1, signature2): Compares two MinHash signatures and returns their estimated similarity.

    The class utilizes linear hash functions of the form h(x) = (a * x + b) % large_prime, where 'a' and 'b' are randomly
    generated coefficients. This approach helps in reducing the likelihood of hash collisions and ensures a uniform
    distribution of hash values.

    Example usage:
        minhash_util = MinHashUtility(num_hashes=200)
        tokens = minhash_util.tokenize("example string", n=3)
        signature = minhash_util.compute_signature(tokens)
        # Further operations such as comparing signatures can be performed.

    More on MinHash: https://en.wikipedia.org/wiki/MinHash
    """

    def __init__(self, num_hashes: int, large_prime: int | None = 999983):
        """
        Initialize the MinHashUtility with the specified number of hash functions.

        Args:
            num_hashes (int): The number of hash functions to use for MinHash calculations.
            large_prime (int): The large prime number to use for hashing. Default is 999983.
        """
        self.logger: logging.Logger = configure_logging(
            module_name=__name__,
            log_file_name=f"minhash_util",
            log_level=DEBUG,
        )

        self.logger.info(
            f"Initializing MinHashUtility with {num_hashes} hash functions..."
        )

        self.num_hashes: int = num_hashes  # Number of hash functions to use for MinHash
        self.logger.info(f"Number of hash functions set to {num_hashes}.")

        self.large_prime: int = large_prime  # large prime number used for hashing
        self.logger.info(f"Large prime number set to {large_prime}.")

        self.hash_fns: List[callable] = (
            self.generate_hash_functions()
        )  # List of hash functions
        self.logger.info("Hash functions generated.")

    def tokenize(self, string: str, n: int = 3) -> Set[str]:
        """
        Tokenize the given string into n-grams to facilitate the identification of similar strings.

        N-grams are contiguous sequences of 'n' characters extracted from a string. This method is useful in various
        applications such as text similarity, search, and indexing where the exact match is not necessary, but approximate
        matches are useful.

        More on n-grams: https://en.wikipedia.org/wiki/N-gram

        Args:
            string (str): The string to be tokenized.
            n (int): The length of each n-gram. Default is 3.

        Returns:
            set: A set containing unique n-grams derived from the input string.

        Raises:
            ValueError: If 'n' is greater than the length of the string or less than 1.
        """
        # If the n-gram length is invalid, raise a ValueError
        self.logger.info(f"Checking if n-gram length is valid...")
        if n > len(string) or n < 1:
            self.logger.error(f"N-gram length is invalid, raising ValueError...")
            raise ValueError(
                "The n-gram length 'n' must be between 1 and the length of the string."
            )

        n_grams: Set[str] = set()  # Set to store unique n-grams
        self.logger.info(f"N-grams set created.")

        # Loop through the string to extract n-grams
        self.logger.info(f"Looping through the string to extract n-grams...")
        for i in range(len(string) - n + 1):
            n_gram: str = string[i : i + n]
            n_grams.add(n_gram)

        self.logger.info(f"N-grams extracted.")

        return n_grams

    def generate_coeeficients(self) -> List[Tuple[int, int]]:
        """
        Generate a list of tuples, each containing a pair of coefficients (a, b) used for hash functions.

        Each tuple consists of:
        - a (int): A randomly chosen multiplier coefficient.
        - b (int): A randomly chosen additive coefficient.

        These coefficients are used in the linear hash functions for MinHash calculations.

        Returns:
            list[tuple[int, int]]: A list of tuples, where each tuple contains two integers (a, b).
        """
        coefficients: List[Tuple[int, int]] = (
            []
        )  # List to store pairs of coefficients (a, b)
        self.logger.info(f"Coefficients list created.")

        # Generate a pair of coefficients for each hash function
        self.logger.info(f"Generating a pair of coefficients for each hash function...")
        for _ in range(self.num_hashes):
            a: int = random.randint(
                1, self.large_prime - 1
            )  # Randomly choose multiplier coefficient
            b: int = random.randint(
                0, self.large_prime - 1
            )  # Randomly choose additive coefficient
            coefficients.append((a, b))

        self.logger.info(f"Coefficients list populated.")

        return coefficients

    def generate_hash_functions(self) -> List[callable]:
        """
        Generate a list of linear hash functions for use in MinHash calculations.

        Each hash function is defined by a unique pair of coefficients (a, b) and is created using a factory function.
        These hash functions are used to compute hash values for elements in a set, which are essential for estimating
        the similarity between sets using the MinHash technique.

        The hash functions are of the form: h(x) = (a * x + b) % large_prime, where 'large_prime' is a large prime number
        used to reduce collisions in hash values.

        Overview of hash functions: https://en.wikipedia.org/wiki/Hash_function

        Returns:
            list: A list of lambda functions, each representing a linear hash function.
        """
        self.logger.info(f"Generating a list of linear hash functions...")

        def _hash_factory(a, b) -> callable:
            """
            Factory function to create a hash function with specified coefficients.

            Args:
                a (int): The multiplier coefficient in the hash function.
                b (int): The additive coefficient in the hash function.

            Returns:
                callable: A lambda function that takes an integer x and returns (a * x + b) % large_prime.
            """
            # Defines a hash function with coefficients a, b
            self.logger.info(f"Defining hash function with coefficients {a} and {b}...")
            return lambda x: (a * x + b) % self.large_prime

        hash_fns: List[callable] = []
        self.logger.info(f"Hash functions list created.")

        for _ in range(self.num_hashes):
            self.logger.info(f"Generating hash function {_}...")
            a: int = random.randint(
                1, self.large_prime - 1
            )  # Randomly choose multiplier coefficient
            self.logger.info(f"Multiplier coefficient {a} generated.")
            b: int = random.randint(
                0, self.large_prime - 1
            )  # Randomly choose additive coefficient
            self.logger.info(f"Additive coefficient {b} generated.")
            hash_fns.append(_hash_factory(a, b))
            self.logger.info(f"Hash function {_} generated.")

        return hash_fns

    def compute_signature(self, tokens: Set[int]) -> List[int]:
        """
        Compute MinHash signature for a set of tokens.
        A MinHash signature consists of the minimum hash value produced by each hash function across all tokens,
        which is used to estimate the similarity between sets of data.

        Detailed explanation of MinHash and its computation: https://en.wikipedia.org/wiki/MinHash

        Args:
            tokens (set[int]): A set of hashed tokens for which to compute the MinHash signature.

        Returns:
            list[int]: A list of minimum hash values, representing the MinHash signature.
        """
        # Initialize the signature with infinity values, which will later be replaced by the minimum hash values found.
        self.logger.info(f"Initializing signature with infinity values...")
        signature: List[int] = [float("inf")] * self.num_hashes
        self.logger.info(f"Signature initialized.")

        # Iterate over each token to compute its hash values using predefined hash functions
        self.logger.info(
            f"Iterating over each token to compute its hash values using predefined hash functions..."
        )
        for token in tokens:
            self.logger.info(f"Computing hash values for token {token}...")

            # Compute hash values for the token using each hash function
            self.logger.info(
                f"Computing hash values for token {token} using each hash function..."
            )
            hashed_values: List[int] = [
                hash_fn(hash(token)) for hash_fn in self.hash_fns
            ]
            self.logger.info(f"Hash values computed for token {token}.")

            # Update the signature by keeping the minimum hash value for each hash function
            self.logger.info(
                f"Updating the signature by keeping the minimum hash value for each hash function..."
            )
            for i in range(self.num_hashes):
                signature[i] = min(signature[i], hashed_values[i])
            self.logger.info(f"Signature updated.")

        self.logger.info(f"MinHash signature computed.")
        return signature

    def compare_signatures(self, signature1: List[int], signature2: List[int]) -> float:
        """
        Compare two MinHash signatures and return their similarity.
        The similarity is calculated as the fraction of hash values that are identical in the two signatures,
        which estimates the Jaccard similarity of the original sets from which these signatures were derived.

        This method is based on the principle that the more similar the sets are, the more hash values they will share,
        thus providing a proxy for the Jaccard index of the sets.

        More on estimating similarity with MinHash: https://en.wikipedia.org/wiki/Jaccard_index#MinHash

        Args:
            signature1 (list[int]): The MinHash signature of the first set, represented as a list of integers.
            signature2 (list[int]): The MinHash signature of the second set, represented as a list of integers.

        Returns:
            float: The estimated similarity between the two sets, based on their MinHash signatures.

        Raises:
            AssertionError: If the two signatures do not have the same length.
        """
        # Ensure both signatures are of the same length to compare them correctly
        self.logger.info(
            f"Ensuring both signatures are of the same length to compare them correctly..."
        )
        assert len(signature1) == len(
            signature2
        ), "Signatures must be of the same length."
        self.logger.info(f"Signatures are of the same length.")

        # Count the number of positions where the two signatures have the same hash value
        self.logger.info(
            f"Counting the number of positions where the two signatures have the same hash value..."
        )
        matching: int = sum(1 for i, j in zip(signature1, signature2) if i == j)
        self.logger.info(f"Number of matching positions counted.")

        # Calculate the similarity as the ratio of matching positions to the total number of hash functions
        self.logger.info(
            f"Calculating the similarity as the ratio of matching positions to the total number of hash functions..."
        )
        similarity: float = matching / len(signature1)
        self.logger.info(f"Similarity calculated.")
        return similarity

            ```

            src/academic_metrics/utils/taxonomy_util.py:
            ```
import json
import logging
from typing import Dict, List, Literal, TypeAlias

from academic_metrics.configs import configure_logging, DEBUG
from academic_metrics.other.in_memory_taxonomy import TAXONOMY_AS_STRING

# Type Aliases
TaxonomyDict: TypeAlias = Dict[str, Dict[str, List[str]]]
"""Type alias representing the taxonomy dictionary structure.

This type represents a three-level nested dictionary structure where:
    - The outer dictionary maps top-level category names to mid-level dictionaries
    - The mid-level dictionaries map mid-level category names to lists of low-level categories
    - The innermost lists contain strings representing low-level category names

Type Structure:
    Dict[str, Dict[str, List[str]]] where:
        - First str: Top-level category name
        - Second str: Mid-level category name
        - List[str]: List of low-level category names

Example Structure:
    .. code-block:: python

        {
            "Computer Science": {                     # Top-level category
                "Artificial Intelligence": [          # Mid-level category
                    "Machine Learning",              # Low-level category
                    "Natural Language Processing",   # Low-level category
                    "Computer Vision"                # Low-level category
                ],
                "Software Engineering": [
                    "Software Design",
                    "Software Testing",
                    "DevOps"
                ]
            }
        }
"""

TaxonomyLevel: TypeAlias = Literal["top", "mid", "low"]
"""Type alias representing valid taxonomy levels.

This type represents the three possible levels in the taxonomy hierarchy using string literals.

Type Structure:
    Literal["top", "mid", "low"] where:
        - "top": Represents the highest level categories (e.g., "Computer Science")
        - "mid": Represents middle-level categories (e.g., "Artificial Intelligence")
        - "low": Represents the most specific categories (e.g., "Machine Learning")

Usage:
    .. code-block:: python

        def example_function(level: TaxonomyLevel) -> None:
            match level:
                case "top":
                    # Handle top-level category
                    pass
                case "mid":
                    # Handle mid-level category
                    pass
                case "low":
                    # Handle low-level category
                    pass

Note:
    The type system will ensure that only these three string literals can be used
    where a TaxonomyLevel is expected. Any other string will result in a type error.

Example:
    .. code-block:: python

        # Valid usage
        level: TaxonomyLevel = "top"      # OK
        level: TaxonomyLevel = "mid"      # OK
        level: TaxonomyLevel = "low"      # OK
        
        # Invalid usage (would cause type error)
        # level: TaxonomyLevel = "other"  # Type error!
"""


class Taxonomy:
    """A class for managing and querying a three-level taxonomy structure.

    This class provides functionality to work with a hierarchical taxonomy that has three levels:
    top, mid, and low. It allows for querying categories at each level, validating categories,
    and finding relationships between categories at different levels.

    Attributes:
        _taxonomy (TaxonomyDict): The complete taxonomy structure as a nested dictionary.
        _valid_levels (List[TaxonomyLevel]): List of valid taxonomy levels ["top", "mid", "low"].
        _all_top_categories (List[str]): Cached list of all top-level categories.
        _all_mid_categories (List[str]): Cached list of all mid-level categories.
        _all_low_categories (List[str]): Cached list of all low-level categories.
        logger (logging.Logger): Logger instance for this class.

    Methods:
        Public Methods:
            get_top_categories(): Get all top-level categories.
            get_mid_categories(top_category): Get mid-level categories for a top category.
            get_low_categories(top_category, mid_category): Get low-level categories.
            get_top_cat_for_mid_cat(mid_cat): Find parent top category of a mid category.
            get_mid_cat_for_low_cat(low_cat): Find parent mid category of a low category.
            is_valid_category(category, level): Check if a category exists at a level.
            get_taxonomy(): Get the complete taxonomy dictionary.

        Private Methods:
            _set_all_top_categories(): Initialize list of top categories.
            _set_all_mid_categories(): Initialize list of mid categories.
            _set_all_low_categories(): Initialize list of low categories.
            _load_taxonomy_from_string(taxonomy_str, logger): Load taxonomy from JSON.

    Examples:
        .. code-block:: python

            # Create a taxonomy instance
            taxonomy = Taxonomy()

            # Get categories at different levels
            top_cats = taxonomy.get_top_categories()
            mid_cats = taxonomy.get_mid_categories(top_cats[0])
            low_cats = taxonomy.get_low_categories(top_cats[0], mid_cats[0])

            # Validate categories
            taxonomy.is_valid_category(top_cats[0], "top")
            True

            # Find parent categories
            parent_top = taxonomy.get_top_cat_for_mid_cat(mid_cats[0])
            parent_mid = taxonomy.get_mid_cat_for_low_cat(low_cats[0])
    """

    def __init__(self) -> None:
        """Initializes a new Taxonomy instance.

        This constructor initializes the taxonomy by loading the taxonomy data from a predefined
        string constant (TAXONOMY_AS_STRING). It sets up logging and initializes internal lists
        of categories at all levels (top, mid, and low).

        The taxonomy follows a three-level hierarchical structure:
        - Top level: Broad categories
        - Mid level: Sub-categories under each top category
        - Low level: Specific categories under each mid category

        Examples:
            .. code-block:: python

                # Create a new taxonomy instance
                taxonomy = Taxonomy()
                isinstance(taxonomy._taxonomy, dict)
                True

                # Verify initialization of category lists
                all(isinstance(cats, list) for cats in [
                    taxonomy._all_top_categories,
                    taxonomy._all_mid_categories,
                    taxonomy._all_low_categories
                ])
                True

                # Check that valid levels are properly set
                taxonomy._valid_levels == ["top", "mid", "low"]
                True
        """
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="taxonomy_util",
            log_level=DEBUG,
        )

        self.logger.info("Initializing Taxonomy")
        self._taxonomy: TaxonomyDict = self._load_taxonomy_from_string(
            TAXONOMY_AS_STRING, self.logger
        )
        self.logger.info("Taxonomy initialized successfully")

        self._valid_levels: List[TaxonomyLevel] = ["top", "mid", "low"]
        self._all_top_categories: List[str] = self._set_all_top_categories()
        self._all_mid_categories: List[str] = self._set_all_mid_categories()
        self._all_low_categories: List[str] = self._set_all_low_categories()

    def __str__(self) -> str:
        """Returns a string representation of the taxonomy.

        Converts the taxonomy dictionary into a formatted JSON string with proper indentation.
        This method is useful for debugging and displaying the taxonomy structure.

        Returns:
            str: A JSON-formatted string representation of the taxonomy.

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                taxonomy_str = str(taxonomy)
                isinstance(taxonomy_str, str)
                True
                # Verify it's valid JSON
                json.loads(taxonomy_str) == taxonomy._taxonomy
                True
        """
        self.logger.info("Converting taxonomy to string")
        taxonomy_str: str = json.dumps(self._taxonomy, indent=4)
        self.logger.info("Taxonomy string converted successfully")
        return taxonomy_str

    def _set_all_top_categories(self) -> List[str]:
        """Sets and returns all top-level categories from the taxonomy.

        This private method initializes the list of all top-level categories
        by calling get_top_categories(). It's used during taxonomy initialization
        to cache the top-level categories for faster access.

        Returns:
            List[str]: A list of all top-level categories in the taxonomy.

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                top_cats = taxonomy._set_all_top_categories()
                isinstance(top_cats, list)
                True
                # Verify all elements are strings
                all(isinstance(cat, str) for cat in top_cats)
                True
                # Verify it matches direct access to top categories
                top_cats == taxonomy.get_top_categories()
                True
        """
        return self.get_top_categories()

    def _set_all_mid_categories(self) -> List[str]:
        """Sets and returns all mid-level categories from the taxonomy.

        This private method collects all mid-level categories across all top-level categories
        by iterating through the taxonomy structure. It's used during taxonomy initialization
        to cache the mid-level categories for faster access.

        Returns:
            List[str]: A list of all mid-level categories in the taxonomy.

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                mid_cats = taxonomy._set_all_mid_categories()
                isinstance(mid_cats, list)
                True
                # Verify all elements are strings
                all(isinstance(cat, str) for cat in mid_cats)
                True
                # Verify each mid category belongs to some top category
                any(mid_cats[0] in taxonomy.get_mid_categories(top_cat)
                    for top_cat in taxonomy.get_top_categories())
                True
        """
        top_cats: List[str] = self.get_top_categories()
        mid_cats: List[str] = []
        for top_cat in top_cats:
            mid_cats.extend(self.get_mid_categories(top_cat))
        return mid_cats

    def _set_all_low_categories(self) -> List[str]:
        """Sets and returns all low-level categories from the taxonomy.

        This private method collects all low-level categories by iterating through
        all top and mid-level categories in the taxonomy structure. It's used during
        taxonomy initialization to cache the low-level categories for faster access.

        Returns:
            List[str]: A list of all low-level categories in the taxonomy.

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                low_cats = taxonomy._set_all_low_categories()
                isinstance(low_cats, list)
                True
                # Verify all elements are strings
                all(isinstance(cat, str) for cat in low_cats)
                True
                # Verify first low category exists in taxonomy structure
                top_cat = taxonomy.get_top_categories()[0]
                mid_cat = taxonomy.get_mid_categories(top_cat)[0]
                low_cats[0] in taxonomy.get_low_categories(top_cat, mid_cat)
                True
        """
        top_cats: List[str] = self.get_top_categories()
        low_cats: List[str] = []
        for top_cat in top_cats:
            for mid_cat in self.get_mid_categories(top_cat):
                low_cats.extend(self.get_low_categories(top_cat, mid_cat))
        return low_cats

    def get_top_categories(self) -> List[str]:
        """Retrieves all top-level categories from the taxonomy.

        Returns:
            List[str]: A list of all top-level category names.

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                top_cats = taxonomy.get_top_categories()
                isinstance(top_cats, list)
                True
                # Verify all elements are strings
                all(isinstance(cat, str) for cat in top_cats)
                True
                # Verify returned list matches taxonomy keys
                top_cats == list(taxonomy._taxonomy.keys())
                True
        """
        return list(self._taxonomy.keys())

    def get_mid_categories(self, top_category: str) -> List[str]:
        """Retrieves all mid-level categories for a given top-level category.

        Args:
            top_category (str): The top-level category name to get mid-level categories for.

        Returns:
            List[str]: A list of all mid-level category names under the specified top category.

        Raises:
            KeyError: If the top_category doesn't exist in the taxonomy.

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                top_cat = taxonomy.get_top_categories()[0]
                mid_cats = taxonomy.get_mid_categories(top_cat)
                isinstance(mid_cats, list)
                True
                # Verify all elements are strings
                all(isinstance(cat, str) for cat in mid_cats)
                True
                # Verify error handling
                try:
                    taxonomy.get_mid_categories("nonexistent_category")
                except KeyError:
                    True
        """
        return list(self._taxonomy[top_category].keys())

    def get_low_categories(self, top_category: str, mid_category: str) -> List[str]:
        """Retrieves all low-level categories for given top and mid-level categories.

        Args:
            top_category (str): The top-level category name.
            mid_category (str): The mid-level category name under the top category.

        Returns:
            List[str]: A list of all low-level category names under the specified categories.

        Raises:
            KeyError: If either the top_category or mid_category doesn't exist in the taxonomy.

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                top_cat = taxonomy.get_top_categories()[0]
                mid_cat = taxonomy.get_mid_categories(top_cat)[0]
                low_cats = taxonomy.get_low_categories(top_cat, mid_cat)
                isinstance(low_cats, list)
                True
                # Verify all elements are strings
                all(isinstance(cat, str) for cat in low_cats)
                True
                # Verify error handling for invalid categories
                try:
                    taxonomy.get_low_categories("nonexistent", "category")
                except KeyError:
                    True
        """
        return self._taxonomy[top_category][mid_category]

    def get_top_cat_for_mid_cat(self, mid_cat: str) -> str:
        """Finds the top-level category that contains a given mid-level category.

        This method searches through the taxonomy structure to find which top-level category
        contains the given mid-level category name.

        Args:
            mid_cat (str): The mid-level category to find the parent for.

        Returns:
            str: The name of the top-level category containing the mid-level category.

        Raises:
            ValueError: If the mid_cat is not found in any top-level category.

        Example:
            .. code-block:: python
                from academic_metrics.utils import Taxonomy

                taxonomy = Taxonomy()
                # Get a known mid category and its top category
                top_cat = taxonomy.get_top_categories()[0]
                mid_cat = taxonomy.get_mid_categories(top_cat)[0]

                # Verify we can find the top category
                found_top = taxonomy.get_top_cat_for_mid_cat(mid_cat)
                assert found_top == top_cat

                # Verify error handling for invalid mid category
                try:
                    taxonomy.get_top_cat_for_mid_cat("nonexistent_category")
                except ValueError:
                    pass  # Expected behavior
        """
        self.logger.info(f"Getting top category for mid category: {mid_cat}")

        for top_cat, mid_cats in self._taxonomy.items():
            self.logger.info(f"Checking mid category: {mid_cat} in {mid_cats}")
            if mid_cat in mid_cats:
                self.logger.info(f"Mid category: {mid_cat} found in {mid_cats}")
                self.logger.info(
                    f"Found top category: {top_cat} for mid category: {mid_cat}"
                )
                return top_cat

        raise ValueError(f"Mid category '{mid_cat}' not found in taxonomy")

    def get_mid_cat_for_low_cat(self, low_cat: str) -> str:
        """Finds the mid-level category that contains a given low-level category.

        This method searches through the taxonomy structure to find which mid-level category
        contains the given low-level category name.

        Args:
            low_cat (str): The low-level category to find the parent for.

        Returns:
            str: The name of the mid-level category containing the low-level category.

        Raises:
            ValueError: If the low_cat is not found in any mid-level category.

        Examples:
            .. code-block:: python

                from academic_metrics.utils import Taxonomy

                taxonomy = Taxonomy()
                # Get a known low category and its parent categories
                top_cat = taxonomy.get_top_categories()[0]
                mid_cat = taxonomy.get_mid_categories(top_cat)[0]
                low_cat = taxonomy.get_low_categories(top_cat, mid_cat)[0]

                # Verify we can find the mid category
                found_mid = taxonomy.get_mid_cat_for_low_cat(low_cat)
                assert found_mid == mid_cat

                # Verify error handling for invalid low category
                try:
                    taxonomy.get_mid_cat_for_low_cat("nonexistent_category")
                except ValueError:
                    pass  # Expected behavior
        """
        self.logger.info(f"Getting mid category for low category: {low_cat}")

        for _, mid_cats in self._taxonomy.items():
            for mid_cat, low_cats in mid_cats.items():
                self.logger.info(f"Checking low category: {low_cat} in {low_cats}")
                if low_cat in low_cats:
                    self.logger.info(f"Low category: {low_cat} found in {low_cats}")
                    self.logger.info(
                        f"Found mid category: {mid_cat} for low category: {low_cat}"
                    )
                    return mid_cat

        # If we complete the loop without finding a match
        raise ValueError(f"Low category '{low_cat}' not found in taxonomy")

    def is_valid_category(self, category: str, level: TaxonomyLevel) -> bool:
        """Validates whether a category exists in the taxonomy at the specified level.

        Args:
            category (str): The name of the category to validate.
            level (TaxonomyLevel): The taxonomy level to validate against.
                TaxonomyLevel is a type alias for the taxonomy levels; it can be one of the following: "top", "mid", or "low".

        Returns:
            bool: True if the category exists at the specified level; otherwise, False.

        Raises:
            ValueError: If the provided taxonomy level is invalid.

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                # Get known categories at each level
                top_cat = taxonomy.get_top_categories()[0]
                mid_cat = taxonomy.get_mid_categories(top_cat)[0]
                low_cat = taxonomy.get_low_categories(top_cat, mid_cat)[0]

                # Test valid categories at each level
                taxonomy.is_valid_category(top_cat, "top")
                True
                taxonomy.is_valid_category(mid_cat, "mid")
                True
                taxonomy.is_valid_category(low_cat, "low")
                True

                # Test invalid categories
                taxonomy.is_valid_category("nonexistent_category", "top")
                False

                # Test category at wrong level
                taxonomy.is_valid_category(top_cat, "low")
                False

                # Test invalid level
                try:
                    taxonomy.is_valid_category(top_cat, "invalid_level")  # type: ignore
                except ValueError:
                    True
        """
        if level not in self._valid_levels:
            raise ValueError(
                f"Invalid taxonomy level: {level}. Must be one of: {self._valid_levels}"
            )

        attribute_name = f"_all_{level}_categories"
        return category in getattr(self, attribute_name)

    def get_taxonomy(self) -> TaxonomyDict:
        """Returns the complete taxonomy dictionary.

        Returns:
            TaxonomyDict: The complete taxonomy structure as a dictionary.

        Note:
            The structure follows the format:

            .. code-block:: python

                {
                    "top_category": {
                        "mid_category": ["low_category1", "low_category2", ...]
                    }
                }

        Examples:
            .. code-block:: python

                taxonomy = Taxonomy()
                tax_dict = taxonomy.get_taxonomy()
                isinstance(tax_dict, dict)
                True
                # Verify structure
                top_cat = list(tax_dict.keys())[0]
                isinstance(tax_dict[top_cat], dict)
                True
                mid_cat = list(tax_dict[top_cat].keys())[0]
                isinstance(tax_dict[top_cat][mid_cat], list)
                True
        """
        return self._taxonomy

    @staticmethod
    def _load_taxonomy_from_string(
        taxonomy_str: str, logger: logging.Logger | None = None
    ) -> TaxonomyDict:
        """Loads and parses a taxonomy from a JSON string.

        Args:
            taxonomy_str (str): JSON string containing the taxonomy structure.
            logger (logging.Logger | None, optional): Logger instance for logging operations.
                Defaults to None.

        Returns:
            TaxonomyDict: The parsed taxonomy dictionary.

        Raises:
            json.JSONDecodeError: If the taxonomy string is not valid JSON.

        Examples:
            .. code-block:: python

                # Create a simple valid taxonomy string
                tax_str = '{"top": {"mid": ["low1", "low2"]}}'
                taxonomy = Taxonomy._load_taxonomy_from_string(tax_str)
                isinstance(taxonomy, dict)
                True
                # Verify structure
                list(taxonomy.keys()) == ["top"]
                True
                # Test invalid JSON
                try:
                    Taxonomy._load_taxonomy_from_string("{invalid json}")
                except json.JSONDecodeError:
                    True
        """
        if logger is not None:
            logger.info("Loading taxonomy from string")

        taxonomy: TaxonomyDict = json.loads(taxonomy_str)

        if logger is not None:
            logger.info("Taxonomy loaded successfully")

        return taxonomy


def demo_taxonomy():
    """Demonstrates the basic functionality of the Taxonomy class."""
    taxonomy: Taxonomy = Taxonomy()
    top_categories: List[str] = taxonomy.get_top_categories()
    print(top_categories)
    for top_category in top_categories:
        mid_categories: List[str] = taxonomy.get_mid_categories(top_category)
        print(f"top category: \n{top_category}, \nmid categories: \n{mid_categories}\n")
        for mid_category in mid_categories:
            low_categories: List[str] = taxonomy.get_low_categories(
                top_category, mid_category
            )
            print(
                f"mid category: \n{mid_category}, \nlow categories: \n{low_categories}\n"
            )
        print("\n\n")


if __name__ == "__main__":
    import unittest
    import argparse

    parser = argparse.ArgumentParser(description="Taxonomy utility")
    parser.add_argument("--demo", action="store_true", help="Run the demo")

    args = parser.parse_args()

    class TestTaxonomy(unittest.TestCase):
        def setUp(self):
            self.taxonomy = Taxonomy()

        def test_get_top_categories(self):
            top_cats = self.taxonomy.get_top_categories()
            self.assertIsInstance(top_cats, list)
            self.assertTrue(all(isinstance(cat, str) for cat in top_cats))

        def test_get_mid_categories(self):
            top_cats = self.taxonomy.get_top_categories()
            for top_cat in top_cats:
                mid_cats = self.taxonomy.get_mid_categories(top_cat)
                self.assertIsInstance(mid_cats, list)
                self.assertTrue(all(isinstance(cat, str) for cat in mid_cats))

        def test_get_low_categories(self):
            top_cats = self.taxonomy.get_top_categories()
            for top_cat in top_cats:
                mid_cats = self.taxonomy.get_mid_categories(top_cat)
                for mid_cat in mid_cats:
                    low_cats = self.taxonomy.get_low_categories(top_cat, mid_cat)
                    self.assertIsInstance(low_cats, list)
                    self.assertTrue(all(isinstance(cat, str) for cat in low_cats))

        def test_is_valid_category(self):
            # Test invalid level
            with self.assertRaises(ValueError):
                self.taxonomy.is_valid_category("test", "invalid_level")

            # Test valid categories
            top_cat = self.taxonomy.get_top_categories()[0]
            self.assertTrue(self.taxonomy.is_valid_category(top_cat, "top"))

            mid_cat = self.taxonomy.get_mid_categories(top_cat)[0]
            self.assertTrue(self.taxonomy.is_valid_category(mid_cat, "mid"))

            low_cat = self.taxonomy.get_low_categories(top_cat, mid_cat)[0]
            self.assertTrue(self.taxonomy.is_valid_category(low_cat, "low"))

            # Test invalid category
            self.assertFalse(
                self.taxonomy.is_valid_category("nonexistent_category", "top")
            )

        def test_get_top_cat_for_mid_cat(self):
            top_cat = self.taxonomy.get_top_categories()[0]
            mid_cat = self.taxonomy.get_mid_categories(top_cat)[0]
            found_top_cat = self.taxonomy.get_top_cat_for_mid_cat(mid_cat)
            self.assertEqual(found_top_cat, top_cat)

        def test_get_mid_cat_for_low_cat(self):
            top_cat = self.taxonomy.get_top_categories()[0]
            mid_cat = self.taxonomy.get_mid_categories(top_cat)[0]
            low_cat = self.taxonomy.get_low_categories(top_cat, mid_cat)[0]
            found_mid_cat = self.taxonomy.get_mid_cat_for_low_cat(low_cat)
            self.assertEqual(found_mid_cat, mid_cat)

    # Run the tests
    unittest.main(argv=[""], exit=False)

    if args.demo:
        demo_taxonomy()

            ```

            src/academic_metrics/utils/unicode_chars_dict.py:
            ```
unicode_chars_dict = {"boxed_checkmark": "\u2705", "boxed_x": "\u274c"}

            ```

            src/academic_metrics/utils/utilities.py:
            ```
from __future__ import annotations

import json
import logging
import os
from typing import TYPE_CHECKING, Any, Dict, List, Tuple

from academic_metrics.configs import configure_logging, DEBUG

if TYPE_CHECKING:
    from academic_metrics.enums import AttributeTypes
    from academic_metrics.strategies import AttributeExtractionStrategy
    from academic_metrics.factories import StrategyFactory
    from academic_metrics.utils import WarningManager


class Utilities:
    """
    A class containing various utility methods for processing and analyzing academic data.

    Attributes:
        strategy_factory (StrategyFactory): An instance of the StrategyFactory class.
        warning_manager (WarningManager): An instance of the WarningManager class.

    Methods:
        get_attributes(self, data, attributes):
            Extracts specified attributes from the data and returns them in a dictionary.
        crossref_file_splitter(self, *, path_to_file, split_files_dir_path):
            Splits a crossref file into individual entries and creates a separate file for each entry in the specified output directory.
        make_files(self, *, path_to_file: str, split_files_dir_path: str):
            Splits a document into individual entries and creates a separate file for each entry in the specified output directory.
    """

    CROSSREF_FILE_NAME_SUFFIX: str = "_crossref_item.json"

    def __init__(
        self,
        *,
        strategy_factory: StrategyFactory,
        warning_manager: WarningManager,
    ):
        """
        Initializes the Utilities class with the provided strategy factory and warning manager.

        Parameters:
            strategy_factory (StrategyFactory): An instance of the StrategyFactory class.
            warning_manager (WarningManager): An instance of the WarningManager class.
        """
        # Set up logger
        # self.log_file_path: str = os.path.join(LOG_DIR_PATH, "utilities.log")
        # self.logger: logging.Logger = logging.getLogger(__name__)
        # self.logger.setLevel(logging.DEBUG)

        # self.logger.handlers = []

        # # Add handler if none exists
        # if not self.logger.handlers:
        #     handler: logging.FileHandler = logging.FileHandler(self.log_file_path)
        #     handler.setLevel(logging.DEBUG)
        #     formatter: logging.Formatter = logging.Formatter(
        #         "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        #     )
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="utilities",
            log_level=DEBUG,
        )

        self.strategy_factory: StrategyFactory = strategy_factory
        self.warning_manager: WarningManager = warning_manager

    def get_attributes(
        self, data: Dict[str, Any], attributes: List[AttributeTypes]
    ) -> dict:
        """
        Extracts specified attributes from the article entry and returns them in a dictionary.
        It also warns about missing or invalid attributes.

        Parameters:
            entry_text (str): The text of the article entry.
            attributes (list of str): A list of attribute names to extract from the entry, e.g., ["title", "author"].

        Returns:
            dict: A dictionary where keys are attribute names and values are tuples.
                  Each tuple contains a boolean indicating success or failure of extraction,
                  and the extracted attribute value or None.

        Raises:
            ValueError: If an attribute not defined in `self.attribute_patterns` is requested.
        """
        attribute_results: Dict[AttributeTypes, Tuple[bool, Any]] = {}
        for attribute in attributes:
            extraction_strategy: AttributeExtractionStrategy = (
                self.strategy_factory.get_strategy(attribute, self.warning_manager)
            )
            attribute_results[attribute] = extraction_strategy.extract_attribute(data)
        return attribute_results

    def crossref_file_splitter(
        self, *, path_to_file: str, split_files_dir_path: str
    ) -> List[str]:
        """
        Splits a crossref file into individual entries and creates a separate file for each entry in the specified output directory.

        Parameters:
            path_to_file (str): The path to the full json file containing all crossref objects to be split
            split_files_dir_path (str): The path to the directory where the individual crossref object files should be saved.

        Returns:
            list: A list of file names.
        """
        with open(path_to_file, "r") as f:
            data: List[Dict[str, Any]] = json.load(f)

        for i, item in enumerate(data):
            file_name: str = f"{i}{self.CROSSREF_FILE_NAME_SUFFIX}"
            path: str = os.path.join(split_files_dir_path, file_name)

            if not os.path.exists(split_files_dir_path):
                os.makedirs(split_files_dir_path, exist_ok=True)

            with open(path, "w") as f:
                json.dump(item, f, indent=4)

        files: List[str] = os.listdir(split_files_dir_path)
        return files

    def make_files(
        self,
        *,
        path_to_file: str,
        split_files_dir_path: str,
    ):
        """
        Splits a document into individual entries and creates a separate file for each entry in the specified output directory.

        Parameters:
            path_to_file (str): The path to the full text file containing all metadata for the entries.
            output_dir (str): The path to the directory where the individual entry files should be saved.

        Returns:
            file_paths: A dictionary where each key is the number of the entry (starting from 1) and each value is the path to the corresponding file.

        This method first splits the document into individual entries using the `splitter` method.
        It then iterates over each entry, extracts the necessary attributes to form a filename,
        ensures the output directory exists, and writes each entry's content to a new file in the output directory.
        Then returns the file_paths dictionary to make referencing any specific document later easier
        """
        return self.crossref_file_splitter(
            path_to_file=path_to_file, split_files_dir_path=split_files_dir_path
        )

            ```

            src/academic_metrics/utils/warning_manager.py:
            ```
import logging
import os
import warnings
from typing import List

from academic_metrics.configs import configure_logging, DEBUG


class CustomWarning(Warning):
    """
    Custom warning class to store warning details.

    Args:
        Warning (str): _description_

    Attributes:
        category (str): The category of the warning.
        message (str): The message of the warning.
        entry_id (str, optional): The entry ID of the warning. Defaults to None.

    Methods:
        __init__(self, category: str, message: str, entry_id: str = None):
            Initializes the CustomWarning class with the provided category, message, and entry ID.

    Summary:
        This class is a custom warning class that is used to store warning details.
        It is used to store warning details in a structured way.
    """

    def __init__(self, category: str, message: str, entry_id: str = None):
        """
        Initializes the CustomWarning class with the provided category, message, and entry ID.

        Args:
            category (str): The category of the warning.
            message (str): The message of the warning.
            entry_id (str, optional): The entry ID of the warning. Defaults to None.

        Summary:
            This method initializes the CustomWarning class with the provided category, message, and entry ID.
        """
        self.category: str = category
        self.message: str = message
        self.entry_id: str = entry_id
        super().__init__(self.message)


class WarningManager:
    """
    Class to manage warnings.

    Attributes:
        warning_count (int): The number of warnings.
        warnings (list): A list of warnings.

    Methods:
        log_warning(self, category: str, warning_message: str, entry_id: str = None) -> CustomWarning:
            Logs a warning with the provided category, message, and entry ID.

            Args:
                category (str): The category of the warning.
                warning_message (str): The message of the warning.
                entry_id (str, optional): The entry ID of the warning. Defaults to None.

            Returns:
                CustomWarning: The warning that was logged.

        display_warning_summary(self):
            Displays the summary of the warnings.

            Args:
                None

            Returns:
                None

    Summary:
        This class is used to manage warnings.
        It is used to store warnings in a list and display the summary of the warnings.
    """

    def __init__(self):
        """
        Initializes the WarningManager class.

        Args:
            None

        Summary:
            This method initializes the WarningManager class.
        """
        # Set up logger
        # current_dir: str = os.path.dirname(os.path.abspath(__file__))
        # log_file_path: str = os.path.join(current_dir, "warning_manager.log")
        # self.logger: logging.Logger = logging.getLogger(__name__)
        # self.logger.setLevel(logging.DEBUG)

        # # Add handler if none exists
        # if not self.logger.handlers:
        #     handler: logging.FileHandler = logging.FileHandler(log_file_path)
        #     handler.setLevel(logging.DEBUG)
        #     formatter: logging.Formatter = logging.Formatter(
        #         "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        #     )
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
        self.logger = configure_logging(
            module_name=__name__,
            log_file_name="warning_manager",
            log_level=DEBUG,
        )

        self.warning_count: int = 0
        self.warnings: List[CustomWarning] = []

    def log_warning(
        self, category: str, warning_message: str, entry_id: str = None
    ) -> CustomWarning:
        """
        Logs a warning with the provided category, message, and entry ID.

        Args:
            category (str): The category of the warning.
            warning_message (str): The message of the warning.
            entry_id (str, optional): The entry ID of the warning. Defaults to None.

        Returns:
            CustomWarning: The warning that was logged.
        """
        warning = CustomWarning(category, warning_message, entry_id)
        warnings.warn(warning)
        self.warning_count += 1
        self.warnings.append(warning)
        return warning

    def display_warning_summary(self):
        """
        Displays the summary of the warnings.

        Args:
            None

        Returns:
            None

        Summary:
            This method displays the summary of the warnings.
            It displays the category, message, and entry ID of the warning.
        """
        if self.warning_count > 0:
            print(f"\nWarning Summary ({self.warning_count} warnings):")
            for i, warning in enumerate(self.warnings, 1):
                print(f"{i}. {warning.category}: {warning.message[:50]}...")

            user_input: str = input(
                "\nEnter a number to see full warning details, or press Enter to continue: "
            )
            if user_input.isdigit() and 1 <= int(user_input) <= len(self.warnings):
                warning: CustomWarning = self.warnings[int(user_input) - 1]
                print("\nFull Warning Details:")
                print(f"Category: {warning.category}")
                print(f"Message: {warning.message}")
                print(f"Entry ID: {warning.entry_id}")

            ```

                src/assets/verification_scripts/reformatJsonScripts/economics_reformat.py:
                ```
import json
import re
import os

current_file_location = os.path.dirname(os.path.abspath(__file__))


class TitleExtractor:
    def __init__(self, json_file_path, output_file_name):
        self.json_file_path = json_file_path
        self.output_file_name = output_file_name
        self.citation_count = 0
        self.match_count = 0
        self.non_match_count = 0
        self.non_match_titles = []
        self.titles = []

        if not os.path.exists(self.json_file_path):
            raise FileNotFoundError(f"File not found: {self.json_file_path}")

        self.data = self.load_json_data()

        output_dir = os.path.dirname(self.output_file_name)
        os.makedirs(output_dir, exist_ok=True)

    def load_json_data(self):
        with open(self.json_file_path, "r", encoding="utf-8") as file:
            return json.load(file)

    def extract_titles(self):
        category_name = list(self.data.keys())[0]
        for item in self.data[category_name]:
            if "Citation" in item:
                self.citation_count += 1
                citation = item["Citation"]
                title = self.extract_title_from_citation(citation)
                if title:
                    self.match_count += 1
                    self.titles.append(title)
                else:
                    self.non_match_count += 1
                    self.non_match_titles.append(citation)
        return self.titles

    def extract_title_from_citation(self, citation):
        # Remove content in square brackets
        citation = re.sub(r"\[.*?\]", "", citation)

        # Remove URLs
        citation = re.sub(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
            "",
            citation,
        )

        # Try to match the title (including all-caps titles)
        patterns = [
            r"\(\d{4}\)\.\s*(.*?)\.\s*(?:[A-Z][a-z]+\s+)+",
            r"\(\d{4}\)\.\s*(.*?)(?:\.\s*(?:https?://|www\.|\[|$))",
            r"\(\d{4}\)\.\s*([A-Z\s]+)\.",  # For all-caps titles
            r"\(\d{4}\)\.\s*(.*?)\.",  # Fallback pattern
            r"\(in press\)\.\s*(.*?)\.",  # For "in press" citations
        ]

        for pattern in patterns:
            match = re.search(pattern, citation)
            if match:
                title = match.group(1).strip()
                # Convert all-caps titles to title case
                if title.isupper():
                    title = title.title()
                return title

        return None

    def get_titles(self):
        return self.extract_titles()

    def save_titles_to_json(self):
        with open(self.output_file_name, "w", encoding="utf-8") as file:
            json.dump(self.titles, file, indent=4, ensure_ascii=False)

    def print_stats(self):
        print(f"Total Citations Processed: {self.citation_count}")
        print(f"Matches Found: {self.match_count}")
        print(f"Non-Matches: {self.non_match_count}")
        if self.non_match_count > 0:
            print("Non-matching citations:")
            for citation in self.non_match_titles:
                print(citation)


# Usage for Economics
economics_json_path = os.path.abspath(
    os.path.join(current_file_location, "..", "..", "json_data", "Economics.json")
)
economics_output_path = os.path.abspath(
    os.path.join(current_file_location, "reformattedFiles", "economics_output.json")
)

# Usage for Accounting and Legal Studies
accounting_json_path = os.path.abspath(
    os.path.join(
        current_file_location,
        "..",
        "..",
        "json_data",
        "Accounting-and-Legal-Studies.json",
    )
)
accounting_output_path = os.path.abspath(
    os.path.join(current_file_location, "reformattedFiles", "accounting_output.json")
)

# Usage for Finance
finance_json_path = os.path.abspath(
    os.path.join(current_file_location, "..", "..", "json_data", "Finance.json")
)
finance_output_path = os.path.abspath(
    os.path.join(current_file_location, "reformattedFiles", "finance_output.json")
)

# Usage for Information and Decision Sciences
ids_json_path = os.path.abspath(
    os.path.join(
        current_file_location,
        "..",
        "..",
        "json_data",
        "Information-and-Decision-Sciences.json",
    )
)
ids_output_path = os.path.abspath(
    os.path.join(current_file_location, "reformattedFiles", "ids_output.json")
)

# Usage for Management
management_json_path = os.path.abspath(
    os.path.join(current_file_location, "..", "..", "json_data", "Management.json")
)
management_output_path = os.path.abspath(
    os.path.join(current_file_location, "reformattedFiles", "management_output.json")
)

# Usage for Marketing
marketing_json_path = os.path.abspath(
    os.path.join(current_file_location, "..", "..", "json_data", "Marketing.json")
)
marketing_output_path = os.path.abspath(
    os.path.join(current_file_location, "reformattedFiles", "marketing_output.json")
)

# Process all six JSON files
for json_path, output_path in [
    (economics_json_path, economics_output_path),
    (accounting_json_path, accounting_output_path),
    (finance_json_path, finance_output_path),
    (ids_json_path, ids_output_path),
    (management_json_path, management_output_path),
    (marketing_json_path, marketing_output_path),
]:
    try:
        extractor = TitleExtractor(json_path, output_path)
        titles = extractor.get_titles()
        extractor.save_titles_to_json()
        print(f"\nProcessing {os.path.basename(json_path)}:")
        extractor.print_stats()
    except FileNotFoundError as e:
        print(e)

                ```

                    src/data/core/ai_outputs/static_references/abstracts.py:
                    ```
from dataclasses import dataclass


@dataclass
class Abstract:
    abstract: str


abstracts = []

# this is in the data
abstract_1 = Abstract(
    abstract="Drawing on expectation states theory and expertise utilization literature, we examine the effects of team members' actual expertise and social status on the degree of influence they exert over team processes via perceived expertise. We also explore the conditions under which teams rely on perceived expertise versus social status in determining influence relationships in teams. To do so, we present a contingency model in which the salience of expertise and social status depends on the types of intragroup conflicts. Using multiwave survey data from 50 student project teams with 320 members at a large national research institute located in South Korea, we found that both actual expertise and social status had direct and indirect effects on member influence through perceived expertise. Furthermore, perceived expertise at the early stage of team projects is driven by social status, whereas perceived expertise at the later stage of a team project is mainly driven by actual expertise. Finally, we found that members who are being perceived as experts are more influential when task conflict is high or when relationship conflict is low. We discuss the implications of these findings for research and practice."
)
abstracts.append(abstract_1.abstract)

# this is not in the data
abstract_2 = Abstract(
    abstract="The goal of this paper is to investigate how deregulating foreign equity ownership influences a firm's innovation investment. We attempt to answer this question using data from 530 Korean manufacturing firms between 1998 and 2003 through generalised estimating equations. Our findings suggest that foreign ownership and R&D investment exhibit an inverted U-shaped relationship because the incentives to monitor managers' decision-making processes initially play a greater role, but this role stagnates as the share owned by foreign investors becomes concentrated. In addition, we consider firm heterogeneity and observe the negative moderation effects of firm age and the positive moderation effects of growth opportunity."
)
abstracts.append(abstract_2.abstract)

# this is in the data
abstract_3 = Abstract(
    abstract="This study examined the role of perceived organizational commitment on managers' assessments of employees' career growth opportunities. Based on a paired sample of 161 legal secretaries and their managers, results indicated that managers used the attitudes and behaviors displayed by employees (strong extra-role performance and enhanced work engagement) as cues from which to base their perceptions of employees' affective commitment to the organization. In turn, employees perceived as highly committed to the organization experienced enhanced content and structural career growth opportunities. Moreover, the relation between managers' perceptions of employees' organizational commitment and content career growth opportunities was stronger for employees perceived as also highly committed to their careers than for employees perceived as less committed to their careers."
)
abstracts.append(abstract_3.abstract)

# this is in the data
abstract_4 = Abstract(
    abstract="This study examined how firms combine alliances and acquisitions in an exploration/exploitation framework. By conducting cluster analysis on a sample of 1270 acquisitions made by 836 firms, we first identified the patterns in alliance and acquisition activities undertaken by these firms. Five distinct patterns were identified: (I) low alliance-low acquisition, (II) low alliance-high acquisition, (III) high alliance-low acquisition, (IV) high alliance-high acquisition, and (V) medium alliance-very high acquisition. Next, we analyzed the different ways in which the two modes were interlinked within these five patterns for exploration/exploitation. Patterns III and IV appeared to involve both exploration/exploitation and mutually reinforce exploration/exploitation. In contrast, in the remaining patterns, the two modes appeared to be more loosely coupled with each other, with a focus on exploitation."
)
abstracts.append(abstract_4.abstract)
# @dataclass
# class PaperMetadata:
#     title: str
#     abstract: str
#     publisher: str
#     author: str


# papers = []

# paper_1 = PaperMetadata(
#     title="The Effects of Expertise and Social Status on Team Member Influence and the Moderating Roles of Intragroup Conflicts",
#     abstract="Drawing on expectation states theory and expertise utilization literature, we examine the effects of team members' actual expertise and social status on the degree of influence they exert over team processes via perceived expertise. We also explore the conditions under which teams rely on perceived expertise versus social status in determining influence relationships in teams. To do so, we present a contingency model in which the salience of expertise and social status depends on the types of intragroup conflicts. Using multiwave survey data from 50 student project teams with 320 members at a large national research institute located in South Korea, we found that both actual expertise and social status had direct and indirect effects on member influence through perceived expertise. Furthermore, perceived expertise at the early stage of team projects is driven by social status, whereas perceived expertise at the later stage of a team project is mainly driven by actual expertise. Finally, we found that members who are being perceived as experts are more influential when task conflict is high or when relationship conflict is low. We discuss the implications of these findings for research and practice.",
#     publisher="SAGE Publications",
#     author="Kwangwook Gang"
# )

# paper_2 = PaperMetadata(
#     title=
# )

                    ```

                    src/data/core/ai_outputs/static_references/output_to_excel.py:
                    ```
import json
import pandas as pd
from abstracts import abstracts
import os
import re

ARTICLE_DATA = None
this_directory = os.path.dirname(os.path.abspath(__file__))
output_files_directory = os.path.join(this_directory, "..", "..", "output_files")
print(f"\n\nOutput files directory: {output_files_directory}")
file_name = "test_processed_crossref_article_stats_obj_data.json"
file_path = os.path.join(output_files_directory, file_name)
print(f"\n\nFile path: {file_path}")
with open(file_path, "r") as f:
    ARTICLE_DATA = json.load(f)

print("\nAbstracts in list:")
for i, abstract in enumerate(abstracts):
    print(f"\nAbstract {i} (first 100 chars): {abstract[:100]}...")

print("\nAbstracts in JSON:")
for doi, article in ARTICLE_DATA.items():
    print(f"\nDOI {doi} (first 100 chars): {article['abstract'][:100]}...")


def clean_text(text):
    """Clean text by removing punctuation, extra whitespace, and converting to lowercase"""
    # Remove punctuation and convert to lowercase
    text = re.sub(r"[^\w\s]", "", text.lower())
    # Normalize whitespace
    text = " ".join(text.split())
    return text


def find_article_details(target_abstract):
    """
    Find article details by matching an abstract in the articles data.

    Args:
        target_abstract (str): The abstract to search for
        articles_data (dict): The dictionary containing article data

    Returns:
        dict: Article details including title, faculty members, and DOI
              Returns None if no match is found
    """
    global ARTICLE_DATA
    # Clean up the target abstract
    new_target_abstract = clean_text(target_abstract)

    # Loop through each article
    for doi, article in ARTICLE_DATA.items():
        article_abstract = clean_text(article["abstract"])

        # Print first 100 chars of both abstracts for debugging
        print(f"\nComparing abstracts:")
        print(f"Target  (first 100): {new_target_abstract[:100]}")
        print(f"Article (first 100): {article_abstract[:100]}")

        if article_abstract == new_target_abstract:
            print(f"âœ… Found match!")
            return {
                "title": article["title"],
                "faculty_members": article["faculty_members"],
                "doi": doi,
            }

    return None


# Create an absolute path for the output Excel file
output_excel_path = os.path.join(this_directory, "classification_output.xlsx")
print(f"\nWill save Excel file to: {output_excel_path}")

data_rows = []  # Initialize an empty list to hold data rows

with open("classification_results.json", "r") as f:
    classification_results = json.load(f)

# Load the raw_classification_outputs JSON data
with open("raw_classification_outputs.json", "r") as f:
    raw_classification_outputs = json.load(f)

# Open a file to log the output
with open("logs.txt", "w") as log_file:
    # Initialize a list to hold data rows for the DataFrame
    data_rows = []

    # Iterate over each abstract in the classification_results
    for abstract_key, categories in classification_results.items():
        print(f"\nProcessing abstract key: {abstract_key}")

        index = int(abstract_key.split("_")[1])
        abstract_text = abstracts[index]
        abstract_details = find_article_details(abstract_text)

        if abstract_details is None:
            print(f"âŒ No article details found for abstract: {abstract_key}")
            continue

        print(f"âœ… Processing {abstract_key}:")
        abstract_title = abstract_details["title"]
        abstract_faculty_members = abstract_details["faculty_members"]
        abstract_doi = abstract_details["doi"]

        # Find the corresponding raw data for this abstract
        raw_data = raw_classification_outputs[index]
        print(f"\n\nRaw data: {raw_data}", file=log_file)

        # Iterate over each category in the classification_results
        def traverse_categories(cat_dict, parent_categories):
            for cat_name, sub_cats in cat_dict.items():
                current_categories = parent_categories + [cat_name]
                full_category = ", ".join(current_categories)
                print(f"\n\nTraversing category: {cat_name}", file=log_file)

                # Match the abstract and category in raw_classification_outputs
                category_reasoning = ""
                category_confidence = ""
                for classification in raw_data.get("classifications", []):
                    if abstract_text == classification.get(
                        "abstract"
                    ) and cat_name in classification.get("categories", []):
                        category_reasoning = classification.get("reasoning", "")
                        category_confidence = classification.get("confidence_score", "")
                        break

                # Prepare the data row
                # data_row = {
                #     'Abstract Key': abstract_key,
                #     'Abstract': abstract_text,
                #     'Category': cat_name,
                #     'Parent Categories': ', '.join(parent_categories),
                #     'Reasoning': category_reasoning,
                #     'Confidence Score': category_confidence,
                #     'Reflection': raw_data.get('reflection', ''),
                #     'Feedback': '; '.join([fb.get('feedback', '') for fb in raw_data.get('feedback', [])])
                # }
                data_row = {
                    "Abstract Key": abstract_key,
                    "Title": abstract_title,
                    "Authors": ", ".join(abstract_faculty_members),
                    "DOI": abstract_doi,
                    "Abstract": abstract_text,
                    "Category": cat_name,
                    "Parent Categories": ", ".join(parent_categories),
                    "Themes": (
                        "; ".join(cat_dict.get("themes", []))
                        if not parent_categories
                        else ""
                    ),
                }
                print(f"\n\nData row: {data_row}", file=log_file)
                data_rows.append(data_row)

                if isinstance(sub_cats, dict):
                    # Recurse with the subcategories
                    traverse_categories(sub_cats, current_categories)

        # Start traversing from the top-level categories
        traverse_categories(categories, [])
        print(f"\n\nCurrent number of rows: {len(data_rows)}")

print(f"\nFinal number of rows collected: {len(data_rows)}")


# Create DataFrame and save to Excel
try:
    print("\nCreating DataFrame...")
    df = pd.DataFrame(data_rows)
    print(f"DataFrame created with shape: {df.shape}")

    # Debug: Print column names
    print("\nColumns in DataFrame:")
    print(df.columns.tolist())

    # Debug: Print first few rows
    print("\nFirst few rows of DataFrame:")
    print(df.head())

    print("\nSaving to Excel...")
    # Explicitly set the column order
    columns = [
        "Abstract Key",
        "Title",
        "Authors",
        "DOI",
        "Abstract",
        "Category",
        "Parent Categories",
        "Themes",
    ]
    df = df[columns]  # Reorder columns

    df.to_excel(output_excel_path, index=False, engine="openpyxl")
    print(f"âœ… Excel file saved successfully to: {output_excel_path}")

except Exception as e:
    print(f"âŒ Error saving Excel file: {str(e)}")

    # Print some debugging info about the data
    print("\nDebugging info:")
    if data_rows:
        print("Sample row data:")
        for key, value in data_rows[0].items():
            print(f"{key}: {value[:100] if isinstance(value, str) else value}")

                    ```

                src/data/core/output_files/article_url_list.py:
                ```
import json

with open("test_processed_crossref_article_stats_obj_data.json", "r") as f:
    data = json.load(f)

url_list = [article["url"] for article in data.values()]

# Write to JSON file with indentation
with open("article_urls.json", "w") as f:
    json.dump(url_list, f, indent=4)

                ```

                src/data/core/output_files/heatmap.py:
                ```
import numpy as np
import pandas as pd
import json
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
import numpy as np
import pandas as pd
import json
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
with open("test_processed_crossref_article_stats_obj_data.json", "r") as f:
    data = json.load(f)

# Get only Salisbury faculty members
salisbury_faculty = set()
for article in data.values():
    for faculty, affiliation in article["faculty_affiliations"].items():
        if "Salisbury" in affiliation and affiliation != "Salisbury University":
            salisbury_faculty.add(faculty)

salisbury_faculty = list(
    salisbury_faculty
)  # Should be ['KwangWook Gang', 'Minseok Park', 'Christy H. Weer']

# Create collaboration matrix for Salisbury faculty
collaboration_matrix = pd.DataFrame(
    0, index=salisbury_faculty, columns=salisbury_faculty
)

# Fill collaboration matrix
for article in data.values():
    faculty_in_paper = [f for f in article["faculty_members"] if f in salisbury_faculty]


# Create heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(
    collaboration_matrix,
    annot=True,
    fmt=".0f",
    cmap="YlOrRd",
    cbar_kws={"label": "Number of Collaborations"},
)
plt.title("Faculty Collaboration Network")
plt.tight_layout()
plt.savefig("faculty_collaboration_heatmap.png")
plt.close()

# Create affiliation-citation matrix
affiliations = set()
for article in data.values():
    affiliations.update(article["faculty_affiliations"].values())
affiliations = list(affiliations)

affiliation_citation_matrix = pd.DataFrame(
    0, index=affiliations, columns=["article_count", "total_citations"]
)

for article in data.values():
    tc_count = article["tc_count"]
    for affiliation in article["faculty_affiliations"].values():
        if affiliation != "Salisbury University" and "Salisbury" in affiliation:
            affiliation_citation_matrix.loc[affiliation, "article_count"] += 1
            affiliation_citation_matrix.loc[affiliation, "total_citations"] += tc_count

# Create heatmap for affiliations
plt.figure(figsize=(10, 8))
sns.heatmap(
    affiliation_citation_matrix,
    annot=True,
    fmt=".0f",
    cmap="YlOrRd",
    cbar_kws={"label": "Count"},
)
plt.title("Institution Impact: Articles vs Citations")
plt.tight_layout()
plt.savefig("institution_impact_heatmap.png")
plt.close()

                ```

                src/data/core/output_files/matrix_vis.py:
                ```
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt

# Load data
with open("test_processed_category_data.json", "r") as f:
    data = json.load(f)

# Define categories for each level
top_level_categories = ["Business", "Social sciences"]
mid_level_categories = [
    "Business administration and management",
    "Economics",
    "Public policy analysis",
    "Sociology, demography, and population studies",
    "Social sciences, other",
]
low_level_categories = [
    "Business management and administration",
    "Business administration and management nec",
    "Organizational leadership",
    "Applied economics",
    "Development economics and international development",
    "Public policy analysis, general",
    "Sociology, general",
    "Social sciences nec",
]


# Bar Charts
def create_bar_chart(categories, level_name):
    plt.figure(figsize=(15, 8))
    x = np.arange(len(categories))
    width = 0.35

    tc_counts = [data[cat]["tc_count"] for cat in categories]
    cit_avgs = [data[cat]["citation_average"] for cat in categories]

    plt.bar(x - width / 2, tc_counts, width, label="Total Citations")
    plt.bar(x + width / 2, cit_avgs, width, label="Citation Average")
    plt.xticks(x, categories, rotation=45, ha="right")
    plt.title(f"{level_name} Categories - Citation Metrics")
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{level_name.lower()}_level_citations_bar.png")
    plt.close()


# Line Charts
def create_line_chart(categories, level_name):
    plt.figure(figsize=(20, 10))
    plt.plot(
        categories,
        [data[cat]["tc_count"] for cat in categories],
        "o-",
        linewidth=2,
        markersize=10,
        label="Total Citations",
    )
    plt.plot(
        categories,
        [data[cat]["citation_average"] for cat in categories],
        "s-",
        linewidth=2,
        markersize=10,
        label="Citation Average",
    )
    plt.xticks(rotation=45, ha="right")
    plt.title(f"{level_name} Categories - Citation Metrics")
    plt.legend(fontsize=12)
    plt.grid(True, linestyle="--", alpha=0.7)
    plt.tight_layout()
    plt.savefig(f"{level_name.lower()}_level_citations_line.png")
    plt.close()


# Create all charts
for categories, level in [
    (top_level_categories, "Top"),
    (mid_level_categories, "Mid"),
    (low_level_categories, "Low"),
]:
    create_bar_chart(categories, level)
    create_line_chart(categories, level)

                ```

                src/data/core/output_files/test.py:
                ```
import json
import pandas as pd

# Define the paths to your JSON files
json_files = [
    "test_processed_faculty_stats_data.json",
    "test_processed_crossref_article_stats_data.json",
    "test_processed_category_data.json",
    "test_processed_crossref_article_stats_obj_data.json",
]

# Initialize empty DataFrames
df_faculty_stats = pd.DataFrame()
df_article_stats = pd.DataFrame()
df_category_stats = pd.DataFrame()

for file in json_files:
    with open(file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Process faculty stats data
    if "faculty_stats" in str(data):
        for category, details in data.items():
            faculty_stats = details.get("faculty_stats", {})
            for faculty_name, stats in faculty_stats.items():
                stats_flat = pd.json_normalize(stats, sep="_")
                stats_flat["faculty_name"] = faculty_name
                stats_flat["category"] = category
                df_faculty_stats = pd.concat(
                    [df_faculty_stats, stats_flat], ignore_index=True
                )

    # Process article stats data (by category)
    elif "article_citation_map" in str(data):
        for category, details in data.items():
            article_citation_map = details.get("article_citation_map", {})
            for doi, article_data in article_citation_map.items():
                article_data_flat = pd.json_normalize(article_data, sep="_")
                article_data_flat["doi"] = doi
                article_data_flat["category"] = category
                df_article_stats = pd.concat(
                    [df_article_stats, article_data_flat], ignore_index=True
                )

    # Process article stats data (by DOI)
    elif any(key.startswith("10.") for key in data.keys()):
        for doi, article_data in data.items():
            article_data_flat = pd.json_normalize(article_data, sep="_")
            article_data_flat["doi"] = doi
            df_article_stats = pd.concat(
                [df_article_stats, article_data_flat], ignore_index=True
            )

    # Process category data
    elif "doi_list" in str(data):
        for category, category_data in data.items():
            category_data_flat = pd.json_normalize(category_data, sep="_")
            category_data_flat["category"] = category
            df_category_stats = pd.concat(
                [df_category_stats, category_data_flat], ignore_index=True
            )

# Now, we need to merge all DataFrames into one comprehensive DataFrame

# Merge article stats with faculty stats on 'doi' and 'faculty_name'

# Expand 'faculty_members' in df_article_stats
if not df_article_stats.empty and "faculty_members" in df_article_stats.columns:
    df_article_stats = df_article_stats.explode("faculty_members").rename(
        columns={"faculty_members": "faculty_name"}
    )
    # Ensure 'faculty_name' is a string
    df_article_stats["faculty_name"] = df_article_stats["faculty_name"].astype(str)
else:
    df_article_stats["faculty_name"] = None

# Merge df_article_stats with df_faculty_stats on 'faculty_name'
df_merged = pd.merge(
    df_article_stats,
    df_faculty_stats,
    on="faculty_name",
    how="outer",
    suffixes=("_article", "_faculty"),
)

# Combine 'category' columns into one
df_merged["category"] = df_merged["category_article"].combine_first(
    df_merged["category_faculty"]
)

# Optionally, drop the old 'category' columns
df_merged = df_merged.drop(columns=["category_article", "category_faculty"])

# Merge with category stats on 'category'
df_final = pd.merge(
    df_merged, df_category_stats, on="category", how="outer", suffixes=("", "_category")
)

# Drop duplicates
df_final = df_final.drop_duplicates()

# Output the final DataFrame
print("Final Merged DataFrame:")
print(df_final.head())

df_final.to_csv("combined_stats.csv", index=False)

                ```

                src/data/core/output_files/test_vis_w_lux.ipynb:
                ```
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m         df_articles \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(article_citation_map, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     38\u001b[0m         df_articles\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(df_articles\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 39\u001b[0m         df_articles \u001b[38;5;241m=\u001b[39m \u001b[43mdf_articles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39massign(category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m     40\u001b[0m         df_article_stats \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_article_stats, df_articles], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Process article stats data (by DOI)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cosc426/lib/python3.12/site-packages/pandas/core/frame.py:4074\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4070\u001b[0m is_mi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex)\n\u001b[1;32m   4071\u001b[0m \u001b[38;5;66;03m# GH#45316 Return view if key is not duplicated\u001b[39;00m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;66;03m# Only use drop_duplicates with duplicates for performance\u001b[39;00m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m-> 4074\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_unique\u001b[49m\n\u001b[1;32m   4075\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4076\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdrop_duplicates(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   4077\u001b[0m ):\n\u001b[1;32m   4078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_cache(key)\n\u001b[1;32m   4080\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cosc426/lib/python3.12/site-packages/pandas/core/indexes/base.py:2346\u001b[0m, in \u001b[0;36mIndex.is_unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_unique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2316\u001b[0m \u001b[38;5;124;03m    Return if the index has unique values.\u001b[39;00m\n\u001b[1;32m   2317\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_unique\u001b[49m\n",
      "File \u001b[0;32mindex.pyx:266\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.is_unique.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:271\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._do_unique_check\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:333\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._ensure_mapping_populated\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7115\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import lux\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your JSON files\n",
    "json_files = [\n",
    "    \"test_processed_faculty_stats_data.json\",\n",
    "    \"test_processed_crossref_article_stats_data.json\",\n",
    "    \"test_processed_category_data.json\",\n",
    "    \"test_processed_crossref_article_stats_obj_data.json\",\n",
    "]\n",
    "\n",
    "# Initialize empty DataFrames\n",
    "df_faculty_stats = pd.DataFrame()\n",
    "df_article_stats = pd.DataFrame()\n",
    "df_category_stats = pd.DataFrame()\n",
    "\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Process faculty stats data\n",
    "    if \"faculty_stats\" in str(data):\n",
    "        # Flatten the JSON structure\n",
    "        for category, details in data.items():\n",
    "            faculty_stats = details.get(\"faculty_stats\", {})\n",
    "            df_faculty = pd.json_normalize(faculty_stats, sep=\"_\").T.reset_index()\n",
    "            df_faculty.columns = [\"faculty_name\"] + list(df_faculty.iloc[0, 1:])\n",
    "            df_faculty = df_faculty[1:].assign(category=category)\n",
    "            df_faculty_stats = pd.concat(\n",
    "                [df_faculty_stats, df_faculty], ignore_index=True\n",
    "            )\n",
    "\n",
    "    # Process article stats data (by category)\n",
    "    elif \"article_citation_map\" in str(data):\n",
    "        for category, details in data.items():\n",
    "            article_citation_map = details.get(\"article_citation_map\", {})\n",
    "            df_articles = pd.json_normalize(\n",
    "                article_citation_map, sep=\"_\"\n",
    "            ).T.reset_index()\n",
    "            df_articles.columns = [\"doi\"] + list(df_articles.iloc[0, 1:])\n",
    "            df_articles = df_articles[1:].assign(category=category)\n",
    "            df_article_stats = pd.concat(\n",
    "                [df_article_stats, df_articles], ignore_index=True\n",
    "            )\n",
    "\n",
    "    # Process article stats data (by DOI)\n",
    "    elif any(key.startswith(\"10.\") for key in data.keys()):\n",
    "        df_articles_obj = pd.json_normalize(data, sep=\"_\").T.reset_index()\n",
    "        df_articles_obj.columns = [\"doi\"] + list(df_articles_obj.iloc[0, 1:])\n",
    "        df_articles_obj = df_articles_obj[1:]\n",
    "        df_article_stats = pd.concat(\n",
    "            [df_article_stats, df_articles_obj], ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Process category data\n",
    "    elif \"doi_list\" in str(data):\n",
    "        df_category = pd.json_normalize(data, sep=\"_\").T.reset_index()\n",
    "        df_category.columns = [\"category\"] + list(df_category.iloc[0, 1:])\n",
    "        df_category = df_category[1:]\n",
    "        df_category_stats = pd.concat(\n",
    "            [df_category_stats, df_category], ignore_index=True\n",
    "        )\n",
    "\n",
    "# Now, we need to merge all DataFrames into one comprehensive DataFrame\n",
    "\n",
    "# Merge article stats with faculty stats on 'doi'\n",
    "# First, ensure that 'doi' is present in both DataFrames\n",
    "if not df_article_stats.empty and \"doi\" in df_article_stats.columns:\n",
    "    # Expand faculty members in df_article_stats\n",
    "    df_article_stats[\"faculty_members\"] = df_article_stats[\"faculty_members\"].apply(\n",
    "        lambda x: x if isinstance(x, list) else []\n",
    "    )\n",
    "    df_article_expanded = df_article_stats.explode(\"faculty_members\").rename(\n",
    "        columns={\"faculty_members\": \"faculty_name\"}\n",
    "    )\n",
    "else:\n",
    "    df_article_expanded = pd.DataFrame()\n",
    "\n",
    "# Merge with faculty stats\n",
    "if not df_article_expanded.empty and not df_faculty_stats.empty:\n",
    "    df_merged = pd.merge(\n",
    "        df_article_expanded,\n",
    "        df_faculty_stats,\n",
    "        on=\"faculty_name\",\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_article\", \"_faculty\"),\n",
    "    )\n",
    "else:\n",
    "    df_merged = pd.DataFrame()\n",
    "\n",
    "# Merge with category stats\n",
    "if not df_merged.empty and not df_category_stats.empty:\n",
    "    df_final = pd.merge(\n",
    "        df_merged,\n",
    "        df_category_stats,\n",
    "        left_on=\"category_article\",\n",
    "        right_on=\"category\",\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"\", \"_category\"),\n",
    "    )\n",
    "else:\n",
    "    df_final = df_merged if not df_merged.empty else df_category_stats\n",
    "\n",
    "# Drop duplicates\n",
    "df_final = df_final.drop_duplicates()\n",
    "\n",
    "# Output the final DataFrame\n",
    "print(\"Final Merged DataFrame:\")\n",
    "print(df_final.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosc426",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

                ```

                src/data/core/output_files/vis_2.py:
                ```
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import json
from collections import defaultdict

# Read data
with open("test_processed_faculty_stats_data.json", "r") as f:
    faculty_data = json.load(f)
with open("test_processed_article_stats_data.json", "r") as f:
    article_data = json.load(f)
with open("test_processed_category_data.json", "r") as f:
    category_data = json.load(f)

# Style selection
for i, style in enumerate(plt.style.available):
    print(f"{i}: {style}")
style = input("Select a style: ")
plt.style.use(plt.style.available[int(style)])

# 1. Faculty Collaboration Network
plt.figure(figsize=(10, 10))  # Adjusted for MacBook M1 Air screen
G = nx.Graph()

# Create collaboration mapping
collaborations = defaultdict(int)
faculty_citations = defaultdict(int)
faculty_dept = {}

for category in faculty_data.values():
    for faculty, stats in category.get("faculty_stats", {}).items():
        faculty_citations[faculty] = stats["total_citations"]
        faculty_dept[faculty] = stats["department_affiliations"].split(",")[0]

        # Find collaborations through shared papers
        for paper_title in stats["citation_map"]["article_citation_map"].keys():
            for other_faculty, other_stats in category.get("faculty_stats", {}).items():
                if (
                    faculty != other_faculty
                    and paper_title
                    in other_stats["citation_map"]["article_citation_map"]
                ):
                    collaborations[
                        (min(faculty, other_faculty), max(faculty, other_faculty))
                    ] += 1

# Create network
for (faculty1, faculty2), weight in collaborations.items():
    G.add_edge(faculty1, faculty2, weight=weight)

# Add nodes with citation counts
for faculty in faculty_citations:
    G.add_node(
        faculty, citations=faculty_citations[faculty], dept=faculty_dept[faculty]
    )

# Draw network
pos = nx.spring_layout(G, k=3, iterations=50)  # Increased k for more spread
node_sizes = [
    max(200, G.nodes[node]["citations"] * 10) for node in G.nodes()
]  # Minimum size of 200
edge_weights = [
    max(1, G.edges[edge]["weight"]) for edge in G.edges()
]  # Minimum weight of 1

# Color nodes by department
dept_colors = {
    dept: plt.cm.tab20(i) for i, dept in enumerate(set(faculty_dept.values()))
}
node_colors = [dept_colors[G.nodes[node]["dept"]] for node in G.nodes()]

# Create legend for departments
legend_elements = [
    plt.Line2D(
        [0], [0], marker="o", color="w", markerfacecolor=color, label=dept, markersize=8
    )
    for dept, color in dept_colors.items()
]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7)
nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.3)
nx.draw_networkx_labels(G, pos, font_size=6)

plt.title(
    "Research Collaboration Network\n\n"
    + "Node size represents citation impact\n"
    + "Edge thickness shows number of collaborations\n"
    + "Colors indicate department affiliation",
    pad=20,
    fontsize=10,
)
plt.legend(handles=legend_elements, loc="center left", bbox_to_anchor=(1, 0.5))
plt.axis("off")
plt.tight_layout()
plt.show()

# 2. Department Impact Matrix
departments = defaultdict(lambda: {"papers": 0, "citations": 0, "faculty": set()})

for category in faculty_data.values():
    for faculty, stats in category.get("faculty_stats", {}).items():
        dept = stats["department_affiliations"].split(",")[0]
        departments[dept]["papers"] += stats["article_count"]
        departments[dept]["citations"] += stats["total_citations"]
        departments[dept]["faculty"].add(faculty)

# Create department metrics DataFrame
dept_metrics = pd.DataFrame(
    {
        "Total Papers": {k: v["papers"] for k, v in departments.items()},
        "Total Citations": {k: v["citations"] for k, v in departments.items()},
        "Faculty Count": {k: len(v["faculty"]) for k, v in departments.items()},
        "Citations per Paper": {
            k: round(v["citations"] / v["papers"], 2) if v["papers"] > 0 else 0
            for k, v in departments.items()
        },
    }
)

# Plot department impact matrix
plt.figure(figsize=(12, 8))  # Adjusted for MacBook M1 Air screen
dept_metrics_normalized = (dept_metrics - dept_metrics.mean()) / dept_metrics.std()
sns.heatmap(
    dept_metrics_normalized.sort_values("Total Citations", ascending=False).head(15),
    cmap="RdYlBu",
    center=0,
    annot=dept_metrics.sort_values("Total Citations", ascending=False).head(
        15
    ),  # Show actual values
    fmt=".0f",
)  # No decimal places for actual values

plt.title(
    "Department Research Impact Comparison\n\n"
    + "Colors show normalized scores (standard deviations from mean)\n"
    + "Numbers show actual values\n"
    + "Departments sorted by total citations",
    pad=20,
    fontsize=14,
)
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

# 3. Citation Distribution
plt.figure(figsize=(12, 6))  # Adjusted for MacBook M1 Air screen
citations = [
    stats["total_citations"]
    for category in faculty_data.values()
    for stats in category.get("faculty_stats", {}).values()
]

sns.histplot(citations, bins=30, log_scale=True)
plt.title(
    "Distribution of Faculty Citations\n\n"
    + "Shows how many faculty members achieve different citation counts\n"
    + "Log scale used to show full range of impact",
    pad=20,
    fontsize=14,
)
plt.xlabel("Citation Count (log scale)")
plt.ylabel("Number of Faculty Members")

# Add summary statistics
plt.text(
    0.02,
    0.98,
    f"Total Faculty: {len(citations)}\n"
    + f"Median Citations: {int(pd.Series(citations).median())}\n"
    + f"Mean Citations: {int(pd.Series(citations).mean())}\n"
    + f"Max Citations: {int(max(citations))}",
    transform=plt.gca().transAxes,
    verticalalignment="top",
    bbox=dict(boxstyle="round", facecolor="white", alpha=0.8),
)

plt.tight_layout()
plt.show()

                ```

                src/data/core/output_files/vis_cat_stats.py:
                ```
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from datetime import datetime
import json

# Read the data
with open("test_processed_category_data.json", "r") as f:
    category_data = json.load(f)
with open("test_processed_crossref_article_stats_obj_data.json", "r") as f:
    article_data = json.load(f)

# Style selection
for i, style in enumerate(plt.style.available):
    print(f"{i}: {style}")
style = input("Select a style: ")
plt.style.use(plt.style.available[int(style)])

# 1. Research Impact Timeline
plt.figure(figsize=(12, 6))
dates = []
citations = []
titles = []

for doi, article in article_data.items():
    date = datetime.strptime(article["date_published_online"], "%Y-%m-%d")
    dates.append(date)
    citations.append(article["tc_count"])
    titles.append(
        article["title"] if isinstance(article["title"], str) else article["title"][0]
    )

plt.scatter(dates, citations, s=100, alpha=0.6)
plt.xlabel("Publication Date")
plt.ylabel("Citations")
plt.title("Research Impact Over Time")
# Annotate points with high citations
for i, (date, cite, title) in enumerate(zip(dates, citations, titles)):
    if cite > 20:  # Only label highly cited papers
        plt.annotate(
            title[:30] + "...",
            (date, cite),
            xytext=(10, 10),
            textcoords="offset points",
        )
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 2. Category Network Graph
plt.figure(figsize=(12, 6))
G = nx.Graph()

# Add nodes (categories)
for category in category_data:
    G.add_node(category, size=category_data[category]["tc_count"])

# Add edges (shared papers)
for cat1 in category_data:
    for cat2 in category_data:
        if cat1 < cat2:  # Avoid duplicates
            shared_papers = len(
                set(category_data[cat1]["doi_list"])
                & set(category_data[cat2]["doi_list"])
            )
            if shared_papers > 0:
                G.add_edge(cat1, cat2, weight=shared_papers)

# Draw the network
pos = nx.spring_layout(G, k=1, iterations=50)
node_sizes = [G.nodes[node]["size"] * 20 for node in G.nodes()]
edge_weights = [G.edges[edge]["weight"] * 2 for edge in G.edges()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, alpha=0.7)
nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.4)
nx.draw_networkx_labels(G, pos, font_size=8)

plt.title(
    "Research Category Network\n(Node size = total citations, Edge thickness = shared papers)"
)
plt.axis("off")
plt.tight_layout()
plt.show()

# 3. Impact Distribution
plt.figure(figsize=(12, 6))
impact_data = pd.DataFrame.from_dict(category_data, orient="index")
impact_data = impact_data.sort_values("citation_average", ascending=True)

# Create horizontal bar chart
plt.barh(range(len(impact_data)), impact_data["citation_average"])
plt.yticks(range(len(impact_data)), impact_data.index, fontsize=8)
plt.xlabel("Average Citations per Paper")
plt.title("Research Impact by Category")
plt.tight_layout()
plt.show()

                ```

        src/other/AbstractCategoryMap.py:
        ```
import os
from openpyxl import Workbook
import sys

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

sys.path.append(project_root)

from GeneralUtilities.file_ops.file_ops import FileOps
from typing import Tuple
from utilities import Utilities
from enums import AttributeTypes
from warning_manager import WarningManager  # for type hinting


class AbstractCategoryMap:
    def __init__(
        self,
        *,
        utilities_obj: Utilities,
        warning_manager: WarningManager,
        dir_path: str,
    ):
        self.utilities = utilities_obj
        self.warning_manager = warning_manager
        self.dir_path = dir_path
        self.file_ops = FileOps(output_dir=".")
        self.results = self.map_abstract_categories(dir_path=self.dir_path)
        self.file_ops.write_json("for_jensen.json", self.results)

    def map_abstract_categories(self, *, dir_path: str):
        results = {}
        for filename in os.listdir(dir_path):
            file_path = os.path.join(dir_path, filename)
            if not os.path.isfile(file_path):
                self.warning_manager.log_warning(
                    "File Processing", f"File not found: {file_path}"
                )
                continue

            file_content = self.file_ops.read_file(file_path)

            attributes = self.utilities.get_attributes(
                file_content,
                [
                    AttributeTypes.TITLE,
                    AttributeTypes.ABSTRACT,
                    AttributeTypes.WC_PATTERN,
                    AttributeTypes.AUTHOR,
                    AttributeTypes.DEPARTMENT,
                ],
            )

            title = (
                attributes[AttributeTypes.TITLE][1]
                if attributes[AttributeTypes.TITLE][0]
                else None
            )
            abstract = (
                attributes[AttributeTypes.ABSTRACT][1]
                if attributes[AttributeTypes.ABSTRACT][0]
                else None
            )
            categories = (
                attributes[AttributeTypes.WC_PATTERN][1]
                if attributes[AttributeTypes.WC_PATTERN][0]
                else []
            )
            authors = (
                attributes[AttributeTypes.AUTHOR][1]
                if attributes[AttributeTypes.AUTHOR][0]
                else []
            )
            department = (
                attributes[AttributeTypes.DEPARTMENT][1]
                if attributes[AttributeTypes.DEPARTMENT][0]
                else []
            )

            if not title:
                self.warning_manager.log_warning(
                    "Attribute Extraction", f"Title not found for file: {file_path}"
                )
            if not abstract:
                self.warning_manager.log_warning(
                    "Attribute Extraction", f"Abstract not found for file: {file_path}"
                )
            else:
                results[title] = {
                    "abstract": abstract,
                    "categories": categories,
                    "authors": authors,
                    "department": department,
                }

        return results

    def get_results(self):
        return self.results

    def get_taxonomy(self):
        taxonomy_path_json = "../../TextAnalysis/Taxonomy.json"
        try:
            taxonomy = self.file_ops.read_json(taxonomy_path_json)
            return taxonomy
        except FileNotFoundError:
            self.warning_manager.log_warning(
                "Taxonomy", f"Taxonomy file not found: {taxonomy_path_json}"
            )
            return {}

    def get_file_ops_obj(self):
        return self.file_ops

    def write_to_excel(self, results, filename: str):
        wb = Workbook()
        ws = wb.active
        ws.title = "Abstract Categories"

        # Write headers
        headers = [
            "Title",
            "Abstract",
            "Categories",
            "Authors",
            "Department",
            "Upper Level Categories",
            "Mid-Level Categories",
            "Themes",
        ]
        for col, header in enumerate(headers, start=1):
            ws.cell(row=1, column=col, value=header)

        # Write data
        for row, (title, data) in enumerate(results.items(), start=2):
            ws.cell(row=row, column=1, value=title)
            ws.cell(row=row, column=2, value=data.get("abstract", ""))
            ws.cell(row=row, column=3, value=", ".join(data.get("categories", [])))
            ws.cell(row=row, column=4, value=", ".join(data.get("authors", [])))

            # Handle 'department' which might be a string or a list
            department = data.get("department", [])
            if isinstance(department, list):
                department = ", ".join(department)
            ws.cell(row=row, column=5, value=department)

            ws.cell(
                row=row,
                column=6,
                value=", ".join(data.get("Upper Level Categories", [])),
            )
            ws.cell(
                row=row, column=7, value=", ".join(data.get("Mid-Level Categories", []))
            )

            # Ensure all items in 'Themes' are strings
            themes = data.get("Themes", [])
            themes = [str(theme) for theme in themes]
            ws.cell(row=row, column=8, value=", ".join(themes))

        try:
            wb.save(filename)
        except Exception as e:
            self.warning_manager.log_warning(
                "Excel Writing", f"Error writing to Excel file {filename}: {str(e)}"
            )


if __name__ == "__main__":
    abstract_category_map = AbstractCategoryMap(
        Utilities_obj=Utilities(), dir_path="./split_files"
    )

    results = abstract_category_map.get_results()
    taxonomy = abstract_category_map.get_taxonomy()

    for key, value in results.items():
        taxonomy_obj = taxonomy[key]
        results[key]["Upper Level Categories"] = taxonomy_obj["Upper Level Categories"]
        results[key]["Mid-Level Categories"] = taxonomy_obj["Mid-Level Categories"]
        results[key]["Themes"] = taxonomy_obj["Themes"]

    file_ops = abstract_category_map.get_file_ops_obj()
    file_ops.write_json("FINAL_ForJensen.json", results)

    # Write to Excel
    abstract_category_map.write_to_excel(results, "CategoryMapping.xlsx")

        ```

        src/other/REAL_verification.py:
        ```
"""
This script compares publication titles from Digital Measures (DM) and Web of Science (WoS) datasets. It loads titles from both sources, compares them using exact and fuzzy matching, and provides statistics on the overlap and differences between the two datasets. The script also includes utility functions for data processing and visualization.
"""

import os
import json
import glob
import re
from datasketch import MinHash, MinHashLSH
from typing import Set, Dict, List, Any, Tuple
import requests
from wos_classification import WosClassification
from strategy_factory import StrategyFactory
from warning_manager import WarningManager
import time
from tqdm import tqdm
import sys
from urllib.parse import quote

#  Stock Market Reactions to Store-in-store Agreements
# https://api.crossref.org/works?query.title=Stock%20Market%20Reactions%20to%20Store-in-store%20Agreements


def query_crossref(
    title: str, max_retries: int = 3, delay: int = 2
) -> Tuple[Dict[str, Any], float]:
    # Convert the title to lowercase except for the first letter of the first word
    words = title.split()
    if words:
        title = (
            words[0].capitalize() + " " + " ".join(word.lower() for word in words[1:])
        )

    # Hardcoded parameters for the specific title

    # Properly encode the title, author, and journal for a URL
    encoded_title = quote(title)

    # Construct the query URL with additional parameters
    url = f"https://api.crossref.org/works?query.title={encoded_title}"
    print(f"Querying Crossref API for title: {title}")
    print(f"URL: {url}")
    input("Press Enter to continue...")

    start_time = time.time()
    for attempt in range(max_retries):
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            # print(f"Response: {data['message']['items'][0]['title']}")
            # input("Press Enter to continue...")
            # print(f"Response: {json.dumps(data, indent=4)}")
            # Check if 'items' list is present in the response
            if "message" in data and "items" in data["message"]:
                items = data["message"]["items"]
                if items:
                    with open("items_test.json", "w") as f:
                        json.dump(items[0], f, indent=4)
                    # print(f"Items: {json.dumps(items[0], indent=4)}")
                    # input("Press Enter to continue...")
                    return (
                        json.dumps(items[0], indent=4),
                        time.time() - start_time,
                    )  # Return the first item and query time
            return (
                {},
                time.time() - start_time,
            )  # Return an empty dict and query time if no items are found
        else:
            print(
                f"Attempt {attempt + 1} failed with status code {response.status_code}. Retrying in {delay} seconds..."
            )
            time.sleep(delay)

    print(f"Failed to retrieve data for title: {title} after {max_retries} attempts.")
    return {}, time.time() - start_time


def save_results_to_file(results: List[Dict[str, Any]], file_path: str) -> None:
    # Read existing data
    # print(f"Results: {results}")
    # input("Press Enter to continue...")
    # print(f"Saving results to file: {file_path}")
    # input("Press Enter to continue...")
    if os.path.exists(file_path):
        with open(file_path, "r") as file:
            try:
                existing_data = json.load(file)
            except json.JSONDecodeError:
                existing_data = []
    else:
        existing_data = []
    existing_data_len = len(existing_data)
    print(f"Existing data length before: {existing_data_len}")
    # Extract items from results and append to existing data
    # new_items = []
    for result in results:
        existing_data.append(json.loads(result))

    existing_data_len_after = len(existing_data)
    print(f"Existing data length after: {existing_data_len_after}")
    print(f"Difference: {existing_data_len_after - existing_data_len}")

    with open(file_path, "w") as file:
        json.dump(existing_data, file, indent=4)


def load_titles_from_reformatted_files(directory_path: str) -> Set[str]:
    """
    Load titles from JSON files in the specified directory.

    Args:
        directory_path (str): Path to the directory containing JSON files.

    Returns:
        set: A set of unique titles from all JSON files in the directory.
    """
    titles_set: Set[str] = set()
    json_files: List[str] = glob.glob(os.path.join(directory_path, "*.json"))
    for file_path in tqdm(
        json_files, desc="Loading titles from reformatted files", position=0, leave=True
    ):
        with open(file_path, "r") as f:
            data: Dict[str, Any] = json.load(f)
            titles_set.update(data)

    return titles_set


def load_titles_from_processed_category_data(file_path: str) -> Set[str]:
    """
    Load titles from a processed category data JSON file.

    Args:
        file_path (str): Path to the processed category data JSON file.

    Returns:
        set: A set of unique titles from the processed category data.
    """
    with open(file_path, "r") as f:
        data: Dict[str, Any] = json.load(f)
    titles_set: Set[str] = set()
    for category, info in tqdm(
        data.items(),
        desc="Loading titles from processed category data",
        position=0,
        leave=True,
    ):
        titles_set.update(info["titles"])
    return titles_set


def aggressive_regularize(s: str) -> str:
    """
    Aggressively regularize a string by removing all non-alphanumeric characters and converting to lowercase.

    Args:
        s (str): The input string to regularize.

    Returns:
        str: The regularized string.
    """
    return re.sub(r"\W+", "", s.lower())


def process_titles(titles: Set[str]) -> Set[str]:
    """
    Process a set of titles by applying aggressive regularization to each title.

    Args:
        titles (set): A set of titles to process.

    Returns:
        set: A set of processed titles.
    """
    return set(
        aggressive_regularize(title)
        for title in tqdm(titles, desc="Processing titles", position=0, leave=True)
    )


def compare_datasets(
    dm_titles: Set[str], wos_titles: Set[str]
) -> Tuple[Set[str], Set[str], Set[str]]:
    """
    Compare two sets of titles and find exact matches, titles only in DM, and titles only in WoS.

    Args:
        dm_titles (set): Set of titles from Digital Measures.
        wos_titles (set): Set of titles from Web of Science.

    Returns:
        tuple: A tuple containing three sets:
            - exact_matches: Titles that match exactly between DM and WoS.
            - only_in_dm: Titles that are only in DM.
            - only_in_wos: Titles that are only in WoS.
    """
    dm_regularized: Set[str] = process_titles(dm_titles)
    wos_regularized: Set[str] = process_titles(wos_titles)

    exact_matches: Set[str] = dm_regularized.intersection(wos_regularized)
    only_in_dm: Set[str] = dm_regularized - wos_regularized
    only_in_wos: Set[str] = wos_regularized - dm_regularized

    return exact_matches, only_in_dm, only_in_wos


def find_similar_titles(
    dm_titles: Set[str],
    wos_titles: Set[str],
    threshold: float = 0.8,
    crossref: bool = False,
) -> Set[str]:
    """
    Find similar titles between DM and WoS datasets using MinHash LSH.

    Args:
        dm_titles (set): Set of titles from Digital Measures.
        wos_titles (set): Set of titles from Web of Science.
        threshold (float): Similarity threshold for considering titles as similar (default: 0.8).

    Returns:
        set: A set of similar titles found in both datasets.
    """
    lsh: MinHashLSH = MinHashLSH(threshold=threshold, num_perm=128)
    index_prefix: str = "wos" if not crossref else "crossref"

    wos_titles_list: List[str] = list(wos_titles)
    for idx, title in enumerate(
        tqdm(wos_titles_list, desc="Indexing WoS titles", position=0, leave=True)
    ):
        m: MinHash = MinHash(num_perm=128)
        for d in aggressive_regularize(title):
            m.update(d.encode("utf8"))
        lsh.insert(f"{index_prefix}_{idx}", m)

    similar_titles: Set[str] = set()
    for dm_title in tqdm(
        dm_titles, desc="Finding similar titles", position=0, leave=True
    ):
        m: MinHash = MinHash(num_perm=128)
        for d in aggressive_regularize(dm_title):
            m.update(d.encode("utf8"))
        result: List[str] = lsh.query(m)
        if result:
            similar_titles.add(dm_title)

    return similar_titles


def display_titles_only_in_dm(
    final_only_in_dm: Set[str], dm_titles_set: Set[str], wos_or_crossref_str: str
) -> None:
    """
    Display titles that are only present in the Digital Measures dataset.

    Args:
        final_only_in_dm (set): Set of regularized titles only in DM after comparison.
        dm_titles_set (set): Original set of DM titles.

    Returns:
        None
    """
    print(
        f"\nThe titles that Digital Measures has but {wos_or_crossref_str} does not, e.g., the titles we are missing:"
    )
    for title in tqdm(
        dm_titles_set, desc="Displaying titles only in DM", position=0, leave=True
    ):
        if aggressive_regularize(title) in final_only_in_dm:
            print(f"- {title}")


def print_fancy_box(message: str) -> None:
    """
    Print a message in a fancy box with centered text and padding.

    Args:
        message (str): The message to be displayed in the fancy box.

    Returns:
        None
    """
    lines: List[str] = message.split("\n")
    max_line_length: int = max(len(line) for line in lines)
    box_width: int = max_line_length + 10  # Add extra padding

    print("\nâ•”" + "â•" * box_width + "â•—")
    print("â•‘" + " " * box_width + "â•‘")  # Add empty line for top padding

    for line in lines:
        padding: int = (box_width - len(line)) // 2
        print(
            "â•‘" + " " * padding + line + " " * (box_width - len(line) - padding) + "â•‘"
        )

    print("â•‘" + " " * box_width + "â•‘")  # Add empty line for bottom padding
    print("â•š" + "â•" * box_width + "â•\n")

    with open(
        os.path.join(
            os.path.dirname(__file__),
            "static_output_files",
            "REAL_verification_results_SUMMARY.txt",
        ),
        "w",
    ) as f:
        print("\nâ•”" + "â•" * box_width + "â•—", file=f)
        print("â•‘" + " " * box_width + "â•‘", file=f)  # Add empty line for top padding

        for line in lines:
            padding: int = (box_width - len(line)) // 2
            print(
                "â•‘"
                + " " * padding
                + line
                + " " * (box_width - len(line) - padding)
                + "â•‘",
                file=f,
            )

        print("â•‘" + " " * box_width + "â•‘", file=f)  # Add empty line for bottom padding
        print("â•š" + "â•" * box_width + "â•\n", file=f)


def main(crossref: bool = False, second_run: bool = False) -> None:
    """
    Main function to execute the comparison between Digital Measures and Web of Science datasets.

    This function loads titles from both datasets, performs comparisons, finds similar titles,
    calculates statistics, and displays the results.

    Args:
        None

    Returns:
        None
    """
    if crossref:
        print(f"Analysis for Crossref API\n\n")
    else:
        print(f"\n\nAnalysis for Web of Science Export\n\n")

    reformatted_files_dir: str = os.path.join(
        os.path.dirname(__file__), "reformattedFiles"
    )
    processed_category_data_file: str = os.path.join(
        os.path.dirname(__file__), "static_output_files", "processed_category_data.json"
    )
    test_processed_category_data_file: str = os.path.join(
        os.path.dirname(__file__), "test_processed_category_data.json"
    )

    wos_or_crossref_str: str = (
        "Web of Science Export" if not crossref else "Crossref API"
    )

    dm_titles_set: Set[str] = load_titles_from_reformatted_files(reformatted_files_dir)
    wos_titles_set: Set[str] = set()
    if crossref:
        wos_titles_set: Set[str] = load_titles_from_processed_category_data(
            test_processed_category_data_file
        )
    else:
        wos_titles_set: Set[str] = load_titles_from_processed_category_data(
            processed_category_data_file
        )

    print(f"Initial Digital Measures titles: {len(dm_titles_set)}")
    print(f"Initial {wos_or_crossref_str} titles: {len(wos_titles_set)}")

    exact_matches, only_in_dm, only_in_wos = compare_datasets(
        dm_titles_set, wos_titles_set
    )

    print("\nInitial comparison:")
    print(f"Exact matches: {len(exact_matches)}")
    print(
        f"Only in Digital Measures: {len(only_in_dm)} = {len(dm_titles_set)} - {len(exact_matches)}"
    )
    print(
        f"Only in {wos_or_crossref_str}: {len(only_in_wos)} = {len(wos_titles_set)} - {len(exact_matches)}"
    )
    print(
        f"\n\nQuerying Crossref API for titles in Digital Measures that are not present in the initial {wos_or_crossref_str} dataset..."
    )

    print("\nSearching for similar titles...")
    similar_titles: Set[str] = find_similar_titles(
        only_in_dm, only_in_wos, threshold=0.8, crossref=crossref
    )
    print(f"Similar titles found: {len(similar_titles)}")

    final_common_set: Set[str] = exact_matches.union(
        set(aggressive_regularize(title) for title in similar_titles)
    )
    final_only_in_dm: Set[str] = only_in_dm - set(
        aggressive_regularize(title) for title in similar_titles
    )
    final_only_in_wos: Set[str] = only_in_wos - set(
        aggressive_regularize(title) for title in similar_titles
    )

    print("\nFinal results:")
    print(
        f"Final common set: {len(final_common_set)} = {len(exact_matches)} + {len(similar_titles)}"
    )
    print(
        f"Final only in Digital Measures: {len(final_only_in_dm)} = {len(only_in_dm)} - {len(similar_titles)}"
    )
    print(
        f"Final only in {wos_or_crossref_str}: {len(final_only_in_wos)} = {len(only_in_wos)} - {len(similar_titles)}"
    )

    total_unique: int = (
        len(final_common_set) + len(final_only_in_dm) + len(final_only_in_wos)
    )
    print(
        f"\nTotal unique titles: {total_unique} = {len(final_common_set)} + {len(final_only_in_dm)} + {len(final_only_in_wos)}"
    )

    dm_exclusive_percentage: float = (len(final_only_in_dm) / len(dm_titles_set)) * 100
    wos_exclusive_percentage: float = (
        len(final_only_in_wos) / len(wos_titles_set)
    ) * 100

    print(
        f"\nPercentage of Digital Measures titles not in {wos_or_crossref_str}: {dm_exclusive_percentage:.2f}% = ({len(final_only_in_dm)} / {len(dm_titles_set)}) * 100"
    )
    print(
        f"Percentage of {wos_or_crossref_str} titles not in Digital Measures: {wos_exclusive_percentage:.2f}% = ({len(final_only_in_wos)} / {len(wos_titles_set)}) * 100"
    )

    percent_wos_has_from_dm: float = 100 - (
        (len(final_only_in_dm) / len(dm_titles_set)) * 100
    )
    print(
        f"Percent of Digital Measures titles that {wos_or_crossref_str} has: {percent_wos_has_from_dm:.2f}%"
    )

    display_titles_only_in_dm(final_only_in_dm, dm_titles_set, wos_or_crossref_str)

    summary = (
        "SUMMARY\n\n"
        f"The {wos_or_crossref_str} export technique is only missing {dm_exclusive_percentage:.2f}% "
        f"of the titles that Digital Measures has.\n\n"
        f"This means that the {wos_or_crossref_str} export technique only lacks 1 out of every "
        f"{1 / dm_exclusive_percentage * 100:.0f} titles that DM has."
    )
    print_fancy_box(summary)

    if crossref and len(final_only_in_dm) > 0:
        # Query Crossref API for titles in Digital Measures that are not present in the initial {wos_or_crossref_str} dataset
        crossref_results = []
        estimated_time_per_query = None

        title_to_exclude = "Reporting Cash Receipts over $10,000"
        dm_titles_set = {
            title
            for title in dm_titles_set
            if aggressive_regularize(title) != aggressive_regularize(title_to_exclude)
        }
        final_only_in_dm = {
            title
            for title in final_only_in_dm
            if aggressive_regularize(title) != aggressive_regularize(title_to_exclude)
        }

        # Inform the user about the exclusion
        print(
            f"The paper '{title_to_exclude}' is published in the Tax Adviser, which does not provide a DOI. It is considered a professional magazine/trade publication rather than an academic journal."
        )

        with tqdm(
            total=len(final_only_in_dm),
            desc="Querying Crossref API",
            position=0,
            leave=True,
        ) as pbar:
            if not second_run:
                if len(final_only_in_dm) > 0:
                    for i, title in enumerate(final_only_in_dm):
                        # Find the original title from dm_titles_set that matches the regularized title
                        original_title = next(
                            (
                                t
                                for t in dm_titles_set
                                if aggressive_regularize(t) == title
                            ),
                            None,
                        )
                        if original_title:
                            result, query_time = query_crossref(original_title)
                            if result:
                                crossref_results.append(result)

                        if i == 0:
                            estimated_time_per_query = query_time * (
                                len(final_only_in_dm) - i + 1
                            )

                        pbar.update(1)
                        if estimated_time_per_query:
                            pbar.set_postfix(
                                {
                                    "Estimated time left": f"{(len(final_only_in_dm) - pbar.n) * estimated_time_per_query:.0f}s"
                                }
                            )

                    # Save crossref results to a file in 'input_files' directory via appending to the current file there
                    crossref_file_path = os.path.join(
                        os.path.dirname(__file__),
                        "input_files",
                        "2017-2024-paper-doi-list.json",
                    )
                    save_results_to_file(crossref_results, crossref_file_path)

                    # Run WosClassification with the new data
                    strategy_factory = StrategyFactory()
                    warning_manager = WarningManager()

                    input_dir_path = "./input_files"
                    output_dir_path = "./crossref_split_files"
                    input_dir_path = os.path.expanduser(input_dir_path)
                    output_dir_path = os.path.expanduser(output_dir_path)
                    # Instantiate the orchestrator class
                    wos_classifiction = WosClassification(
                        input_dir_path=input_dir_path,
                        output_dir_path=output_dir_path,
                        strategy_factory=strategy_factory,
                        warning_manager=warning_manager,
                        crossref_run=True,
                        make_files=True,
                        extend=True,
                    )

                    print(
                        f"second_run: {second_run}, crossref_results: {len(crossref_results)}"
                    )
                    input("Press Enter to continue...")

                    # call main again to run the analysis on the updated data
                    # Only call main again if there are new results to process and it's not the second run
                    if crossref_results and not second_run:
                        print("Calling main again for the second run.")
                        main(crossref=True, second_run=True)
                    else:
                        print("Stopping recursion.")


if __name__ == "__main__":
    crossref: bool = True
    main(crossref=crossref)
    main(crossref=False)

        ```

        src/other/__init__.py:
        ```

        ```

        src/other/_faculty_set_postprocessor.py:
        ```
from __future__ import annotations

import logging
import os
import random
from dataclasses import dataclass, field
from typing import Any, Dict, TYPE_CHECKING, List, Set, Tuple

from academic_metrics.configs import (
    configure_logging,
    DEBUG,
)

if TYPE_CHECKING:
    from academic_metrics.dataclass_models import CategoryInfo


class FacultyPostprocessor:
    """
    A class responsible for processing and standardizing faculty data across different categories.

    This class provides methods to extract faculty sets from category data, remove near-duplicate names,
    and standardize faculty names to ensure consistency across categories. It utilizes MinHash for estimating
    the similarity between names to effectively identify and remove duplicates. Additionally, it maintains
    a dictionary of name variations to track the most frequent spelling variations of each name.

    Attributes:
        temp_dict (dict): Temporary storage for faculty names and their occurrences.
        faculty_occurence_dict (dict): Tracks occurrences of faculty names across categories.
        processed_sets_list (list): Stores processed faculty sets after deduplication and standardization.
        minhash_util (MinHashUtility): Utility for generating MinHash signatures and comparing them.
        name_variations (dict): Stores NameVariation objects for each normalized faculty name.

    Methods:
        get_temp_dict(): Returns the temporary dictionary containing faculty names and their occurrences.
        extract_faculty_sets(category_dict): Extracts faculty sets from each CategoryInfo object in the provided dictionary.
        remove_near_duplicates(category_dict): Processes each CategoryInfo object to remove near-duplicate faculty names and standardize them.
        standardized_data_update(category_dict, standardized_sets): Updates CategoryInfo objects with standardized faculty sets.
        standardize_faculty(category_dict): Standardizes faculty names across all categories based on the most frequent spelling variations.
        remove_update_faculty(category_dict, faculty_sets_list): Removes near-duplicate faculty names within each faculty set.
        duplicate_postprocessor(faculty_set, faculty_sets, similarity_threshold): Processes a set of faculty names to remove near-duplicates.
        process_name_pair(similarity_threshold, most_frequent_variation, name_signatures, to_remove, n1, n2): Compares two names and determines which to keep.
        name_to_remove(most_frequent_variation, n1, n2, n1_normalized, n2_normalized): Determines which of two names to remove based on their variations.
        get_duplicate_utilities(faculty_set, faculty_sets): Generates utilities needed for duplicate removal.
        generate_signatures(faculty_set): Generates MinHash signatures for each name in a faculty set.
        get_most_frequent_name_variation(faculty_sets_list): Maps each normalized name to its most frequent spelling variation.
        standardize_names_across_sets(faculty_sets_list): Standardizes names in faculty sets based on the most frequent name variation.
    """

    def __init__(self):
        self.logger: logging.Logger = configure_logging(
            module_name=__name__,
            log_file_name="faculty_set_postprocessor",
            log_level=DEBUG,
        )

        self.logger.info("Initializing FacultyPostprocessor...")
        self.temp_dict: Dict[str, Any] = {}
        self.faculty_occurence_dict: Dict[str, Any] = {}
        self.processed_sets_list: List[Set[str]] = (
            []
        )  # List to store processed faculty sets

        self.logger.info("Initializing MinHashUtility...")
        self.minhash_util: MinHashUtility = MinHashUtility(
            num_hashes=100
        )  # Initialize MinHashUtility with 100 hash functions

        self.logger.info("Initializing NameVariation dictionary...")
        self.name_variations: Dict[str, NameVariation] = (
            {}
        )  # Dictionary to store NameVariation objects for each normalized name

    def get_temp_dict(self) -> Dict[str, Any]:
        """Returns the temporary dictionary containing faculty names and their occurrences.

        Returns:
            dict: The temporary dictionary containing faculty names and their occurrences.
        """
        self.logger.info("Returning temporary dictionary...")
        return self.temp_dict

    def extract_faculty_sets(
        self, category_dict: Dict[str, CategoryInfo]
    ) -> List[Set[str]]:
        """
        Extracts the faculty attribute from each CategoryInfo object in the provided dictionary.

        This method iterates over a dictionary of CategoryInfo objects and collects the 'faculty' set from each object.
        These faculty sets are typically used for further processing, such as deduplication or analysis.

        Args:
            category_dict (dict): A dictionary where the keys are category identifiers and the values are CategoryInfo objects.

        Returns:
            list[set]: A list containing the faculty set from each CategoryInfo object.
        """
        self.logger.info("Extracting faculty sets...")
        list_of_faculty_sets: List[Set[str]] = [
            category_info.faculty for category_info in category_dict.values()
        ]
        self.logger.info("Faculty sets extracted.")
        return list_of_faculty_sets

    def remove_near_duplicates(
        self, *, category_dict: Dict[str, CategoryInfo]
    ) -> Dict[str, CategoryInfo]:
        """
        Processes each CategoryInfo object to remove near-duplicate faculty names and standardize them across categories.

        This method orchestrates several steps to enhance the integrity and consistency of faculty data:
        1. Extract faculty sets.
        2. Remove near-duplicate names within each faculty set.
        3. Standardize faculty names across all categories.
        4. Update the category data with cleaned and standardized faculty sets.

        Args:
            category_dict (dict): A dictionary where the keys are category identifiers and the values are CategoryInfo objects.

        Returns:
            dict: The updated dictionary with cleaned and standardized faculty names across all CategoryInfo objects.
        """
        self.logger.info("Removing near-duplicates...")
        # Step 1
        faculty_sets_list: List[Set[str]] = self.extract_faculty_sets(
            category_dict=category_dict
        )
        self.logger.info("Faculty sets extracted.")

        # Step 2
        self.logger.info("Removing near-duplicate faculty names...")
        self.remove_update_faculty(category_dict, faculty_sets_list)
        self.logger.info("Near-duplicate faculty names removed.")

        # Step 3
        self.logger.info("Standardizing faculty names...")
        standardized_sets: List[Set[str]] = self.standardize_faculty(category_dict)
        self.logger.info("Faculty names standardized.")

        # Step 4
        self.logger.info("Updating category data...")
        self.standardized_data_update(category_dict, standardized_sets)
        self.logger.info("Category data updated.")
        return category_dict

    def standardized_data_update(
        self, category_dict: Dict[str, CategoryInfo], standardized_sets: List[Set[str]]
    ) -> None:
        """
        Updates the CategoryInfo objects in the dictionary with standardized faculty sets.

        Args:
            category_dict (dict): A dictionary where the keys are category identifiers and the values are CategoryInfo objects.
            standardized_sets (list[set]): A list of sets containing standardized faculty names.

        This method iterates over the category dictionary and updates each CategoryInfo object with the corresponding standardized faculty set.
        """
        self.logger.info("Updating category data...")
        for (_, category_info), standardized_set in zip(
            category_dict.items(), standardized_sets
        ):
            category_info.faculty = standardized_set
        self.logger.info("Category data updated.")

    def standardize_faculty(
        self, category_dict: Dict[str, CategoryInfo]
    ) -> List[Set[str]]:
        """
        Standardizes faculty names across all categories based on the most frequent spelling variations.

        Args:
            category_dict (dict): A dictionary where the keys are category identifiers and the values are CategoryInfo objects.

        Returns:
            list[set]: A list of sets containing the standardized faculty names across all categories.

        This method extracts updated faculty sets after duplicate removal and standardizes names across all sets based on the most frequent global variation.
        """
        self.logger.info("Standardizing faculty names...")
        updated_faculty_sets: List[Set[str]] = self.extract_faculty_sets(
            category_dict=category_dict
        )
        self.logger.info("Faculty sets extracted.")

        self.logger.info("Standardizing faculty names across all sets...")
        standardized_sets: List[Set[str]] = self.standardize_names_across_sets(
            updated_faculty_sets
        )
        self.logger.info("Faculty names standardized.")

        return standardized_sets

    def remove_update_faculty(
        self, category_dict: Dict[str, CategoryInfo], faculty_sets_list: List[Set[str]]
    ) -> None:
        """
        Removes near-duplicate faculty names within each faculty set based on MinHash similarity.

        Args:
            category_dict (dict): A dictionary where the keys are category identifiers and the values are CategoryInfo objects.
            faculty_sets_list (list[set]): A list containing the faculty set from each CategoryInfo object.

        This method iterates over each category and processes the faculty set to remove near-duplicates, updating the faculty attribute of each CategoryInfo object.
        """
        self.logger.info("Removing near-duplicate faculty names within each set...")
        for _, category_info in category_dict.items():
            final_set: Set[str] = self.duplicate_postprocessor(
                category_info.faculty, faculty_sets_list
            )
            category_info.faculty = final_set
        self.logger.info("Near-duplicate faculty names removed.")

    def duplicate_postprocessor(
        self,
        faculty_set: Set[str] | List[str],
        faculty_sets: List[Set[str]],
        similarity_threshold: float = 0.5,
    ) -> Set[str]:
        """
        Processes a set of faculty names to remove near-duplicate names based on MinHash similarity and most frequent variations.
        This method first generates the necessary utilities for comparison and removal.
        It then compares each name against all others in the set for near duplicates.
        If a name is deemed to be a duplicate based on MinHash similarity and the most frequent variation, it is added to the set of names to be removed.
        Finally, the refined faculty set is returned, excluding any names deemed to be duplicates.

        Args:
            faculty_set (Set[str]): A set of faculty names to be processed.
            faculty_sets (List[Set[str]]): A list of sets of faculty names, where each set contains the faculty names from a different category.
            similarity_threshold (float): The threshold for considering names as duplicates based on MinHash similarity.

        Returns:
            set[str]: The refined faculty set, excluding any names deemed to be duplicates.
        """
        # Generate needed utilities
        # most_frequent_variation is a dictionary that maps each normalized name to its most frequent variation
        # name_signatures is a dictionary that maps each name to its MinHash signature
        # to_remove is a set of names to be removed from the faculty set
        self.logger.info("Generating needed utilities...")
        if not isinstance(faculty_set, set):
            faculty_set = set(faculty_set)

        (
            most_frequent_variation,
            name_signatures,
            to_remove,
        ) = self.get_duplicate_utilities(faculty_set, faculty_sets)
        self.logger.info("Needed utilities generated.")

        # Step 3: Compare each name against all others in the set for near duplicates
        self.logger.info(
            "Comparing each name against all others in the set for near duplicates..."
        )
        for n1 in faculty_set:
            self.logger.info(f"Comparing {n1} against all others in the set...")
            for n2 in faculty_set:
                self.logger.info(f"Comparing {n1} against {n2}...")
                if n1 == n2:
                    self.logger.info(f"{n1} is the same as {n2}, skipping...")
                    continue
                self.logger.info(f"Comparing {n1} and {n2}...")
                self.process_name_pair(
                    similarity_threshold,
                    most_frequent_variation,
                    name_signatures,
                    to_remove,
                    n1,
                    n2,
                )

        self.logger.info("Refining faculty set...")
        refined_fac_set: Set[str] = faculty_set - to_remove
        self.logger.info("Faculty set refined.")
        return refined_fac_set

    def process_name_pair(
        self,
        similarity_threshold: float,
        most_frequent_variation: Dict[str, str],
        name_signatures: Dict[str, List[int]],
        to_remove: Set[str],
        n1: str,
        n2: str,
    ) -> None:
        """
        Compares two names and determines which one to keep based on MinHash similarity and most frequent variation.
        This method first compares the MinHash signatures of the two names to determine their similarity.
        If the similarity exceeds the specified threshold, it then determines which name to remove based on the most frequent variation.
        The name not chosen as the most frequent variation is added to the set of names to be removed.

        Args:
            n1, n2 (str): Names to compare.
            name_signatures (Dict[str, List[int]]): Dictionary of MinHash signatures.
            most_frequent_variation (Dict[str, str]): Dictionary mapping normalized names to their most frequent variations.
            to_remove (Set[str]): Set of names to be removed.
            similarity_threshold (float): Threshold for considering names as duplicates.
        """
        self.logger.info(f"Comparing {n1} and {n2}...")

        self.logger.info(f"Getting MinHash signatures for {n1} and {n2}...")
        signature1: List[int] = name_signatures[n1]
        signature2: List[int] = name_signatures[n2]
        self.logger.info(f"MinHash signatures for {n1} and {n2} retrieved.")

        self.logger.info(f"Comparing MinHash signatures for {n1} and {n2}...")
        similarity: float = self.minhash_util.compare_signatures(signature1, signature2)
        self.logger.info(f"MinHash signatures for {n1} and {n2} compared.")

        # Early exit if the similarity is below the threshold
        self.logger.info(f"Similarity: {similarity}")
        if similarity <= similarity_threshold:
            self.logger.info(f"Similarity is below the threshold, skipping...")
            return

        self.logger.info(f"Normalizing {n1} and {n2}...")
        n1_normalized: str = n1.lower().replace(" ", "")
        n2_normalized: str = n2.lower().replace(" ", "")
        self.logger.info(f"{n1} and {n2} normalized.")

        # Decide which name to keep
        self.logger.info(f"Deciding which name to keep...")
        to_remove.add(
            self.name_to_remove(
                most_frequent_variation, n1, n2, n1_normalized, n2_normalized
            )
        )
        self.logger.info(f"Name to remove decided.")

    def name_to_remove(
        self,
        most_frequent_variation: Dict[str, str],
        n1: str,
        n2: str,
        n1_normalized: str,
        n2_normalized: str,
    ) -> str:
        """
        Determines which of the two names to remove based on their normalized forms and the most frequent variations.
        This method checks if the normalized form of each name matches the most frequent variation.
        If one name matches its most frequent variation and the other does not, the non-matching name is chosen for removal.
        If neither or both names match their most frequent variations, the lexicographically greater name is chosen for removal.

        Args:
            most_frequent_variation (Dict[str, str]): Dictionary mapping normalized names to their most frequent variations.
            n1, n2 (str): Original names to compare.
            n1_normalized, n2_normalized (str): Normalized forms of the names.

        Returns:
            str: The name to be removed.
        """
        self.logger.info(f"Deciding which name to remove...")
        if (
            most_frequent_variation[n1_normalized] == n1
            and most_frequent_variation[n2_normalized] != n2
        ):
            self.logger.info(f"{n2} is the name to remove.")
            return n2
        elif (
            most_frequent_variation[n2_normalized] == n2
            and most_frequent_variation[n1_normalized] != n1
        ):
            self.logger.info(f"{n1} is the name to remove.")
            return n1
        # If none of the above conditions hold, return the lexicographically greater name
        self.logger.info(f"Returning the lexicographically greater name...")
        return n2 if n1 < n2 else n1

    def get_duplicate_utilities(
        self, faculty_set: set[str], faculty_sets: list[set[str]]
    ) -> tuple[Dict[str, str], Dict[str, List[int]], Set[str]]:
        """Generates utilities needed for duplicate removal.

        Args:
            faculty_set (set[str]): A set of faculty names for which to generate MinHash signatures.
            faculty_sets (list[set[str]]): A list of sets of faculty names, where each set contains the faculty names from a different category.

        Returns:
            tuple[Dict[str, str], Dict[str, List[int]], Set[str]]: A tuple containing the most frequent name variation dictionary, the name signatures dictionary, and the set of names to be removed.
        """
        self.logger.info("Generating utilities needed for duplicate removal...")
        most_frequent_variation: Dict[str, str] = self.get_most_frequent_name_variation(
            faculty_sets
        )
        self.logger.info("Most frequent name variation dictionary generated.")

        name_signatures: Dict[str, List[int]] = self.generate_signatures(faculty_set)
        self.logger.info("Name signatures dictionary generated.")

        to_remove: Set[str] = set()
        self.logger.info("Set of names to remove generated.")

        return most_frequent_variation, name_signatures, to_remove

    def generate_signatures(self, faculty_set: Set[str]) -> Dict[str, List[int]]:
        """
        Generates MinHash signatures for each name in the given faculty set.

        This method tokenizes each name into n-grams, computes a MinHash signature for these n-grams, and stores the result.
        A MinHash signature is a compact representation of the set of n-grams and is used to estimate the similarity between sets of names.

        Args:
            faculty_set (set[str]): A set of faculty names for which to generate MinHash signatures.

        Returns:
            dict[str, list[int]]: A dictionary mapping each name in the faculty set to its corresponding MinHash signature.
        """
        self.logger.info(
            "Generating MinHash signatures for each name in the faculty set..."
        )
        # Dictionary comprehension to generate a MinHash signature for each name
        name_signatures: Dict[str, List[int]] = {
            name: self.minhash_util.compute_signature(self.minhash_util.tokenize(name))
            for name in faculty_set
        }
        self.logger.info("MinHash signatures generated.")

        return name_signatures

    def get_most_frequent_name_variation(self, faculty_sets_list) -> Dict[str, str]:
        """
        Creates a dictionary that maps each unique normalized name to the most commonly occurring spelling variation of that name across all provided faculty sets. A 'normalized name' is derived by converting the original name to lowercase and removing all spaces, which helps in identifying different spellings of the same name as equivalent. The 'most frequent variation' refers to the spelling of the name that appears most often in the data, maintaining the original case and spaces.

        Args:
            faculty_sets_list (list of sets): A list where each set contains faculty names from a specific category. Each set is a collection of names that may include various spelling variations.

        Returns:
            most_frequent_variation (Dict[str, str]): A dictionary with normalized names as keys and their most frequent original spelling variations as values.
        """
        # Dictionary to store NameVariation objects for each normalized name
        self.logger.info(
            "Creating dictionary to store NameVariation objects for each normalized name..."
        )
        name_variations: Dict[str, NameVariation] = self.name_variations
        self.logger.info("Dictionary to store NameVariation objects created.")

        # Iterate over each set of faculty names
        self.logger.info("Iterating over each set of faculty names...")
        for faculty_set in faculty_sets_list:
            # Process each name in the set
            self.logger.info(f"Processing names in set: {faculty_set}...")
            for name in faculty_set:
                self.logger.info(f"Processing name: {name}...")
                # Normalize the name by converting to lowercase and removing spaces
                normalized_name: str = name.lower().replace(" ", "")
                self.logger.info(f"Normalized name: {normalized_name}...")

                self.logger.info(f"Checking if normalized name is in dictionary...")
                # If the normalized name is not already in the dictionary, add it with a new NameVariation object
                if normalized_name not in name_variations:
                    self.logger.info(f"Adding normalized name to dictionary...")
                    name_variations[normalized_name] = NameVariation(normalized_name)
                # Add the current variation of the name to the NameVariation object
                self.logger.info(
                    f"Adding current variation of the name to NameVariation object..."
                )
                name_variations[normalized_name].add_variation(name)

        # Create a dictionary to store the most frequent variation for each normalized name
        self.logger.info(
            "Creating dictionary to store the most frequent variation for each normalized name..."
        )
        most_frequent_variation: Dict[str, str] = {
            normalized_name: variation.most_frequent_variation()
            for normalized_name, variation in name_variations.items()
        }
        self.logger.info(
            "Dictionary to store the most frequent variation for each normalized name created."
        )

        return most_frequent_variation

    def standardize_names_across_sets(
        self, faculty_sets_list: List[Set[str]]
    ) -> List[Set[str]]:
        """Standardizes names across all sets by mapping each name to its most frequent variation across all sets.

        Args:
            faculty_sets_list (List[Set[str]]): A list of sets of faculty names, where each set contains the faculty names from a different category.

        Returns:
            List[Set[str]]: A list of sets containing the standardized faculty names across all categories.
        """
        # First, generate the most frequent name variation mapping across all sets
        self.logger.info(
            "Generating the most frequent name variation mapping across all sets..."
        )
        most_frequent_variation: Dict[str, str] = self.get_most_frequent_name_variation(
            faculty_sets_list
        )
        self.logger.info("Most frequent name variation mapping generated.")

        # Then, iterate through each set and standardize names based on the global mapping
        standardized_sets: List[Set[str]] = []
        self.logger.info("Iterating through each set and standardizing names...")
        for faculty_set in faculty_sets_list:
            self.logger.info(f"Standardizing names in set: {faculty_set}...")
            standardized_set: Set[str] = set()
            for name in faculty_set:
                normalized_name: str = name.lower().replace(" ", "")
                self.logger.info(f"Normalized name: {normalized_name}...")
                # Replace the name with its most frequent variation, if available
                standardized_name: str = most_frequent_variation.get(
                    normalized_name, name
                )
                self.logger.info(f"Standardized name: {standardized_name}...")
                standardized_set.add(standardized_name)
            standardized_sets.append(standardized_set)

        return standardized_sets


class MinHashUtility:
    """
    A utility class for performing MinHash calculations to estimate the similarity between sets of data.

    This class provides methods for generating hash functions, tokenizing strings into n-grams, computing MinHash signatures,
    and comparing these signatures to estimate the similarity between sets. The MinHash technique is particularly useful
    in applications where exact matches are not necessary, but approximate matches are sufficient, such as duplicate
    detection, document similarity, and clustering.

    Attributes:
        num_hashes (int): The number of hash functions to use in MinHash calculations, affecting the accuracy and
                          performance of the similarity estimation.
        large_prime (int): A large prime number used as the modulus in hash functions to minimize collisions.
        hash_fns (list[callable]): A list of pre-generated hash functions used for computing MinHash signatures.

    Methods:
        tokenize(string, n): Tokenizes a string into n-grams.
        generate_coefficients(): Generates random coefficients for hash functions.
        generate_hash_functions(): Creates a list of hash functions based on generated coefficients.
        compute_signature(tokens): Computes the MinHash signature for a set of tokens.
        compare_signatures(signature1, signature2): Compares two MinHash signatures and returns their estimated similarity.

    The class utilizes linear hash functions of the form h(x) = (a * x + b) % large_prime, where 'a' and 'b' are randomly
    generated coefficients. This approach helps in reducing the likelihood of hash collisions and ensures a uniform
    distribution of hash values.

    Example usage:
        minhash_util = MinHashUtility(num_hashes=200)
        tokens = minhash_util.tokenize("example string", n=3)
        signature = minhash_util.compute_signature(tokens)
        # Further operations such as comparing signatures can be performed.

    More on MinHash: https://en.wikipedia.org/wiki/MinHash
    """

    def __init__(self, num_hashes: int):
        """
        Initialize the MinHashUtility with the specified number of hash functions.

        Args:
            num_hashes (int): The number of hash functions to use for MinHash calculations.
        """
        self.logger.info(
            f"Initializing MinHashUtility with {num_hashes} hash functions..."
        )

        self.num_hashes: int = num_hashes  # Number of hash functions to use for MinHash
        self.logger.info(f"Number of hash functions set to {num_hashes}.")

        self.large_prime: int = 999983  # large prime number used for hashing
        self.logger.info(f"Large prime number set to 999983.")

        self.hash_fns: List[callable] = (
            self.generate_hash_functions()
        )  # List of hash functions
        self.logger.info("Hash functions generated.")

    def tokenize(self, string: str, n: int = 3) -> Set[str]:
        """
        Tokenize the given string into n-grams to facilitate the identification of similar strings.

        N-grams are contiguous sequences of 'n' characters extracted from a string. This method is useful in various
        applications such as text similarity, search, and indexing where the exact match is not necessary, but approximate
        matches are useful.

        More on n-grams: https://en.wikipedia.org/wiki/N-gram

        Args:
            string (str): The string to be tokenized.
            n (int): The length of each n-gram. Default is 3.

        Returns:
            set: A set containing unique n-grams derived from the input string.

        Raises:
            ValueError: If 'n' is greater than the length of the string or less than 1.
        """
        # If the n-gram length is invalid, raise a ValueError
        self.logger.info(f"Checking if n-gram length is valid...")
        if n > len(string) or n < 1:
            self.logger.error(f"N-gram length is invalid, raising ValueError...")
            raise ValueError(
                "The n-gram length 'n' must be between 1 and the length of the string."
            )

        n_grams: set = set()  # Set to store unique n-grams
        self.logger.info(f"N-grams set created.")

        # Loop through the string to extract n-grams
        self.logger.info(f"Looping through the string to extract n-grams...")
        for i in range(len(string) - n + 1):
            n_gram: str = string[i : i + n]
            n_grams.add(n_gram)

        self.logger.info(f"N-grams extracted.")

        return n_grams

    def generate_coeeficients(self) -> List[Tuple[int, int]]:
        """
        Generate a list of tuples, each containing a pair of coefficients (a, b) used for hash functions.

        Each tuple consists of:
        - a (int): A randomly chosen multiplier coefficient.
        - b (int): A randomly chosen additive coefficient.

        These coefficients are used in the linear hash functions for MinHash calculations.

        Returns:
            list[tuple[int, int]]: A list of tuples, where each tuple contains two integers (a, b).
        """
        coefficients: list = []  # List to store pairs of coefficients (a, b)
        self.logger.info(f"Coefficients list created.")

        # Generate a pair of coefficients for each hash function
        self.logger.info(f"Generating a pair of coefficients for each hash function...")
        for _ in range(self.num_hashes):
            a = random.randint(
                1, self.large_prime - 1
            )  # Randomly choose multiplier coefficient
            b = random.randint(
                0, self.large_prime - 1
            )  # Randomly choose additive coefficient
            coefficients.append((a, b))

        self.logger.info(f"Coefficients list populated.")

        return coefficients

    def generate_hash_functions(self) -> List[callable]:
        """
        Generate a list of linear hash functions for use in MinHash calculations.

        Each hash function is defined by a unique pair of coefficients (a, b) and is created using a factory function.
        These hash functions are used to compute hash values for elements in a set, which are essential for estimating
        the similarity between sets using the MinHash technique.

        The hash functions are of the form: h(x) = (a * x + b) % large_prime, where 'large_prime' is a large prime number
        used to reduce collisions in hash values.

        Overview of hash functions: https://en.wikipedia.org/wiki/Hash_function

        Returns:
            list: A list of lambda functions, each representing a linear hash function.
        """
        self.logger.info(f"Generating a list of linear hash functions...")

        def _hash_factory(a, b) -> callable:
            """
            Factory function to create a hash function with specified coefficients.

            Args:
                a (int): The multiplier coefficient in the hash function.
                b (int): The additive coefficient in the hash function.

            Returns:
                callable: A lambda function that takes an integer x and returns (a * x + b) % large_prime.
            """
            # Defines a hash function with coefficients a, b
            self.logger.info(f"Defining hash function with coefficients {a} and {b}...")
            return lambda x: (a * x + b) % self.large_prime

        hash_fns: list = []
        self.logger.info(f"Hash functions list created.")

        for _ in range(self.num_hashes):
            self.logger.info(f"Generating hash function {_}...")
            a = random.randint(
                1, self.large_prime - 1
            )  # Randomly choose multiplier coefficient
            self.logger.info(f"Multiplier coefficient {a} generated.")
            b = random.randint(
                0, self.large_prime - 1
            )  # Randomly choose additive coefficient
            self.logger.info(f"Additive coefficient {b} generated.")
            hash_fns.append(_hash_factory(a, b))
            self.logger.info(f"Hash function {_} generated.")

        return hash_fns

    def compute_signature(self, tokens: Set[int]) -> List[int]:
        """
        Compute MinHash signature for a set of tokens.
        A MinHash signature consists of the minimum hash value produced by each hash function across all tokens,
        which is used to estimate the similarity between sets of data.

        Detailed explanation of MinHash and its computation: https://en.wikipedia.org/wiki/MinHash

        Args:
            tokens (set[int]): A set of hashed tokens for which to compute the MinHash signature.

        Returns:
            list[int]: A list of minimum hash values, representing the MinHash signature.
        """
        # Initialize the signature with infinity values, which will later be replaced by the minimum hash values found.
        self.logger.info(f"Initializing signature with infinity values...")
        signature: list[int] = [float("inf")] * self.num_hashes
        self.logger.info(f"Signature initialized.")

        # Iterate over each token to compute its hash values using predefined hash functions
        self.logger.info(
            f"Iterating over each token to compute its hash values using predefined hash functions..."
        )
        for token in tokens:
            self.logger.info(f"Computing hash values for token {token}...")

            # Compute hash values for the token using each hash function
            self.logger.info(
                f"Computing hash values for token {token} using each hash function..."
            )
            hashed_values: list[int] = [
                hash_fn(hash(token)) for hash_fn in self.hash_fns
            ]
            self.logger.info(f"Hash values computed for token {token}.")

            # Update the signature by keeping the minimum hash value for each hash function
            self.logger.info(
                f"Updating the signature by keeping the minimum hash value for each hash function..."
            )
            for i in range(self.num_hashes):
                signature[i] = min(signature[i], hashed_values[i])
            self.logger.info(f"Signature updated.")

        self.logger.info(f"MinHash signature computed.")
        return signature

    def compare_signatures(self, signature1: List[int], signature2: List[int]) -> float:
        """
        Compare two MinHash signatures and return their similarity.
        The similarity is calculated as the fraction of hash values that are identical in the two signatures,
        which estimates the Jaccard similarity of the original sets from which these signatures were derived.

        This method is based on the principle that the more similar the sets are, the more hash values they will share,
        thus providing a proxy for the Jaccard index of the sets.

        More on estimating similarity with MinHash: https://en.wikipedia.org/wiki/Jaccard_index#MinHash

        Args:
            signature1 (list[int]): The MinHash signature of the first set, represented as a list of integers.
            signature2 (list[int]): The MinHash signature of the second set, represented as a list of integers.

        Returns:
            float: The estimated similarity between the two sets, based on their MinHash signatures.

        Raises:
            AssertionError: If the two signatures do not have the same length.
        """
        # Ensure both signatures are of the same length to compare them correctly
        self.logger.info(
            f"Ensuring both signatures are of the same length to compare them correctly..."
        )
        assert len(signature1) == len(
            signature2
        ), "Signatures must be of the same length."
        self.logger.info(f"Signatures are of the same length.")

        # Count the number of positions where the two signatures have the same hash value
        self.logger.info(
            f"Counting the number of positions where the two signatures have the same hash value..."
        )
        matching: int = sum(1 for i, j in zip(signature1, signature2) if i == j)
        self.logger.info(f"Number of matching positions counted.")

        # Calculate the similarity as the ratio of matching positions to the total number of hash functions
        self.logger.info(
            f"Calculating the similarity as the ratio of matching positions to the total number of hash functions..."
        )
        similarity: float = matching / len(signature1)
        self.logger.info(f"Similarity calculated.")
        return similarity


@dataclass
class NameVariation:
    """
    A data class for tracking different spelling variations of a normalized name and determining the most frequent variation.

    Attributes:
        normalized_name (str): The base form of the name, typically normalized to lower case and stripped of spaces.
        variations (dict[str, int]): A dictionary where keys are variations of the name and values are the counts of how often each variation occurs.

    Methods:
        add_variation(variation: str): Adds a variation of the name to the dictionary or increments its count if it already exists.
        most_frequent_variation(): Returns the variation of the name that occurs most frequently.
    """

    normalized_name: str

    # Default factory for variations ensures it starts as an empty dict.
    variations: Dict[str, int] = field(default_factory=dict)

    def add_variation(self, variation: str) -> None:
        """
        Adds a variation of the name to the dictionary or increments its count if it already exists.

        Args:
            variation (str): A specific spelling variation of the name.

        This method modifies the variations dictionary by either adding a new key with a count of 1 or incrementing the count of an existing key.
        """
        # Check if the variation is already in the dictionary and increment its count, otherwise add it with a count of 1
        self.logger.info(
            f"Checking if the variation {variation} is already in the dictionary and incrementing its count..."
        )
        if variation in self.variations:
            self.logger.info(
                f"Variation {variation} already in dictionary, incrementing count..."
            )
            self.variations[variation] += 1
            return
        self.logger.info(
            f"Variation {variation} not in dictionary, adding it with a count of 1..."
        )
        self.variations[variation] = 1

    def most_frequent_variation(self) -> str:
        """
        Returns the variation of the name that occurs most frequently.

        This method finds the key with the highest value in the variations dictionary, which represents the most common spelling variation.
        If the variations dictionary is empty, it raises a ValueError indicating that no variations have been added.

        Returns:
            str: The most frequent variation of the name.

        Raises:
            ValueError: If no variations are found in the dictionary, detailing the normalized name associated with the error.
        """
        # Check if the variations dictionary is empty and raise an error if it is
        self.logger.info(f"Checking if the variations dictionary is empty...")
        if not self.variations:
            self.logger.error(
                f"No variations found for the normalized name: '{self.normalized_name}', raising ValueError..."
            )
            raise ValueError(
                f"No variations found for the normalized name: '{self.normalized_name}'."
            )
        self.logger.info(f"Variations dictionary is not empty.")

        # Return the key (variation) with the maximum value (count) in the variations dictionary
        self.logger.info(
            f"Returning the key (variation) with the maximum value (count) in the variations dictionary..."
        )
        return max(self.variations, key=self.variations.get)

        ```

            src/other/custom_logging/logger.py:
            ```
import logging
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def configure_logger(
    name,
    log_file_directory=None,
    log_file_name="log_file.txt",
    level=logging.DEBUG,
    enable_console_logging=False,
):
    """Configures and returns a logger with both file and optional console handlers.

    Parameters:
    - name (str): Name of the logger.
    - log_file (str, optional): Path to the log file. Defaults to 'log_file.txt' in the current working directory.
    - level (logging.LEVEL, optional): Logging level, e.g., logging.DEBUG, logging.INFO.
    - enable_console_logging (bool, optional): If True, logs will also be printed to the console. Default is False.

    Returns:
    - logging.Logger: Configured logger with specified settings.
    """

    logger = logging.getLogger(name)
    logger.setLevel(level)
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # Ensure the logger has no duplicate handlers if function is called multiple times
    if logger.hasHandlers():
        logger.handlers.clear()

    # Set up file logging
    if log_file_directory is None:
        log_file_directory = os.getcwd()
    log_file = os.path.join(log_file_directory, log_file_name)
    fh = logging.FileHandler(log_file)
    fh.setLevel(level)
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    # Set up console logging if enabled
    if enable_console_logging:
        ch = logging.StreamHandler()
        ch.setLevel(level)
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    return logger

            ```

            src/other/custom_logging/logger_Documentation.md:
            ```

---

# Logger Configuration Utility

The `configure_logger` utility function facilitates setting up a logger for Python applications. It provides customization for logging levels, formats, and outputs to both a file and optionally to the console.

## Usage

```python
configure_logger(name, log_file=None, level=logging.DEBUG, enable_console_logging=True)
```

### Parameters

- **name** (`str`): Name of the logger. Typically, the module or component name.
- **log_file** (`str`, optional): Path to the log file. Defaults to `log_file.txt` in the current working directory.
- **level** (`logging.LEVEL`, optional): Logging level threshold. Defaults to `logging.DEBUG`, for less info you can do `.INFO`, `.WARNING`, `.ERROR`, or `.CRITICAL`, each will have less output than the previous.
- **enable_console_logging** (`bool`, optional): If `True`, enables logging output to the console. Defaults to `True`.

### Returns

- **logger** (`logging.Logger`): A configured logger instance.

### Example

```python
import logging
from GeneralUtilities.logging.logger import logging

# Basic configuration with console output
logger = configure_logger('my_script')

# Configuration without console output
logger = configure_logger('my_script', enable_console_logging=False)

# Custom log file and level, with console output
logger = configure_logger('my_script', 'path/to/my_log.txt', level=logging.DEBUG)

# Use the logger
logger.info("Info message.")
logger.debug("Debug message.")
```

### Function Details

- **Log File**: Defaults to `log_file.txt` in the current working directory if not specified.
- **Log Format**: Includes timestamp, logger name, log level, and message.
- **Console Logging**: Optional. Controlled by `enable_console_logging`.
- **Directory Creation**: Automatically creates needed directories for the log file.

---
            ```

            src/other/digital_measures_verification/__init__.py:
            ```
from .verification import run_dm_verification

            ```

            src/other/digital_measures_verification/verification.py:
            ```
"""
This script compares publication titles from Digital Measures (DM) and Web of Science (WoS) datasets. It loads titles from both sources, compares them using exact and fuzzy matching, and provides statistics on the overlap and differences between the two datasets. The script also includes utility functions for data processing and visualization.
"""

import glob
import json
import os
import re
import time
from typing import Any, Dict, List, Set, Tuple
from urllib.parse import quote

import requests
from datasketch import MinHash, MinHashLSH
from tqdm import tqdm

from academic_metrics.factories import StrategyFactory
from academic_metrics.utils import WarningManager
from academic_metrics.orchestrators import CategoryDataOrchestrator


def query_crossref(
    title: str, max_retries: int = 3, delay: int = 2
) -> Tuple[Dict[str, Any], float]:
    # Convert the title to lowercase except for the first letter of the first word
    words = title.split()
    if words:
        title = (
            words[0].capitalize() + " " + " ".join(word.lower() for word in words[1:])
        )

    # Hardcoded parameters for the specific title

    # Properly encode the title, author, and journal for a URL
    encoded_title = quote(title)
    """
    if title is = This is a Paper

    turns into 

    This%20is%20a%20paper
    """

    # Construct the query URL with additional parameters
    url = f"https://api.crossref.org/works?query.title={encoded_title}"
    print(f"Querying Crossref API for title: {title}")
    print(f"URL: {url}")
    input("Press Enter to continue...")

    start_time = time.time()
    for attempt in range(max_retries):
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            # print(f"Response: {data['message']['items'][0]['title']}")
            # input("Press Enter to continue...")
            # print(f"Response: {json.dumps(data, indent=4)}")
            # Check if 'items' list is present in the response
            if "message" in data and "items" in data["message"]:
                items = data["message"]["items"]
                if items:
                    with open("items_test.json", "w") as f:
                        json.dump(items[0], f, indent=4)
                    # print(f"Items: {json.dumps(items[0], indent=4)}")
                    # input("Press Enter to continue...")
                    return (
                        json.dumps(items[0], indent=4),
                        time.time() - start_time,
                    )  # Return the first item and query time
            return (
                {},
                time.time() - start_time,
            )  # Return an empty dict and query time if no items are found
        else:
            print(
                f"Attempt {attempt + 1} failed with status code {response.status_code}. Retrying in {delay} seconds..."
            )
            time.sleep(delay)

    print(f"Failed to retrieve data for title: {title} after {max_retries} attempts.")
    return {}, time.time() - start_time


def save_results_to_file(results: List[Dict[str, Any]], file_path: str) -> None:
    # Read existing data
    # print(f"Results: {results}")
    # input("Press Enter to continue...")
    # print(f"Saving results to file: {file_path}")
    # input("Press Enter to continue...")
    if os.path.exists(file_path):
        with open(file_path, "r") as file:
            try:
                existing_data = json.load(file)
            except json.JSONDecodeError:
                existing_data = []
    else:
        existing_data = []
    existing_data_len = len(existing_data)
    print(f"Existing data length before: {existing_data_len}")
    # Extract items from results and append to existing data
    # new_items = []
    for result in results:
        existing_data.append(json.loads(result))

    existing_data_len_after = len(existing_data)
    print(f"Existing data length after: {existing_data_len_after}")
    print(f"Difference: {existing_data_len_after - existing_data_len}")

    with open(file_path, "w") as file:
        json.dump(existing_data, file, indent=4)


def load_titles_from_reformatted_files(directory_path: str) -> Set[str]:
    """
    Load titles from JSON files in the specified directory.

    Args:
        directory_path (str): Path to the directory containing JSON files.

    Returns:
        set: A set of unique titles from all JSON files in the directory.
    """
    titles_set: Set[str] = set()
    json_files: List[str] = glob.glob(os.path.join(directory_path, "*.json"))
    for file_path in tqdm(
        json_files, desc="Loading titles from reformatted files", position=0, leave=True
    ):
        with open(file_path, "r") as f:
            data: Dict[str, Any] = json.load(f)
            titles_set.update(data)

    return titles_set


def load_titles_from_processed_category_data(file_path: str) -> Set[str]:
    """
    Load titles from a processed category data JSON file.

    Args:
        file_path (str): Path to the processed category data JSON file.

    Returns:
        set: A set of unique titles from the processed category data.
    """
    with open(file_path, "r") as f:
        data: Dict[str, Any] = json.load(f)
    titles_set: Set[str] = set()
    for category, info in tqdm(
        data.items(),
        desc="Loading titles from processed category data",
        position=0,
        leave=True,
    ):
        titles_set.update(info["titles"])
    return titles_set


def aggressive_regularize(s: str) -> str:
    """
    Aggressively regularize a string by removing all non-alphanumeric characters and converting to lowercase.

    Args:
        s (str): The input string to regularize.

    Returns:
        str: The regularized string.
    """
    return re.sub(r"\W+", "", s.lower())


def process_titles(titles: Set[str]) -> Set[str]:
    """
    Process a set of titles by applying aggressive regularization to each title.

    Args:
        titles (set): A set of titles to process.

    Returns:
        set: A set of processed titles.
    """
    return set(
        aggressive_regularize(title)
        for title in tqdm(titles, desc="Processing titles", position=0, leave=True)
    )


def compare_datasets(
    dm_titles: Set[str], wos_titles: Set[str]
) -> Tuple[Set[str], Set[str], Set[str]]:
    """
    Compare two sets of titles and find exact matches, titles only in DM, and titles only in WoS.

    Args:
        dm_titles (set): Set of titles from Digital Measures.
        wos_titles (set): Set of titles from Web of Science.

    Returns:
        tuple: A tuple containing three sets:
            - exact_matches: Titles that match exactly between DM and WoS.
            - only_in_dm: Titles that are only in DM.
            - only_in_wos: Titles that are only in WoS.
    """
    dm_regularized: Set[str] = process_titles(dm_titles)
    wos_regularized: Set[str] = process_titles(wos_titles)

    exact_matches: Set[str] = dm_regularized.intersection(wos_regularized)
    only_in_dm: Set[str] = dm_regularized - wos_regularized
    only_in_wos: Set[str] = wos_regularized - dm_regularized

    return exact_matches, only_in_dm, only_in_wos


def find_similar_titles(
    dm_titles: Set[str],
    wos_titles: Set[str],
    threshold: float = 0.8,
    crossref: bool = False,
) -> Set[str]:
    """
    Find similar titles between DM and WoS datasets using MinHash LSH.

    Args:
        dm_titles (set): Set of titles from Digital Measures.
        wos_titles (set): Set of titles from Web of Science.
        threshold (float): Similarity threshold for considering titles as similar (default: 0.8).

    Returns:
        set: A set of similar titles found in both datasets.
    """
    lsh: MinHashLSH = MinHashLSH(threshold=threshold, num_perm=128)
    index_prefix: str = "wos" if not crossref else "crossref"

    wos_titles_list: List[str] = list(wos_titles)
    for idx, title in enumerate(
        tqdm(wos_titles_list, desc="Indexing WoS titles", position=0, leave=True)
    ):
        m: MinHash = MinHash(num_perm=128)
        for d in aggressive_regularize(title):
            m.update(d.encode("utf8"))
        lsh.insert(f"{index_prefix}_{idx}", m)

    similar_titles: Set[str] = set()
    for dm_title in tqdm(
        dm_titles, desc="Finding similar titles", position=0, leave=True
    ):
        m: MinHash = MinHash(num_perm=128)
        for d in aggressive_regularize(dm_title):
            m.update(d.encode("utf8"))
        result: List[str] = lsh.query(m)
        if result:
            similar_titles.add(dm_title)

    return similar_titles


def display_titles_only_in_dm(
    final_only_in_dm: Set[str], dm_titles_set: Set[str], wos_or_crossref_str: str
) -> None:
    """
    Display titles that are only present in the Digital Measures dataset.

    Args:
        final_only_in_dm (set): Set of regularized titles only in DM after comparison.
        dm_titles_set (set): Original set of DM titles.

    Returns:
        None
    """
    print(
        f"\nThe titles that Digital Measures has but {wos_or_crossref_str} does not, e.g., the titles we are missing:"
    )
    for title in tqdm(
        dm_titles_set, desc="Displaying titles only in DM", position=0, leave=True
    ):
        if aggressive_regularize(title) in final_only_in_dm:
            print(f"- {title}")


def print_fancy_box(message: str) -> None:
    """
    Print a message in a fancy box with centered text and padding.

    Args:
        message (str): The message to be displayed in the fancy box.

    Returns:
        None
    """
    lines: List[str] = message.split("\n")
    max_line_length: int = max(len(line) for line in lines)
    box_width: int = max_line_length + 10  # Add extra padding

    print("\nâ•”" + "â•" * box_width + "â•—")
    print("â•‘" + " " * box_width + "â•‘")  # Add empty line for top padding

    for line in lines:
        padding: int = (box_width - len(line)) // 2
        print(
            "â•‘" + " " * padding + line + " " * (box_width - len(line) - padding) + "â•‘"
        )

    print("â•‘" + " " * box_width + "â•‘")  # Add empty line for bottom padding
    print("â•š" + "â•" * box_width + "â•\n")

    with open(
        os.path.join(
            os.path.dirname(__file__),
            "static_output_files",
            "REAL_verification_results_SUMMARY.txt",
        ),
        "w",
    ) as f:
        print("\nâ•”" + "â•" * box_width + "â•—", file=f)
        print("â•‘" + " " * box_width + "â•‘", file=f)  # Add empty line for top padding

        for line in lines:
            padding: int = (box_width - len(line)) // 2
            print(
                "â•‘"
                + " " * padding
                + line
                + " " * (box_width - len(line) - padding)
                + "â•‘",
                file=f,
            )

        print("â•‘" + " " * box_width + "â•‘", file=f)  # Add empty line for bottom padding
        print("â•š" + "â•" * box_width + "â•\n", file=f)


def do_dm_verification(crossref: bool = False, second_run: bool = False) -> None:
    """
    Main function to execute the comparison between Digital Measures and Web of Science datasets.

    This function loads titles from both datasets, performs comparisons, finds similar titles,
    calculates statistics, and displays the results.

    Args:
        None

    Returns:
        None
    """
    if crossref:
        print("Analysis for Crossref API\n\n")
    else:
        print("\n\nAnalysis for Web of Science Export\n\n")

    reformatted_files_dir: str = os.path.join(
        os.path.dirname(__file__), "reformattedFiles"
    )
    processed_category_data_file: str = os.path.join(
        os.path.dirname(__file__), "static_output_files", "processed_category_data.json"
    )
    test_processed_category_data_file: str = os.path.join(
        os.path.dirname(__file__), "test_processed_category_data.json"
    )

    wos_or_crossref_str: str = (
        "Web of Science Export" if not crossref else "Crossref API"
    )

    dm_titles_set: Set[str] = load_titles_from_reformatted_files(reformatted_files_dir)
    wos_titles_set: Set[str] = set()
    if crossref:
        wos_titles_set: Set[str] = load_titles_from_processed_category_data(
            test_processed_category_data_file
        )
    else:
        wos_titles_set: Set[str] = load_titles_from_processed_category_data(
            processed_category_data_file
        )

    print(f"Initial Digital Measures titles: {len(dm_titles_set)}")
    print(f"Initial {wos_or_crossref_str} titles: {len(wos_titles_set)}")

    exact_matches, only_in_dm, only_in_wos = compare_datasets(
        dm_titles_set, wos_titles_set
    )

    print("\nInitial comparison:")
    print(f"Exact matches: {len(exact_matches)}")
    print(
        f"Only in Digital Measures: {len(only_in_dm)} = {len(dm_titles_set)} - {len(exact_matches)}"
    )
    print(
        f"Only in {wos_or_crossref_str}: {len(only_in_wos)} = {len(wos_titles_set)} - {len(exact_matches)}"
    )
    print(
        f"\n\nQuerying Crossref API for titles in Digital Measures that are not present in the initial {wos_or_crossref_str} dataset..."
    )

    print("\nSearching for similar titles...")
    similar_titles: Set[str] = find_similar_titles(
        only_in_dm, only_in_wos, threshold=0.8, crossref=crossref
    )
    print(f"Similar titles found: {len(similar_titles)}")

    final_common_set: Set[str] = exact_matches.union(
        set(aggressive_regularize(title) for title in similar_titles)
    )
    final_only_in_dm: Set[str] = only_in_dm - set(
        aggressive_regularize(title) for title in similar_titles
    )
    final_only_in_wos: Set[str] = only_in_wos - set(
        aggressive_regularize(title) for title in similar_titles
    )

    print("\nFinal results:")
    print(
        f"Final common set: {len(final_common_set)} = {len(exact_matches)} + {len(similar_titles)}"
    )
    print(
        f"Final only in Digital Measures: {len(final_only_in_dm)} = {len(only_in_dm)} - {len(similar_titles)}"
    )
    print(
        f"Final only in {wos_or_crossref_str}: {len(final_only_in_wos)} = {len(only_in_wos)} - {len(similar_titles)}"
    )

    total_unique: int = (
        len(final_common_set) + len(final_only_in_dm) + len(final_only_in_wos)
    )
    print(
        f"\nTotal unique titles: {total_unique} = {len(final_common_set)} + {len(final_only_in_dm)} + {len(final_only_in_wos)}"
    )

    dm_exclusive_percentage: float = (len(final_only_in_dm) / len(dm_titles_set)) * 100
    wos_exclusive_percentage: float = (
        len(final_only_in_wos) / len(wos_titles_set)
    ) * 100

    print(
        f"\nPercentage of Digital Measures titles not in {wos_or_crossref_str}: {dm_exclusive_percentage:.2f}% = ({len(final_only_in_dm)} / {len(dm_titles_set)}) * 100"
    )
    print(
        f"Percentage of {wos_or_crossref_str} titles not in Digital Measures: {wos_exclusive_percentage:.2f}% = ({len(final_only_in_wos)} / {len(wos_titles_set)}) * 100"
    )

    percent_wos_has_from_dm: float = 100 - (
        (len(final_only_in_dm) / len(dm_titles_set)) * 100
    )
    print(
        f"Percent of Digital Measures titles that {wos_or_crossref_str} has: {percent_wos_has_from_dm:.2f}%"
    )

    display_titles_only_in_dm(final_only_in_dm, dm_titles_set, wos_or_crossref_str)

    summary = (
        "SUMMARY\n\n"
        f"The {wos_or_crossref_str} export technique is only missing {dm_exclusive_percentage:.2f}% "
        f"of the titles that Digital Measures has.\n\n"
        f"This means that the {wos_or_crossref_str} export technique only lacks 1 out of every "
        f"{1 / dm_exclusive_percentage * 100:.0f} titles that DM has."
    )
    print_fancy_box(summary)

    if crossref and len(final_only_in_dm) > 0:
        # Query Crossref API for titles in Digital Measures that are not present in the initial {wos_or_crossref_str} dataset
        crossref_results = []
        estimated_time_per_query = None

        title_to_exclude = "Reporting Cash Receipts over $10,000"
        dm_titles_set = {
            title
            for title in dm_titles_set
            if aggressive_regularize(title) != aggressive_regularize(title_to_exclude)
        }
        final_only_in_dm = {
            title
            for title in final_only_in_dm
            if aggressive_regularize(title) != aggressive_regularize(title_to_exclude)
        }

        # Inform the user about the exclusion
        print(
            f"The paper '{title_to_exclude}' is published in the Tax Adviser, which does not provide a DOI. It is considered a professional magazine/trade publication rather than an academic journal."
        )

        with tqdm(
            total=len(final_only_in_dm),
            desc="Querying Crossref API",
            position=0,
            leave=True,
        ) as pbar:
            if not second_run:
                if len(final_only_in_dm) > 0:
                    for i, title in enumerate(final_only_in_dm):
                        # Find the original title from dm_titles_set that matches the regularized title
                        original_title = next(
                            (
                                t
                                for t in dm_titles_set
                                if aggressive_regularize(t) == title
                            ),
                            None,
                        )
                        if original_title:
                            result, query_time = query_crossref(original_title)
                            if result:
                                crossref_results.append(result)

                        if i == 0:
                            estimated_time_per_query = query_time * (
                                len(final_only_in_dm) - i + 1
                            )

                        pbar.update(1)
                        if estimated_time_per_query:
                            pbar.set_postfix(
                                {
                                    "Estimated time left": f"{(len(final_only_in_dm) - pbar.n) * estimated_time_per_query:.0f}s"
                                }
                            )

                    # Save crossref results to a file in 'input_files' directory via appending to the current file there
                    crossref_file_path = os.path.join(
                        os.path.dirname(__file__),
                        "input_files",
                        "2017-2024-paper-doi-list.json",
                    )
                    save_results_to_file(crossref_results, crossref_file_path)

                    # Run WosClassification with the new data
                    strategy_factory = StrategyFactory()
                    warning_manager = WarningManager()

                    input_dir_path = "./input_files"
                    output_dir_path = "./crossref_split_files"
                    input_dir_path = os.path.expanduser(input_dir_path)
                    output_dir_path = os.path.expanduser(output_dir_path)
                    # Instantiate the orchestrator class
                    category_data_orchestrator = CategoryDataOrchestrator(
                        input_dir_path=input_dir_path,
                        output_dir_path=output_dir_path,
                        strategy_factory=strategy_factory,
                        warning_manager=warning_manager,
                        crossref_run=True,
                        make_files=True,
                        extend=True,
                    )

                    print(
                        f"second_run: {second_run}, crossref_results: {len(crossref_results)}"
                    )
                    input("Press Enter to continue...")

                    # call main again to run the analysis on the updated data
                    # Only call main again if there are new results to process and it's not the second run
                    if crossref_results and not second_run:
                        print("Calling main again for the second run.")
                        do_dm_verification(crossref=True, second_run=True)
                    else:
                        print("Stopping recursion.")


def run_dm_verification() -> None:
    """
    Run the Digital Measures verification process for both Crossref and Web of Science.

    This function orchestrates the entire process, including data loading, comparison,
    and analysis, for both Crossref and Web of Science datasets."""
    crossref: bool = True
    do_dm_verification(crossref=crossref)
    do_dm_verification(crossref=False)

            ```

        src/other/docs.md:
        ```
# Documentation for Data Collection Scripts

## Table of Contents

1. [Introduction](#introduction)
2. [Scraper](#scraper)
    - [Overview](#overview)
    - [Usage](#usage)
    - [Functions](#functions)
3. [Crossref Wrapper](#crossref-wrapper)
    - [Overview](#overview-1)
    - [Usage](#usage-1)
    - [Functions](#functions-1)
4. [License](#license)
5. [Acknowledgments](#acknowledgments)

## Introduction

This documentation provides an overview and usage instructions for the data collection scripts used in this project. The scripts include a web scraper (`scraper.py`) and a Crossref API wrapper (`crossrefwrapper.py`).

## Scraper

### Overview

The `scraper.py` script is designed to fetch and clean text data from web pages. It uses Selenium for web scraping and BeautifulSoup for parsing HTML content. The script also integrates with OpenAI's API to clean and format the extracted text.

### Usage

To use the scraper, you need to have the required dependencies installed. You can install them using:

```bash
pip install -r requirements.txt
```

Then, you can run the script as follows:

```bash
python scraper.py
```

### Functions

#### `setup_chain(output_list)`

- **Description**: Sets up the chain manager for cleaning the extracted text.
- **Parameters**:
  - `output_list` (list): List of text elements to be cleaned.
- **Returns**: A dictionary containing the cleaned text.

#### `get_abstract(url)`

- **Description**: Fetches and processes the abstract from a given URL.
- **Parameters**:
  - `url` (str): The URL of the web page to scrape.
- **Returns**: A dictionary containing the abstract and extra context.

## Crossref Wrapper

### Overview

The `crossrefwrapper.py` script is designed to interact with the Crossref API to fetch metadata for academic papers. It supports asynchronous data fetching and processes the data to include only relevant information.

### Usage

To use the Crossref wrapper, you need to have the required dependencies installed. You can install them using:

```bash
pip install -r requirements.txt
```

Then, you can run the script as follows:

```bash
python crossrefwrapper.py
```

### Functions

#### `__init__(self, base_url, affiliation, from_year, to_year, logger)`

- **Description**: Initializes the CrossrefWrapper class.
- **Parameters**:
  - `base_url` (str): The base URL for the Crossref API.
  - `affiliation` (str): The affiliation to filter results by.
  - `from_year` (int): The starting year for the data fetch.
  - `to_year` (int): The ending year for the data fetch.
  - `logger` (logging.Logger): Logger instance for logging.

#### `fetch_data(self, session, url, headers, retries, retry_delay)`

- **Description**: Fetches data from the Crossref API.
- **Parameters**:
  - `session` (aiohttp.ClientSession): The aiohttp session.
  - `url` (str): The URL to fetch data from.
  - `headers` (dict): Headers for the request.
  - `retries` (int): Number of retries in case of failure.
  - `retry_delay` (int): Delay between retries.
- **Returns**: The fetched data as a dictionary.

#### `build_request_url(self, base_url, affiliation, from_date, to_date, n_element, sort_type, sort_ord, cursor, has_abstract)`

- **Description**: Builds the request URL for the Crossref API.
- **Parameters**:
  - `base_url` (str): The base URL for the Crossref API.
  - `affiliation` (str): The affiliation to filter results by.
  - `from_date` (str): The starting date for the data fetch.
  - `to_date` (str): The ending date for the data fetch.
  - `n_element` (str): Number of elements to fetch.
  - `sort_type` (str): The type of sorting.
  - `sort_ord` (str): The order of sorting.
  - `cursor` (str): The cursor for pagination.
  - `has_abstract` (bool): Whether to filter by presence of abstract.
- **Returns**: The constructed URL as a string.

#### `process_items(self, data, from_date, to_date, affiliation)`

- **Description**: Processes the fetched data to filter relevant items.
- **Parameters**:
  - `data` (dict): The fetched data.
  - `from_date` (str): The starting date for the data fetch.
  - `to_date` (str): The ending date for the data fetch.
  - `affiliation` (str): The affiliation to filter results by.
- **Returns**: A list of filtered items.

#### `acollect_yrange(self, session, from_date, to_date, n_element, sort_type, sort_ord, cursor, retries, retry_delay)`

- **Description**: Asynchronously collects data for a given year range.
- **Parameters**:
  - `session` (aiohttp.ClientSession): The aiohttp session.
  - `from_date` (str): The starting date for the data fetch.
  - `to_date` (str): The ending date for the data fetch.
  - `n_element` (str): Number of elements to fetch.
  - `sort_type` (str): The type of sorting.
  - `sort_ord` (str): The order of sorting.
  - `cursor` (str): The cursor for pagination.
  - `retries` (int): Number of retries in case of failure.
  - `retry_delay` (int): Delay between retries.
- **Returns**: A list of collected items.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Thanks to the contributors of the Selenium and BeautifulSoup libraries.
- Special thanks to the Crossref API team for providing access to their data.

        ```

        src/other/faculty_department_manager.py:
        ```
import warnings
import time


class FacultyDepartmentManager:
    def __init__(self, category_processor):
        self.category_processor = category_processor

    def update_faculty_set(self, categories, faculty_members):
        for category in categories:
            if category in self.category_processor.category_data:
                category_info = self.category_processor.category_data[category]
                for faculty_member in faculty_members:
                    category_info.faculty.add(faculty_member)
            else:
                warnings.warn(
                    f"Warning: Category {category} not found in category_data. Continuing to next."
                )

    def update_department_set_2(self, categories, department_info):
        if department_info == "Unknown":
            return

        else:
            department_members = department_info
            for category in categories:
                if category in self.category_processor.category_data:
                    category_info = self.category_processor.category_data[category]
                    if isinstance(department_members, list):
                        for department_member in department_members:
                            category_info.departments.add(department_member)
                    elif isinstance(department_members, str):
                        category_info.departments.add(department_members)
                    else:
                        warnings.warn(
                            f"Unexpected department_members type: {type(department_members)}"
                        )
                else:
                    warnings.warn(
                        f"WARNING: Category {category} not found in category_data. Continuing to next category."
                    )

    def update_title_set(self, categories, title):
        for category in categories:
            if category in self.category_processor.category_data:
                category_info = self.category_processor.category_data[category]
                if isinstance(title, list):
                    for t in title:
                        if t in category_info.titles:
                            print(f"Duplicate title found: {t}")
                        category_info.titles.add(t)
                elif isinstance(title, str):
                    if title in category_info.titles:
                        print(f"Duplicate title found: {title}")
                    category_info.titles.add(title)

    def update_faculty_count(self):
        for category, category_info in self.category_processor.category_data.items():
            category_info.faculty_count = len(category_info.faculty)

    def update_department_count(self):
        for category, category_info in self.category_processor.category_data.items():
            category_info.department_count = len(category_info.departments)

    def update_article_counts(self, categories_dict):
        """Returns total article count which is = to length of the files set"""
        for _, info in categories_dict.items():
            info.article_count = len(info.titles)
        return categories_dict

    def update_tc_count(self, categories_dict):
        for _, info in categories_dict.items():
            info.tc_count = len(info.tc_list)
        return categories_dict

    def get_total_article_count(self):
        total_article_count = 0
        for category, category_info in self.category_processor.category_data.items():
            total_article_count += category_info.article_count
        return total_article_count

        ```

        src/other/file_convert.py:
        ```
"""
Author: Cole Barbes
Last Updated: 03/27/2024
File Converter class structure to manage all needed file conversions
"""

import pandas as pd

# import torch
import numpy as np
import json
from striprtf.striprtf import rtf_to_text
import csv
import requests
import xml.etree.ElementTree as ET
from .data_class import CitationData, TopicData


class File_Convert:
    # init the process class with a file location
    def __init__(self, path):
        self.path = path
        self.file = None
        self.text = None

    def from_csv(self, output_format):
        """
        convert the csv file to a pandas data frame
        """
        self.file = pd.read_csv(self.path)

        if output_format == "json":
            self.to_json("csv")

    def from_rtf(self, output_format):
        """
        convert the rtf using the library an send the text output to a given type
        """
        with open(self.path, "r") as self.file:
            rtf_text = self.file.read()

        plain_text = rtf_to_text(rtf_text)
        self.text = plain_text

        if output_format == "json":
            self.to_json("rtf")

    def to_json(self, type):
        if type == "csv":
            # create dictionary of proposals
            row_dict = {
                i: self.file.loc[i].to_dict() for i in range(0, self.file.shape[0])
            }

            output_json = "Proposals.json"

            # fill the file
            with open(output_json, "w") as json_file:
                for i in range(0, len(row_dict)):
                    json.dump(row_dict[i], json_file, indent=4)

            print("Row inserted into JSON file successfully")
        elif type == "rtf":
            txt_list = self.text.splitlines()[6:]

            txt_list = [list(filter(None, item.split("|"))) for item in txt_list]

            txt_list = [i for i in txt_list if len(i) != 0]

            txt_dict = {}
            current_section = None

            for entry in txt_list:
                if len(entry) == 1:  # Get the Section name
                    current_section = entry[0]
                    txt_dict[current_section] = []
                elif len(entry) == 2:  # Extract the Citation
                    txt_dict[current_section].append(
                        {"Citation": entry[0], "Total Citations": entry[1]}
                    )
                elif len(entry) == 3:  # Extract the Journal name entry or a total
                    # grab the entry that has the totals for the journal
                    if "Total" in entry[0]:
                        txt_dict[current_section].append(
                            {
                                entry[0]: {
                                    "Total Publications": entry[1],
                                    "Total Citations": entry[2],
                                }
                            }
                        )
                    # grab the entry that has the number of publications and the rank of the journal
                    else:
                        txt_dict[current_section].append(
                            {
                                entry[0]: {
                                    "Number of Publications": entry[1],
                                    "Rank": entry[2],
                                }
                            }
                        )
            # output all the json to a json file for each section
            for key, value in txt_dict.items():
                filename = "../../assets/json_data/" + key.replace(" ", "-") + ".json"
                with open(filename, "w") as file:
                    json.dump({key: value}, file, indent=4)

        ```

        src/other/firecrawl_test.py:
        ```
from langchain_community.document_loaders import FireCrawlLoader
from dotenv import load_dotenv
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    PromptTemplate,
)
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
import json
from typing import Optional

# class FacultyMember(BaseModel):
#     name: str = Field(..., description="The name of the faculty member")
#     position: str = Field(..., description="The position of the faculty member")
#     department: str = Field(..., description="The department of the faculty member")
#     email: Optional[str] = Field(None, description="The email of the faculty member")
#     phone: Optional[str] = Field(None, description="The phone number of the faculty member")

from pydantic import BaseModel, Field
from typing import Dict, List


class FacultyMember(BaseModel):
    name: str = Field(..., description="The name of the faculty member")
    position: str = Field(..., description="The position of the faculty member")
    department: str = Field(..., description="The department of the faculty member")
    email: str | None = Field(None, description="The email of the faculty member")
    phone: str | None = Field(
        None, description="The phone number of the faculty member"
    )


class AIResponseFormat(BaseModel):
    faculty_members: List[FacultyMember] = Field(
        ..., description="The list of faculty members"
    )


load_dotenv()


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY)

json_format = """
    {
        "faculty_members": [
            {
                "name": "Name of the faculty member",
                "position": "Position of the faculty member",
                "department": "Department of the faculty member",
                "email": "Email of the faculty member" | null,
                "phone": "Phone number of the faculty member" | null
            },
            {
                "name": "Name of the faculty member",
                "position": "Position of the faculty member",
                "department": "Department of the faculty member",
                "email": "Email of the faculty member" | null,
                "phone": "Phone number of the faculty member" | null
            },
            ...
        ]
    }
"""

system_prompt_template = PromptTemplate(
    template="""
    You are an AI assistant tasked with taking in markdown formatted text scraped off of a university website and extracting information about faculty members.
    
    The information you are to extract is as follows:
    - Name (name of the faculty member)
    - Position (position of the faculty member)
    - Department (department of the faculty member)
    - Email (email of the faculty member - if available)
    - Phone (phone number of the faculty member - if available)

    You are always to output your response in JSON format.
    The format of the JSON you will output is as follows:
    {json_format}
    
    IMPORTANT: You are always to attempt to output your response in JSON format. If you do output your response in JSON format you have failed.
    IMPORTANT: Remember you are required to always extract the name, position, and department of the faculty members.
    IMPORTANT: You are always to attempt to extract the email and phone number of the faculty members.
    IMPORTANT: In the event the email or phone number cannot be found, you are to set them to None.
    """
)

human_prompt_template = PromptTemplate(
    template="""
    ***EXTRACTED INFORMATION BELOW\n\n***
    {context}
    """
)


prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(system_prompt_template.template),
        HumanMessagePromptTemplate.from_template(human_prompt_template.template),
    ]
)


def spencers_loader_func():
    FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")
    loader = FireCrawlLoader(
        api_key=FIRECRAWL_API_KEY,
        url="https://www.salisbury.edu/faculty-and-staff/",
        mode="scrape",
    )
    docs = loader.load()
    context = ""
    with open("firecrawl_test_results.md", "w") as f:
        for doc in docs:
            f.write(f'# {doc.metadata.get("title", "Untitled")}\n\n')
            f.write(
                f"## {doc.metadata.get('description', 'No description available')}\n\n"
            )
            f.write(f"{doc.page_content}\n\n")
    for doc in docs:
        context += f'# {doc.metadata.get("title", "Untitled")}\n\n'
        context += (
            f"## {doc.metadata.get('description', 'No description available')}\n\n"
        )
        context += f"{doc.page_content}\n\n"
    return context


parser = JsonOutputParser(pydantic_object=AIResponseFormat)

chain = (
    {"context": lambda x: x["context"], "json_format": lambda x: x["json_format"]}
    | prompt
    | llm
    | parser
)

context = spencers_loader_func()
result = chain.invoke({"context": context, "json_format": json_format})
print(json.dumps(result, indent=4))

with open("firecrawl_test_results.json", "w") as f:
    json.dump(result, f, indent=4)

        ```

        src/other/firecrawl_test_results.md:
        ```
# Faculty & Staff Directory | Salisbury University

## View contact information for all faculty and staff at SU in our directory.

This website uses cookies to ensure the best user experience.
[Learn more](https://www.cookiesandyou.com)

Got It!

[Skip to Main Content](#main) [Skip to Header](#js-global-header) [Skip to Footer](#js-global-footer)

[Salisbury University](/)

- [Explore Academics](/explore-academics/)  - [Find Your Program](/explore-academics/programs/)
  - [Colleges & Schools](https://www.salisbury.edu/explore-academics/colleges-schools-and-departments.aspx)
  - [Graduate School](https://www.salisbury.edu/explore-academics/graduate-school.aspx)
  - [English Language Institute](/administration/academic-affairs/center-for-international-education/english-language-institute/)
  - [Libraries & Academic Resources](https://www.salisbury.edu/explore-academics/student-resources.aspx)
  - [Our Faculty](https://www.salisbury.edu/explore-academics/our-faculty.aspx)
  - [Regional Programs](https://www.salisbury.edu/administration/academic-affairs/regional-programs.aspx)
  - [Research](/explore-academics/research/)
  - [Study Abroad](https://www.salisbury.edu/explore-academics/study-abroad.aspx)
  - [Internships](https://www.salisbury.edu/explore-academics/internships.aspx)
- [Admissions & Aid](/admissions/)  - [Visit Campus](https://www.salisbury.edu/admissions/visit-campus.aspx)
  - [Apply Now](https://www.salisbury.edu/admissions/apply-now.aspx)
  - [Financial Aid](/admissions/financial-aid/)
  - [Cost of Attendance](/admissions/financial-aid/cost-of-attendance/)
  - [Admitted Students](https://www.salisbury.edu/admissions/admitted-students.aspx)
  - [New Student Orientation](/admissions/orientation/)
- [Experience Campus](/experience-campus/)  - [Diversity & Inclusion](https://www.salisbury.edu/experience-campus/diversity-and-inclusion.aspx)
  - [Housing & Residence Life](https://www.salisbury.edu/experience-campus/residence-life.aspx)
  - [University Dining](https://www.salisbury.edu/experience-campus/university-dining.aspx)
  - [Athletics](/experience-campus/athletics/)
  - [Clubs & Organizations](/administration/student-affairs/center-for-student-involvement-and-leadership/)
  - [Student Services](https://www.salisbury.edu/experience-campus/student-services.aspx)
  - [Events](/events/)
  - [News](/news/)
  - [Facilities](https://www.salisbury.edu/experience-campus/facilities.aspx)
- [Discover SU](/discover-su/)  - [Outcomes & Rankings](https://www.salisbury.edu/discover-su/outcomes.aspx)
  - [Mission & Values](https://www.salisbury.edu/discover-su/mission-values.aspx)
  - [Campus Leadership](https://www.salisbury.edu/discover-su/campus-leadership.aspx)
  - [Campus History](/discover-su/campus-history/)
  - [Community Outreach](/discover-su/community-outreach/)
  - [Campus Spotlights](https://www.salisbury.edu/discover-su/campus-spotlights.aspx)
  - [Offices and Departments](https://www.salisbury.edu/discover-su/offices-and-departments.aspx)
  - [Our Region](https://www.salisbury.edu/discover-su/our-region.aspx)
  - [Campus Map & Directions](/discover-su/campus-map/)

- [Calendars & Events](/events/)
- [Students](/current-students/)
- [Faculty & Staff](/employees/)
- [Alumni](/alumni/)
- [Give](https://giving.salisbury.edu/pages/salisbury-home-page)

Search Salisbury UniversitySearchSearch

Common Searches

- [Admissions](/admissions/)
- [Campus Calendars](/calendars/)
- [Employment Opportunities](/administration/administration-and-finance-offices/human-resources/careers/)
- [Majors & Minors](/explore-academics/programs/)
- [News](/news/) & [Events](/events/)

- [GullNet](/gullnet-login.aspx)
- [IT Help Desk](/administration/administration-and-finance-offices/information-technology/help-desk/)
- [Mobile Printing](/mobileprint/)
- [MyClasses](/administration/academic-affairs/instructional-design-delivery/cms/)
- [Registrar](/administration/academic-affairs/registrar/)

- [Directory (Faculty & Staff)](/faculty-and-staff/)
- [Offices & Departments](/discover-su/offices-and-departments.aspx)
- [Map of the Campus](/discover-su/campus-map/)
- [Visitor Parking](/administration/administration-and-finance-offices/financial-services/accounts-receivable-cashiers-office/parking-services/visitor-permits.aspx)
- [University Police](/police/)

![Faculty & Staff](https://www.salisbury.edu/_files/img/masthead/faculty.jpg)

Directory

## Filters

Filter By Personnel Type

- All Faculty & Staff
- Faculty
- Staff

Filter By Area

- Filter By Area


- All Schools
- Fulton School of Liberal Arts
- College of Health and Human Services
- Graduate Studies
- Henson School of Science & Technology
- Libraries
- Perdue School of Business
- Seidel School of Education

[Reset Filters](/faculty-and-staff/)

Search

- Search


- Enter search term:
Submit

Printable Directory

- Printable Directory


- [Directory (PDF)](https://webapps.salisbury.edu/directory/pdf/output/directory.pdf)
- [Front Section (PDF)](https://webapps.salisbury.edu/directory/pdf/output/Directory_front.pdf)
- [Directory Cover (PDF)](https://webapps.salisbury.edu/directory/pdf/output/DirectoryCov.pdf)

Update Request

- Request Change


- [Request a change to the online directory](https://salisbury.co1.qualtrics.com/jfe/form/SV_d5QAbPuc40kvJtA)

**Follow us on Social Media**

[Social Media Directory](/discover-su/social-media-directory.aspx)

# Faculty & Staff Directory

All Faculty & Staff

![AJAX Loading Symbol](https://www.salisbury.edu/_files/img/ajax-loader.gif)

[Visit Salisbury](https://www.salisbury.edu/admissions/visit-campus.aspx)

[Apply Now](https://www.salisbury.edu/admissions/apply-now.aspx)

[Request Info](/admissions/request-info/)

1101 Camden AvenueSalisbury, MD 21801

[410-543-6000](tel:4105436000)

- [News and Events](/news/)
- [Academic Calendar](/administration/academic-affairs/registrar/)
- [Libraries](/libraries/)

- [Give to SU](https://giving.salisbury.edu/pages/salisbury-home-page)
- [Careers](/administration/administration-and-finance-offices/human-resources/careers/)
- [Bookstore](https://www.salisbury.edu/administration/student-affairs/book-store.aspx)

- [Like Salisbury University on Facebook](https://www.facebook.com/SalisburyU/ "Like Salisbury University on Facebook")
- [Follow Us on Twitter](https://twitter.com/salisburyu "Follow Us on Twitter")
- [Connect With Salisbury University on LinkedIn](https://www.linkedin.com/school/salisbury-university/ "Connect With Salisbury University on LinkedIn")
- [Visit Salisbury University on Flickr](https://www.flickr.com/photos/supublications/ "Visit Salisbury University on Flickr")
- [Visit Salisbury University's YouTube Channel](https://www.youtube.com/user/salisburyuniversity "Visit Salisbury University's YouTube Channel")
- [Follow Salisbury University on instagram](https://www.instagram.com/salisburyuniversity/ "Follow Salisbury University on instagram")

- [Privacy](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/privacy.aspx)
- [Web Accessibility](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/web-accessibility.aspx)
- [Terms of Use](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/terms-of-use.aspx)
- [Copyright](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/copyright.aspx)
- [Student Consumer Information](https://www.salisbury.edu/administration/general-counsel/student-consumer-information.aspx)

Â© 2024 Salisbury University.


        ```

        src/other/firecrawl_test_results_only_cs.md:
        ```
# Department of Computer Science Faculty & Staff | Salisbury University

## The department of computer science at Salisbury University, faculty consists of full and part-time staff dedicated to seeing you succeed. 

This website uses cookies to ensure the best user experience.
[Learn more](https://cookiesandyou.com)

Got It!

[Skip to Main Content](#main) [Skip to Header](#js-global-header) [Skip to Footer](#js-global-footer)

[Salisbury University](/)

- [Explore Academics](/explore-academics/)  - [Find Your Program](/explore-academics/programs/)
  - [Colleges & Schools](https://www.salisbury.edu/explore-academics/colleges-schools-and-departments.aspx)
  - [Graduate School](https://www.salisbury.edu/explore-academics/graduate-school.aspx)
  - [English Language Institute](/administration/academic-affairs/center-for-international-education/english-language-institute/)
  - [Libraries & Academic Resources](https://www.salisbury.edu/explore-academics/student-resources.aspx)
  - [Our Faculty](https://www.salisbury.edu/explore-academics/our-faculty.aspx)
  - [Regional Programs](https://www.salisbury.edu/administration/academic-affairs/regional-programs.aspx)
  - [Research](/explore-academics/research/)
  - [Study Abroad](https://www.salisbury.edu/explore-academics/study-abroad.aspx)
  - [Internships](https://www.salisbury.edu/explore-academics/internships.aspx)
- [Admissions & Aid](/admissions/)  - [Visit Campus](https://www.salisbury.edu/admissions/visit-campus.aspx)
  - [Apply Now](https://www.salisbury.edu/admissions/apply-now.aspx)
  - [Financial Aid](/admissions/financial-aid/)
  - [Cost of Attendance](/admissions/financial-aid/cost-of-attendance/)
  - [Admitted Students](https://www.salisbury.edu/admissions/admitted-students.aspx)
  - [New Student Orientation](/admissions/orientation/)
- [Experience Campus](/experience-campus/)  - [Diversity & Inclusion](https://www.salisbury.edu/experience-campus/diversity-and-inclusion.aspx)
  - [Housing & Residence Life](https://www.salisbury.edu/experience-campus/residence-life.aspx)
  - [University Dining](https://www.salisbury.edu/experience-campus/university-dining.aspx)
  - [Athletics](/experience-campus/athletics/)
  - [Clubs & Organizations](/administration/student-affairs/center-for-student-involvement-and-leadership/)
  - [Student Services](https://www.salisbury.edu/experience-campus/student-services.aspx)
  - [Events](/events/)
  - [News](/news/)
  - [Facilities](https://www.salisbury.edu/experience-campus/facilities.aspx)
- [Discover SU](/discover-su/)  - [Outcomes & Rankings](https://www.salisbury.edu/discover-su/outcomes.aspx)
  - [Mission & Values](https://www.salisbury.edu/discover-su/mission-values.aspx)
  - [Campus Leadership](https://www.salisbury.edu/discover-su/campus-leadership.aspx)
  - [Campus History](/discover-su/campus-history/)
  - [Community Outreach](/discover-su/community-outreach/)
  - [Campus Spotlights](https://www.salisbury.edu/discover-su/campus-spotlights.aspx)
  - [Offices and Departments](https://www.salisbury.edu/discover-su/offices-and-departments.aspx)
  - [Our Region](https://www.salisbury.edu/discover-su/our-region.aspx)
  - [Campus Map & Directions](/discover-su/campus-map/)

- [Calendars & Events](/events/)
- [Students](/current-students/)
- [Faculty & Staff](/employees/)
- [Alumni](/alumni/)
- [Give](https://giving.salisbury.edu/pages/salisbury-home-page)

Search Salisbury UniversitySearchSearch

Common Searches

- [Admissions](/admissions/)
- [Campus Calendars](/calendars/)
- [Employment Opportunities](/administration/administration-and-finance-offices/human-resources/careers/)
- [Majors & Minors](/explore-academics/programs/)
- [News](/news/) & [Events](/events/)

- [GullNet](/gullnet-login.aspx)
- [IT Help Desk](/administration/administration-and-finance-offices/information-technology/help-desk/)
- [Mobile Printing](/mobileprint/)
- [MyClasses](/administration/academic-affairs/instructional-design-delivery/cms/)
- [Registrar](/administration/academic-affairs/registrar/)

- [Directory (Faculty & Staff)](/faculty-and-staff/)
- [Offices & Departments](/discover-su/offices-and-departments.aspx)
- [Map of the Campus](/discover-su/campus-map/)
- [Visitor Parking](/administration/administration-and-finance-offices/financial-services/accounts-receivable-cashiers-office/parking-services/visitor-permits.aspx)
- [University Police](/police/)

![Salisbury University students on campus](https://0utwqfl7.cdn.imgeng.in/_images/wave/gac-students-default-masthead.png)

1. [Go back to the Salisbury University home page](/)
2. [Henson](/academic-offices/science-and-technology/)
3. [Computer Science](/academic-offices/science-and-technology/computer-science/)
4. Faculty & Staff

## Computer Science

### [Up One Section](/academic-offices/science-and-technology/)

- [Section Home](/academic-offices/science-and-technology/computer-science/)

- [What's New](/academic-offices/science-and-technology/computer-science/news/)
- Faculty & Staff
- [Courses](courses.aspx)
- [Schedules & Syllabi](schedules-and-syllabi.aspx)
- [Undergraduate Research](undergraduate-research.aspx)
- [Internships](/academic-offices/science-and-technology/computer-science/internships/)
- [Academic Help Center](academic-assistance.aspx)
- [Student Clubs](student-clubs.aspx)
- [Awards & Honors](awards-honors.aspx)
- [Student Testimonials](student-testimonials.aspx)
- [Eastern Shore High School Computer Programming Competition](eshs-coding-competition.aspx)
- [Scholarship Opportunities](scholarship.aspx)

# Faculty & Staff

- [Full Time Faculty](javascript:void(0);)
[![Giulia Franchi](https://www.salisbury.edu//_images/directory/gxfranchi.jpg)\\
\\
**Giulia Franchi** \\
\\
* * *\\
\\
**Associate Professor / Computer Science  Conway Hall (TE) 201D**\\
\\
- P410-543-6145\\
- E@](/faculty-and-staff/gxfranchi)

[![Yaping Jing](https://www.salisbury.edu//_images/directory/yxjing.jpg)\\
\\
**Yaping Jing** \\
\\
* * *\\
\\
**Associate Professor / Computer Science  Conway Hall (TE) 202**\\
\\
- P410-543-6470\\
- E@](/faculty-and-staff/yxjing)

[![Enyue Lu](https://www.salisbury.edu//_images/directory/ealu.jpg)\\
\\
**Enyue Lu** \\
\\
* * *\\
\\
**Professor / Computer Science  Conway Hall (TE) 201E**\\
\\
- P410-543-6144\\
- E@](/faculty-and-staff/ealu)

[![Sang-Eon Park](https://www.salisbury.edu//_images/directory/sxpark.jpg)\\
\\
**Sang-Eon Park** \\
\\
* * *\\
\\
**Associate Professor / Computer Science  Conway Hall (TE) 205**\\
\\
- P410-677-5007\\
- E@](/faculty-and-staff/sxpark)

[![Andrew Thompson](https://www.salisbury.edu//_images/directory/awthompson.jpg)\\
\\
**Andrew Thompson** \\
\\
* * *\\
\\
**Lecturer / Computer Science  Conway Hall (TE) 208**\\
\\
- E@](/faculty-and-staff/awthompson)

[![Junyi Tu](https://www.salisbury.edu//_images/directory/jxtu.jpg)\\
\\
**Junyi Tu** \\
\\
* * *\\
\\
**Assistant Professor / Computer Science  Conway Hall (TE) 203**\\
\\
- P410-677-0027\\
- E@](/faculty-and-staff/jxtu)

[![Shuangquan Wang](https://www.salisbury.edu//_images/directory/spwang.jpg)\\
\\
**Shuangquan Wang** \\
\\
* * *\\
\\
**Assistant Professor / Computer Science  Conway Hall (TE) 204**\\
\\
- P410-677-0032\\
- E@](/faculty-and-staff/spwang)

[![Xiaohong Wang](https://www.salisbury.edu//_images/directory/xswang.jpg)\\
\\
**Xiaohong Wang** \\
\\
* * *\\
\\
**Professor, Chair / Computer Science  Conway Hall (TE) 201C**\\
\\
- P410-677-5380\\
- E@](/faculty-and-staff/xswang)

[![Soeun You](https://www.salisbury.edu//_images/directory/sxyou.jpg)\\
\\
**Soeun You** \\
\\
* * *\\
\\
**Lecturer / Computer Science  Conway Hall (TE) 201F**\\
\\
- P410-677-0177\\
- E@](/faculty-and-staff/sxyou)

- [Administrative Staff](javascript:void(0);)
[![Lauren Ruark](https://www.salisbury.edu//_images/directory/su-seal.jpg)\\
\\
**Lauren Ruark** \\
\\
* * *\\
\\
**Administrative Assistant Ii / Computer Science**\\
\\
- P410-543-6140\\
- E@](/faculty-and-staff/laruark)

## [Computer Science](/academic-offices/science-and-technology/computer-science/)

- [What's New](/academic-offices/science-and-technology/computer-science/news/)
- Faculty & Staff
- [Courses](courses.aspx)
- [Schedules & Syllabi](schedules-and-syllabi.aspx)
- [Undergraduate Research](undergraduate-research.aspx)
- [Internships](/academic-offices/science-and-technology/computer-science/internships/)
- [Academic Help Center](academic-assistance.aspx)
- [Student Clubs](student-clubs.aspx)
- [Awards & Honors](awards-honors.aspx)
- [Student Testimonials](student-testimonials.aspx)
- [Eastern Shore High School Computer Programming Competition](eshs-coding-competition.aspx)
- [Scholarship Opportunities](scholarship.aspx)

## Contact Information

Lauren RuarkAdministrative Assistant IIConway Hall (TE) 201

- P [410-543-6140](tel:410-543-6140)
- E [laruark@salisbury.edu](mailto:laruark@salisbury.edu)

[Visit Salisbury](https://www.salisbury.edu/admissions/visit-campus.aspx)

[Apply Now](https://www.salisbury.edu/admissions/apply-now.aspx)

[Request Info](/admissions/request-info/)

1101 Camden AvenueSalisbury, MD 21801

[410-543-6000](tel:4105436000)

- [News and Events](/news/)
- [Academic Calendar](/administration/academic-affairs/registrar/)
- [Libraries](/libraries/)

- [Give to SU](https://giving.salisbury.edu/pages/salisbury-home-page)
- [Careers](/administration/administration-and-finance-offices/human-resources/careers/)
- [Bookstore](https://www.salisbury.edu/administration/student-affairs/book-store.aspx)

- [Like Salisbury University on Facebook](https://www.facebook.com/SalisburyU/ "Like Salisbury University on Facebook")
- [Follow Us on Twitter](https://twitter.com/salisburyu "Follow Us on Twitter")
- [Connect With Salisbury University on LinkedIn](https://www.linkedin.com/school/salisbury-university/ "Connect With Salisbury University on LinkedIn")
- [Visit Salisbury University on Flickr](https://www.flickr.com/photos/supublications/ "Visit Salisbury University on Flickr")
- [Visit Salisbury University's YouTube Channel](https://www.youtube.com/user/salisburyuniversity "Visit Salisbury University's YouTube Channel")
- [Follow Salisbury University on instagram](https://www.instagram.com/salisburyuniversity/ "Follow Salisbury University on instagram")

- [Privacy](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/privacy.aspx)
- [Web Accessibility](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/web-accessibility.aspx)
- [Terms of Use](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/terms-of-use.aspx)
- [Copyright](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/copyright.aspx)
- [Student Consumer Information](https://www.salisbury.edu/administration/general-counsel/student-consumer-information.aspx)

Â© 2024 Salisbury University.


        ```

        src/other/fullRecs_to_splitRecs.py:
        ```
from utilities import Utilities
import json


def main():
    path_to_file = "savedrecs-5.txt"
    output_dir = "./split_files_2"

    utils = Utilities()

    file_paths = utils.make_files(path_to_file=path_to_file, output_dir=output_dir)

    # Save file_paths to a JSON file
    with open("file_paths.json", "w") as fp:
        json.dump(file_paths, fp)

    # Print entries to ensure the right amount were made
    i = 0
    for entry, path in file_paths.items():
        print(f"Entry {entry} saved to: {path}")
        print(f"Entry Number: {i}")
        i += 1


def load_and_print_entry(entry_number):
    # Load file paths from the JSON file
    with open("file_paths.json", "r") as fp:
        file_paths = json.load(fp)

    # Access and print the specified entry
    path = file_paths.get(str(entry_number))  # JSON keys are always strings

    if path:
        print(f"Entry: {entry_number} saved to: {path}")
    else:
        print(f"No entry found for number: {entry_number}")


if __name__ == "__main__":
    # If commented out, uncomment the below line if you need to run the script to make files again
    main()

    # If commented out, uncomment the line below to load and print a specific entry without rerunning the entire program
    # for i in range(1, 655):
    #   load_and_print_entry(i)
    #  print('\n')

        ```

        src/other/get_absracts.py:
        ```
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utilities import Utilities
import json
from enums import AttributeTypes

if __name__ == "__main__":
    split_files_path = "./split_files"
    utilities = Utilities()
    all_abstracts = []
    for file in os.listdir(split_files_path):
        with open(os.path.join(split_files_path, file), "r") as f:
            text = f.read()
            abstract = utilities.get_attributes(text, [AttributeTypes.ABSTRACT])[
                AttributeTypes.ABSTRACT
            ][1]
            all_abstracts.append(abstract)
            print(abstract)
    abstract_dict = {"abstracts": all_abstracts}
    with open("ALL_ABSTRACTS.json", "w") as f:
        json.dump(abstract_dict, f, indent=4)

        ```

        src/other/get_crossref_ab.py:
        ```
import ijson
import json
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utilities import Utilities
from enums import AttributeTypes


def get_crossref_ab(file_path: str, output_file_path: str, utils: Utilities):
    items = ijson.items(open(file_path, "r"), "item")
    abstracts = []
    for item in items:
        abstract = utils.get_attributes(item, [AttributeTypes.CROSSREF_ABSTRACT])[
            AttributeTypes.CROSSREF_ABSTRACT
        ][1]
        if abstract != None and abstract != "":
            abstracts.append(abstract)

    with open(output_file_path, "w") as file:
        json.dump(abstracts, file, indent=4)


if __name__ == "__main__":
    utils = Utilities()
    file_path = "paper-doi-list.json"
    output_file_path = "paper-abstracts-list.json"
    get_crossref_ab(file_path, output_file_path, utils)

        ```

        src/other/get_crossref_ab_title.py:
        ```
import ijson
import json
import sys
import os
from unidecode import unidecode

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from academic_metrics.factories import StrategyFactory
from academic_metrics.utils import WarningManager, Utilities, FileHandler
from academic_metrics.enums import AttributeTypes


def clean_unicode_escapes(text):
    clean_text = unidecode(text)
    return clean_text


def get_crossref_ab_title(file_path: str, output_file_path: str, utils: Utilities):
    items = ijson.items(open(file_path, "r"), "item")
    paper_dict = {}
    for item in items:
        abstract = utils.get_attributes(item, [AttributeTypes.CROSSREF_ABSTRACT])[
            AttributeTypes.CROSSREF_ABSTRACT
        ][1]
        # abstract = abstract[1]
        title_list = utils.get_attributes(item, [AttributeTypes.CROSSREF_TITLE])[
            AttributeTypes.CROSSREF_TITLE
        ][1]
        # Ensure title_list is a list
        if not isinstance(title_list, list):
            title_list = [title_list]
        title = clean_unicode_escapes(title_list[0])
        if abstract != None and abstract != "":
            paper_dict[title] = clean_unicode_escapes(abstract)
        else:
            print(title)

    with open(output_file_path, "w") as file:
        json.dump(paper_dict, file, indent=4)


if __name__ == "__main__":
    utils = Utilities(strategy_factory=StrategyFactory, warning_manager=WarningManager)
    file_path = "fullData.json"
    output_file_path = "title-abstract.json"
    get_crossref_ab_title(file_path, output_file_path, utils)

        ```

        src/other/get_crossref_authors.py:
        ```
import ijson
import json
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utilities import Utilities
import logging
from enums import AttributeTypes

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    filename="crossref_authors_extraction.log",
    filemode="w",
)


def get_crossref_ab(file_path: str, output_file_path: str, utils: Utilities):
    items = ijson.items(open(file_path, "r"), "item")
    authors = []
    for i, item in enumerate(items):
        new_authors = utils.get_attributes(item, [AttributeTypes.CROSSREF_AUTHORS])[
            AttributeTypes.CROSSREF_AUTHORS
        ][1]
        authors.extend(new_authors)
        logging.info(
            f"Item {i}: Added {len(new_authors)} authors. Total authors: {len(authors)}"
        )
        if i % 100 == 0:
            logging.info(f"Current authors list size: {sys.getsizeof(authors)} bytes")
    authors_dict = {"authors": authors}
    logging.info(f"Final authors list size: {sys.getsizeof(authors)} bytes")
    logging.info(f"Final authors dict size: {sys.getsizeof(authors_dict)} bytes")

    with open(output_file_path, "w") as file:
        json.dump(authors_dict, file, indent=4)
    logging.info(f"Output file size: {os.path.getsize(output_file_path)} bytes")


if __name__ == "__main__":
    utils = Utilities()
    file_path = "paper-doi-list.json"
    output_file_path = "paper-authors-list.json"
    get_crossref_ab(file_path, output_file_path, utils)

        ```

        src/other/get_crossref_titles.py:
        ```
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utilities import Utilities
import json
import ijson
from enums import AttributeTypes


def get_crossref_titles(file_path: str, output_file_path: str, utils: Utilities):
    items = ijson.items(open(file_path, "r"), "item")
    title_dict = {}
    titles = []
    for item in items:
        title_list = utils.get_attributes(item, [AttributeTypes.CROSSREF_TITLE])[
            AttributeTypes.CROSSREF_TITLE
        ][1]
        # Ensure title_list is a list
        if not isinstance(title_list, list):
            title_list = [title_list]
        for title in title_list:
            print(title)
            titles.append(title)
    title_dict["CrossrefTitles"] = titles
    with open(output_file_path, "w") as file:
        json.dump(title_dict, file, indent=4)


if __name__ == "__main__":
    utils = Utilities()
    file_path = "paper-doi-list.json"
    output_file_path = "paper-titles-list.json"
    get_crossref_titles(file_path, output_file_path, utils)

        ```

        src/other/get_n_items_from_crossref_static_data.py:
        ```
import ijson
import json


def save_items_to_file(*, path: str, items: list[dict]) -> None:
    with open(path, "w") as f:
        json.dump(items, f, indent=4)
    print("Saved Successfully")


def get_n_items(*, path):
    n = 0
    with open(path, "r") as f:
        data = json.load(f)
    n = len(data)
    save_items_to_file(path=f"input_files/{n}-paper-doi-list.json", items=data)


if __name__ == "__main__":
    path = "./test.json"
    get_n_items(path=path)

        ```

        src/other/get_titles.py:
        ```
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utilities import Utilities
import json
from enums import AttributeTypes

if __name__ == "__main__":
    split_files_path = "./split_files_2"
    utilities = Utilities()
    all_titles = []
    for file in os.listdir(split_files_path):
        with open(os.path.join(split_files_path, file), "r") as f:
            text = f.read()
            title = utilities.get_attributes(text, [AttributeTypes.TITLE])[
                AttributeTypes.TITLE
            ][1]
            if title is None:
                print(file)
                input("Press Enter to continue...")
                continue
            all_titles.append(title)
    with open("2024_titles.json", "w") as f:
        titles_dict = {
            "year": 2024,
            "titles_count": len(all_titles),
            "titles": all_titles,
        }
        json.dump(titles_dict, f, indent=4)

        ```

        src/other/guide.md:
        ```
Have some crossref jsons 

do the following for each json object

<!-- 1) go through current json object and extract salisbury authors, if there are no salisbury authors, not a salisbury paper, throw away -->
2) go through current json object and extract abstract
3) pass the abstract to get categorized
4) get categoreis back, and insert them into the json object

do that for each json object then you have categories in all the json objects
run process as normal


categories for abstract

for each of those categories, create a categoryInfo object
parsing through the same json object pulling out salisbury authors, adding them in, pulling out departments, adding them in, pulling out citations adding em in, pulling out citation count adding them in. 

Say now go to next json object

abstract gets categorized into different categories, do the same thing, create a categoryInfo object, add in the salisbury authors, add in the departments, add in the citations, add in the citation count. 

Say now go to next json object
abstracts gets categorized into a category that already has some stuff, add all that info into the categoryInfo object.


=============================================================================================================

When start Wos_classification.py, it takes in a big ass wos record file, it splits these files into individual records, and slaps them in a directory. 

It goes over each file in there (each record) and does all that shit that i just wrote above.

=============================================================================================================
have all the json objects

we pass in the list of json objects, and perform the above steps.



        ```

        src/other/json_transformer.py:
        ```
import json


def convert_sets_to_lists(obj):
    """Recursively convert sets in the object to lists for JSON serialization."""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {k: convert_sets_to_lists(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj


class JsonTransformer:
    def __init__(self):
        # Initialization can include any required setup. The prefix is no longer needed.
        pass

    def make_dict_json(self, dictionary):
        """
        Serializes a dictionary to a JSON file, converting sets to lists as needed.
        Assumes that the dictionary values may already be partially serialized by CategoryInfo.to_dict().

        Parameters:
            dictionary (dict): The dictionary to serialize, where values are expected to be dictionaries themselves.
        """
        # Convert sets to lists in the dictionary for JSON serialization.
        # This is a precaution in case there are nested sets not handled by CategoryInfo.to_dict().
        new_dictionary = convert_sets_to_lists(dictionary)

        # Serialize the dictionary to JSON, writing to 'categories_and_category_metadata.json'.
        with open("categories_and_category_metadata.json", "w") as json_file:
            json.dump(new_dictionary, json_file, indent=4)


if __name__ == "__main__":
    jt = JsonTransformer()
    jt.remove_files()

        ```

        src/other/professor_dump_to_pandas.py:
        ```
from GeneralUtilities.tracing.tracer import trace
from GeneralUtilities.file_ops.file_ops import read_csv
import pandas as pd
import csv


class ProfessorDumpToPandas:
    def __init__(self, csv_path: str):
        self.csv_path = csv_path

    def read_csv(self):
        return pd.read_csv(self.csv_path)

        ```

        src/other/raw_firecrawl_test_results_only_cs.md:
        ```
METADATA: {'title': 'Department of Computer Science Faculty & Staff | Salisbury University', 'description': 'The department of computer science at Salisbury University, faculty consists of full and part-time staff dedicated to seeing you succeed. ', 'language': 'en', 'robots': 'index, follow', 'ogTitle': 'Department of Computer Science Faculty & Staff', 'ogDescription': 'The department of computer science at Salisbury University, faculty consists of full and part-time staff dedicated to seeing you succeed. ', 'ogUrl': 'https://www.salisbury.edu/academic-offices/science-and-technology/computer-science/faculty-and-staff.aspx', 'ogImage': 'https://www.salisbury.edu/_images/SU-Social-Default.png', 'ogLocaleAlternate': [], 'ogSiteName': 'Salisbury University', 'sourceURL': 'https://www.salisbury.edu/academic-offices/science-and-technology/computer-science/faculty-and-staff.aspx', 'pageStatusCode': 200}

PAGE CONTENT: This website uses cookies to ensure the best user experience.
[Learn more](https://cookiesandyou.com)

Got It!

[Skip to Main Content](#main) [Skip to Header](#js-global-header) [Skip to Footer](#js-global-footer)

[Salisbury University](/)

- [Explore Academics](/explore-academics/)  - [Find Your Program](/explore-academics/programs/)
  - [Colleges & Schools](https://www.salisbury.edu/explore-academics/colleges-schools-and-departments.aspx)
  - [Graduate School](https://www.salisbury.edu/explore-academics/graduate-school.aspx)
  - [English Language Institute](/administration/academic-affairs/center-for-international-education/english-language-institute/)
  - [Libraries & Academic Resources](https://www.salisbury.edu/explore-academics/student-resources.aspx)
  - [Our Faculty](https://www.salisbury.edu/explore-academics/our-faculty.aspx)
  - [Regional Programs](https://www.salisbury.edu/administration/academic-affairs/regional-programs.aspx)
  - [Research](/explore-academics/research/)
  - [Study Abroad](https://www.salisbury.edu/explore-academics/study-abroad.aspx)
  - [Internships](https://www.salisbury.edu/explore-academics/internships.aspx)
- [Admissions & Aid](/admissions/)  - [Visit Campus](https://www.salisbury.edu/admissions/visit-campus.aspx)
  - [Apply Now](https://www.salisbury.edu/admissions/apply-now.aspx)
  - [Financial Aid](/admissions/financial-aid/)
  - [Cost of Attendance](/admissions/financial-aid/cost-of-attendance/)
  - [Admitted Students](https://www.salisbury.edu/admissions/admitted-students.aspx)
  - [New Student Orientation](/admissions/orientation/)
- [Experience Campus](/experience-campus/)  - [Diversity & Inclusion](https://www.salisbury.edu/experience-campus/diversity-and-inclusion.aspx)
  - [Housing & Residence Life](https://www.salisbury.edu/experience-campus/residence-life.aspx)
  - [University Dining](https://www.salisbury.edu/experience-campus/university-dining.aspx)
  - [Athletics](/experience-campus/athletics/)
  - [Clubs & Organizations](/administration/student-affairs/center-for-student-involvement-and-leadership/)
  - [Student Services](https://www.salisbury.edu/experience-campus/student-services.aspx)
  - [Events](/events/)
  - [News](/news/)
  - [Facilities](https://www.salisbury.edu/experience-campus/facilities.aspx)
- [Discover SU](/discover-su/)  - [Outcomes & Rankings](https://www.salisbury.edu/discover-su/outcomes.aspx)
  - [Mission & Values](https://www.salisbury.edu/discover-su/mission-values.aspx)
  - [Campus Leadership](https://www.salisbury.edu/discover-su/campus-leadership.aspx)
  - [Campus History](/discover-su/campus-history/)
  - [Community Outreach](/discover-su/community-outreach/)
  - [Campus Spotlights](https://www.salisbury.edu/discover-su/campus-spotlights.aspx)
  - [Offices and Departments](https://www.salisbury.edu/discover-su/offices-and-departments.aspx)
  - [Our Region](https://www.salisbury.edu/discover-su/our-region.aspx)
  - [Campus Map & Directions](/discover-su/campus-map/)

- [Calendars & Events](/events/)
- [Students](/current-students/)
- [Faculty & Staff](/employees/)
- [Alumni](/alumni/)
- [Give](https://giving.salisbury.edu/pages/salisbury-home-page)

Search Salisbury UniversitySearchSearch

Common Searches

- [Admissions](/admissions/)
- [Campus Calendars](/calendars/)
- [Employment Opportunities](/administration/administration-and-finance-offices/human-resources/careers/)
- [Majors & Minors](/explore-academics/programs/)
- [News](/news/) & [Events](/events/)

- [GullNet](/gullnet-login.aspx)
- [IT Help Desk](/administration/administration-and-finance-offices/information-technology/help-desk/)
- [Mobile Printing](/mobileprint/)
- [MyClasses](/administration/academic-affairs/instructional-design-delivery/cms/)
- [Registrar](/administration/academic-affairs/registrar/)

- [Directory (Faculty & Staff)](/faculty-and-staff/)
- [Offices & Departments](/discover-su/offices-and-departments.aspx)
- [Map of the Campus](/discover-su/campus-map/)
- [Visitor Parking](/administration/administration-and-finance-offices/financial-services/accounts-receivable-cashiers-office/parking-services/visitor-permits.aspx)
- [University Police](/police/)

![Salisbury University students on campus](https://0utwqfl7.cdn.imgeng.in/_images/wave/gac-students-default-masthead.png)

1. [Go back to the Salisbury University home page](/)
2. [Henson](/academic-offices/science-and-technology/)
3. [Computer Science](/academic-offices/science-and-technology/computer-science/)
4. Faculty & Staff

## Computer Science

### [Up One Section](/academic-offices/science-and-technology/)

- [Section Home](/academic-offices/science-and-technology/computer-science/)

- [What's New](/academic-offices/science-and-technology/computer-science/news/)
- Faculty & Staff
- [Courses](courses.aspx)
- [Schedules & Syllabi](schedules-and-syllabi.aspx)
- [Undergraduate Research](undergraduate-research.aspx)
- [Internships](/academic-offices/science-and-technology/computer-science/internships/)
- [Academic Help Center](academic-assistance.aspx)
- [Student Clubs](student-clubs.aspx)
- [Awards & Honors](awards-honors.aspx)
- [Student Testimonials](student-testimonials.aspx)
- [Eastern Shore High School Computer Programming Competition](eshs-coding-competition.aspx)
- [Scholarship Opportunities](scholarship.aspx)

# Faculty & Staff

- [Full Time Faculty](javascript:void(0);)
[![Giulia Franchi](https://www.salisbury.edu//_images/directory/gxfranchi.jpg)\\
\\
**Giulia Franchi** \\
\\
* * *\\
\\
**Associate Professor / Computer Science  Conway Hall (TE) 201D**\\
\\
- P410-543-6145\\
- E@](/faculty-and-staff/gxfranchi)

[![Yaping Jing](https://www.salisbury.edu//_images/directory/yxjing.jpg)\\
\\
**Yaping Jing** \\
\\
* * *\\
\\
**Associate Professor / Computer Science  Conway Hall (TE) 202**\\
\\
- P410-543-6470\\
- E@](/faculty-and-staff/yxjing)

[![Enyue Lu](https://www.salisbury.edu//_images/directory/ealu.jpg)\\
\\
**Enyue Lu** \\
\\
* * *\\
\\
**Professor / Computer Science  Conway Hall (TE) 201E**\\
\\
- P410-543-6144\\
- E@](/faculty-and-staff/ealu)

[![Sang-Eon Park](https://www.salisbury.edu//_images/directory/sxpark.jpg)\\
\\
**Sang-Eon Park** \\
\\
* * *\\
\\
**Associate Professor / Computer Science  Conway Hall (TE) 205**\\
\\
- P410-677-5007\\
- E@](/faculty-and-staff/sxpark)

[![Andrew Thompson](https://www.salisbury.edu//_images/directory/awthompson.jpg)\\
\\
**Andrew Thompson** \\
\\
* * *\\
\\
**Lecturer / Computer Science  Conway Hall (TE) 208**\\
\\
- E@](/faculty-and-staff/awthompson)

[![Junyi Tu](https://www.salisbury.edu//_images/directory/jxtu.jpg)\\
\\
**Junyi Tu** \\
\\
* * *\\
\\
**Assistant Professor / Computer Science  Conway Hall (TE) 203**\\
\\
- P410-677-0027\\
- E@](/faculty-and-staff/jxtu)

[![Shuangquan Wang](https://www.salisbury.edu//_images/directory/spwang.jpg)\\
\\
**Shuangquan Wang** \\
\\
* * *\\
\\
**Assistant Professor / Computer Science  Conway Hall (TE) 204**\\
\\
- P410-677-0032\\
- E@](/faculty-and-staff/spwang)

[![Xiaohong Wang](https://www.salisbury.edu//_images/directory/xswang.jpg)\\
\\
**Xiaohong Wang** \\
\\
* * *\\
\\
**Professor, Chair / Computer Science  Conway Hall (TE) 201C**\\
\\
- P410-677-5380\\
- E@](/faculty-and-staff/xswang)

[![Soeun You](https://www.salisbury.edu//_images/directory/sxyou.jpg)\\
\\
**Soeun You** \\
\\
* * *\\
\\
**Lecturer / Computer Science  Conway Hall (TE) 201F**\\
\\
- P410-677-0177\\
- E@](/faculty-and-staff/sxyou)

- [Administrative Staff](javascript:void(0);)
[![Lauren Ruark](https://www.salisbury.edu//_images/directory/su-seal.jpg)\\
\\
**Lauren Ruark** \\
\\
* * *\\
\\
**Administrative Assistant Ii / Computer Science**\\
\\
- P410-543-6140\\
- E@](/faculty-and-staff/laruark)

## [Computer Science](/academic-offices/science-and-technology/computer-science/)

- [What's New](/academic-offices/science-and-technology/computer-science/news/)
- Faculty & Staff
- [Courses](courses.aspx)
- [Schedules & Syllabi](schedules-and-syllabi.aspx)
- [Undergraduate Research](undergraduate-research.aspx)
- [Internships](/academic-offices/science-and-technology/computer-science/internships/)
- [Academic Help Center](academic-assistance.aspx)
- [Student Clubs](student-clubs.aspx)
- [Awards & Honors](awards-honors.aspx)
- [Student Testimonials](student-testimonials.aspx)
- [Eastern Shore High School Computer Programming Competition](eshs-coding-competition.aspx)
- [Scholarship Opportunities](scholarship.aspx)

## Contact Information

Lauren RuarkAdministrative Assistant IIConway Hall (TE) 201

- P [410-543-6140](tel:410-543-6140)
- E [laruark@salisbury.edu](mailto:laruark@salisbury.edu)

[Visit Salisbury](https://www.salisbury.edu/admissions/visit-campus.aspx)

[Apply Now](https://www.salisbury.edu/admissions/apply-now.aspx)

[Request Info](/admissions/request-info/)

1101 Camden AvenueSalisbury, MD 21801

[410-543-6000](tel:4105436000)

- [News and Events](/news/)
- [Academic Calendar](/administration/academic-affairs/registrar/)
- [Libraries](/libraries/)

- [Give to SU](https://giving.salisbury.edu/pages/salisbury-home-page)
- [Careers](/administration/administration-and-finance-offices/human-resources/careers/)
- [Bookstore](https://www.salisbury.edu/administration/student-affairs/book-store.aspx)

- [Like Salisbury University on Facebook](https://www.facebook.com/SalisburyU/ "Like Salisbury University on Facebook")
- [Follow Us on Twitter](https://twitter.com/salisburyu "Follow Us on Twitter")
- [Connect With Salisbury University on LinkedIn](https://www.linkedin.com/school/salisbury-university/ "Connect With Salisbury University on LinkedIn")
- [Visit Salisbury University on Flickr](https://www.flickr.com/photos/supublications/ "Visit Salisbury University on Flickr")
- [Visit Salisbury University's YouTube Channel](https://www.youtube.com/user/salisburyuniversity "Visit Salisbury University's YouTube Channel")
- [Follow Salisbury University on instagram](https://www.instagram.com/salisburyuniversity/ "Follow Salisbury University on instagram")

- [Privacy](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/privacy.aspx)
- [Web Accessibility](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/web-accessibility.aspx)
- [Terms of Use](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/terms-of-use.aspx)
- [Copyright](https://www.salisbury.edu/administration/general-counsel/university-policies-legal/copyright.aspx)
- [Student Consumer Information](https://www.salisbury.edu/administration/general-counsel/student-consumer-information.aspx)

Â© 2024 Salisbury University.


        ```

        src/other/some_stat_visualizations.py:
        ```
import sys
import pandas as pd
import numpy as np
import networkx as nx
from PyQt5 import QtWidgets
import pyqtgraph as pg
from pyqtgraph.Qt import QtCore, QtGui
from tqdm import tqdm  # Import tqdm for progress bar

# Load data
print("Loading data...")
articles_df = pd.read_json("test_processed_article_stats_data.json")
print("Data loaded.")


class CollaborationNetworkApp(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Collaboration Network")
        self.setGeometry(100, 100, 800, 600)

        # Create a graph
        self.graph = nx.Graph()
        self.edge_info = {}

        print("Building graph...")
        # Build the graph and collect edge information
        for article, details in articles_df["math"]["article_citation_map"].items():
            authors = details["faculty_members"]
            for i in range(len(authors)):
                for j in range(i + 1, len(authors)):
                    edge = tuple(sorted((authors[i], authors[j])))
                    self.graph.add_edge(*edge)
                    if edge not in self.edge_info:
                        self.edge_info[edge] = []
                    self.edge_info[edge].append(article)
        print(
            "Graph built with",
            len(self.graph.nodes),
            "nodes and",
            len(self.graph.edges),
            "edges.",
        )

        # Create a plot widget
        self.plot_widget = pg.PlotWidget()
        self.setCentralWidget(self.plot_widget)

        # Draw the network
        self.draw_network()

        # Connect mouse press event
        self.plot_widget.scene().sigMouseClicked.connect(self.on_mouse_click)

    def draw_network(self):
        print("Calculating layout...")
        pos = nx.spring_layout(self.graph, k=0.15, iterations=20)
        self.node_positions = pos
        print("Layout calculated.")

        # Draw edges with progress bar
        print("Drawing edges...")
        total_edges = len(self.graph.edges())
        with tqdm(total=total_edges, desc="Drawing edges", unit="edge") as pbar:
            for idx, edge in enumerate(self.graph.edges(), start=1):
                tqdm.write(f"Drawing edge between {edge[0]} and {edge[1]}")
                x0, y0 = pos[edge[0]]
                tqdm.write(f"x0: {x0}, y0: {y0}")
                x1, y1 = pos[edge[1]]
                tqdm.write(f"x1: {x1}, y1: {y1}")
                line = pg.PlotDataItem(
                    [x0, x1], [y0, y1], pen=pg.mkPen("gray", width=1)
                )
                tqdm.write(f"Line: {line}")
                tqdm.write(f"Adding line to plot widget")
                self.plot_widget.addItem(line)
                tqdm.write(f"Line added to plot widget")
                remaining_items = total_edges - idx
                completion_percentage = (idx / total_edges) * 100
                tqdm.write(
                    f"Remaining items: {remaining_items}, Completion: {completion_percentage:.2f}%"
                )
                tqdm.write(f"now doing the next edge")
                pbar.update(1)  # Update the progress bar
        print("Edges drawn.")

        # Draw nodes
        print("Drawing nodes...")
        self.node_items = {}
        for node, (x, y) in pos.items():
            node_item = pg.ScatterPlotItem(
                [x], [y], size=10, brush=pg.mkBrush(255, 0, 0, 120)
            )
            self.plot_widget.addItem(node_item)
            self.node_items[node] = node_item
        print("Nodes drawn.")

    def on_mouse_click(self, event):
        pos = event.scenePos()
        mouse_point = self.plot_widget.plotItem.vb.mapSceneToView(pos)
        x, y = mouse_point.x(), mouse_point.y()

        # Check if a node is clicked
        for node, node_item in self.node_items.items():
            node_pos = self.node_positions[node]
            if (
                np.linalg.norm(np.array([x, y]) - np.array(node_pos)) < 0.1
            ):  # Simple proximity check
                self.show_node_info(node)
                return

        # Check if an edge is clicked
        for edge in self.graph.edges():
            x0, y0 = self.node_positions[edge[0]]
            x1, y1 = self.node_positions[edge[1]]
            if self.is_point_near_line(x, y, x0, y0, x1, y1):
                self.show_edge_info(edge)
                return

    def is_point_near_line(self, px, py, x0, y0, x1, y1, tol=0.05):
        # Check if point (px, py) is near the line segment (x0, y0) to (x1, y1)
        line_vec = np.array([x1 - x0, y1 - y0])
        point_vec = np.array([px - x0, py - y0])
        line_len = np.linalg.norm(line_vec)
        if line_len < 1e-5:
            return False
        line_unitvec = line_vec / line_len
        proj_length = np.dot(point_vec, line_unitvec)
        if proj_length < 0 or proj_length > line_len:
            return False
        nearest_point = np.array([x0, y0]) + proj_length * line_unitvec
        dist = np.linalg.norm(np.array([px, py]) - nearest_point)
        return dist < tol

    def show_node_info(self, node):
        QtWidgets.QMessageBox.information(
            self,
            "Node Info",
            f"Node: {node}\nCollaborations: {self.graph.degree[node]}",
        )

    def show_edge_info(self, edge):
        articles = ", ".join(self.edge_info[edge])
        QtWidgets.QMessageBox.information(
            self,
            "Edge Info",
            f"Collaboration between {edge[0]} and {edge[1]}\nArticles: {articles}",
        )


def main():
    app = QtWidgets.QApplication(sys.argv)
    main_window = CollaborationNetworkApp()
    main_window.show()
    sys.exit(app.exec_())


if __name__ == "__main__":
    main()

        ```

            src/other/testing/professor_dump_to_pd.ipynb:
            ```
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfessorDumpToPandas:\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = self.read_csv()\n",
    "\n",
    "    def read_csv(self):\n",
    "        return pd.read_csv(self.csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdump = ProfessorDumpToPandas(\n",
    "    \"/mnt/linuxlab/home/spresley1/Desktop/425Reset/Rommel-Center-Research/PythonCode/Utilities/RuntimeFiles/professor_department_dump.csv\"\n",
    ")\n",
    "pdump_df = pdump.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_professor_names_and_dept: dict[str, str] = {}\n",
    "\n",
    "for row in pdump_df.itertuples(index=False):\n",
    "    key = f\"{row.lastName}, {row.firstName}\"\n",
    "    if key not in official_professor_names_and_dept:\n",
    "        official_professor_names_and_dept[key] = [row.departmentCode]\n",
    "    else:\n",
    "        official_professor_names_and_dept[key].append(row.departmentCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rayne, Karen E.', 'Vitanovec, David G', 'Rose, Stephanie Christine', 'Willoughby, Robert W', 'Starkey, A J', 'Moriarty, Loren', 'Brown III, Edward', 'Ramanathan, Gurupriya', 'Jodlbauer, Eric A', 'Habermeyer, Ryan M', 'Wilson, Vickie Julene', 'Caviglia-Harris, Jill L', 'Carter, Michael S', 'Harvey, Stephen Philip', 'Winterson, April C', 'Lynerd, Dana Leigh', 'Cardoni, Mary Patricia', 'Freda, Kaynabess', 'Andes, Jon M', 'Hults, Diana Katherine', 'Wilson, Darrell K', 'Yao, Hong', 'Auerbach, Anna Jo J', 'Jeon, Kwonchan', 'Jensen, Scott R', 'Keter, Dawn A.', 'Pubill, Corinne', 'Shannon, Kathleen M.', 'Martin, Linda Sines', 'Goldhagen, Carl E', 'Brown Jr, Marvin O.', 'Hunter, Kimberly L', 'Propper, Stacey Elizabeth', 'Phillips, Sara Lynn', 'Rogers, Janet R', 'Maloof, Joan E.', 'Van Metre, Edward H.', 'Wright, Jennifer Owens', 'Imbrenda, Jon-Philip', 'Martino, Andrew P', 'Ennis, Bonnie Hall', 'Hutchinson, Victoria V.', 'Ward, Thomas Woodrow Wilson', 'McCormick, Kimberly M', 'Williams, Ellis Eugene', 'Higgins, Dawn M', 'Vicens Saiz, Belen', 'Kendzejeski, Rachel L', 'Rojas, Anthony J', 'Tigani, Michael A', 'King, Fred T', 'Messick, Angela M', 'Morrison, Lucy J', 'Aguilar, Alexis L.', 'Stoddard, Ann B', 'Clark, Colleen M', 'White, Arlene F.', 'Green, Daniel C', 'Walter, Mark I.', 'Reckley-Murphy, Joy A', 'Moghaddam, Masoud', 'Bolek, Sarah Alma', 'Wright, John W.', 'Jewell, Jennifer R', 'Semanoff, Jason Henry', 'Prouse, Angela B', 'Clark, Shanetia P', 'Moszer, Letha Faye  Meyer', 'Howard, Lisa Noelle', 'Davis, Karen Sue', 'Diamonte-Mock, Christina M', 'Guy, Sarah Ann', 'Fabbri, Kimberly A', 'Gutoskey, David J.', 'Miller, Jason Lawrence', 'Maykrantz, Sherry Azadi', 'Shaffer, Mark A', 'Verzi, Diana W', 'Timko-Jodlbauer, Sarah A', 'Ingolia, Joseph', 'Todd, Arthur W.', 'Sala, Araina Day', 'Gibson IV, Samuel Thomas', 'Bradley, Christina J', 'Zimmer, Susan Elizabeth', 'Eun, Jihyun', 'Beebe, Alfred S', 'Bukolt, Deborah A', 'Binkley, Anne V', 'Sims, Thomas', 'Thorpe, Laura T.', 'Smith, Michael D', 'Piatselchyts, Irina A', 'Usilton, Leigh Elizabeth', 'Mullins, Darrell G.', 'Harris, Larry Albert', 'An, Jin', 'Thomas, Rosemary M.', 'Welsh, Gail S.', 'Stanzione, Eileen H', 'Bailey, Stephen M.', 'Kundell, Frederick A.', 'Gilbert, Katie M', 'Brewington, Denise Condon', 'Wu, Yun', 'Keene, Brooke Lee', 'Friese, Martin', 'Martin, Donna M', 'Cook, Emilie A.', 'Buffone, Joan C.', 'Nestor Jr, James P.', 'Dickerson, Crystal Cecelia', 'Richerson, Nancy B', 'Hines, Megan Reese', 'Bergin, Molly M', 'Nina-Matos, Belgica J.', 'Spicer, Theresa Ann', 'Van de Wiele, Aurelie', 'Becker, Robert W', 'Molenda, Sally M', 'Kjeldsen, Joan Bromhall', 'Ripperger, Shelby Jo', 'Wilhelm, Kelli Dawn', 'Eckard, Ashley M', 'Barzilai, Harel', 'Davis, Wesley P', 'Sadowski, Amy Elizabeth', 'Ramseyer, Craig A', 'TBD, nan', 'Dombrowski, Robert F.', 'Buchanan, Rachel L.', 'Kirtsos, Michael Patrick', 'Mathews, Deborah Ann', 'Lamboni, Patrick M.', 'Michel, Lucia Claire', 'Walton-Wade, Kristen P', 'White, Barbara J', 'Wilson, Laura Ann', 'Lewis, Sara Wolff', 'Williams, Evangeline B', 'Dean, Jeffrey James', 'Donaway, Tammy Lynn', 'Miao, Chao', 'Crissman, Erin S', 'England, Richard K.', 'Gilheany, Eileen P', 'Israel, Nathan Joseph', 'Barry Jr, James P', 'Gudelunas, William A', 'Fitzgerald, Evan', 'Wood, Warren C.', 'Angstadt, Frances V', 'Gauger, Michael A', 'Lamey, Thomas W', 'Kanarr, David M.', 'Gunes, Itir', 'Williams, Alisha M', 'Gurlly, Aaron W', 'Wise, Jason Laird', 'Grear, Teresa', 'Losonczy-Marshall, Marta E.', 'Park, Minseok', 'Bryan, Cathy Jo', 'Burkett, Katie Lynn', 'Campbell, Carlene Lynn', 'Kellersohn, Keith B.', 'Disbennett Jr, Robert W.', 'Knight, Margaret J.', 'Shivers, George', 'Smith, Robert', 'Boster, Charles R', 'Swift, Leslie White', 'Kelly, Megan S', 'Counts, Clement L', 'Bristow, Devon Rose', 'Ragan, Elizabeth A.', 'Elliott, Kathy June', 'Schermerhorn, Jennifer L', 'Drabick, Justin M', 'Stephenson, Jill A.', 'Sen, Argha', 'Kyriacopoulos, Konstantine A', 'Matthews, Brittany Drewer', 'Tyndall, Chelsea Nicole', 'Ali Moustafa, Amal K.', 'Madison, Emily Baldwin', 'West, Jessie S', 'Hammerer, Kristen Marie', 'Bradford, Randy L.', 'Wright, Kathleen M.', 'Ingoglia, Anthony V', 'Von Den Bosch, Nicole Martine', 'Dewald, Alison H', 'Kasven, Stefanie R', 'Forsythe, Kelly Durham', 'Hatch Pokhrel, Lauren Lee', 'Kutchen, John E', 'Reb, Maureen Courtney', 'Nelson, Paula F.', 'Smullen, Vanessa S', 'Oneal-Self, Amy E.', 'Moazzam, Mohammad', 'Malone, Kevin C.', 'Terrill, Brandy J.', 'Baskerville, Andrew Thomas', 'Wood, Carol A.', 'Schiff, Marcelle S', 'Brower, Keith H.', 'Miller, Connie L', 'Fletcher, Michelle Nicole', 'Bardzell, Michael J.', 'Clark-Andrejkovics, Amanda J', 'Lutz, Marie C', 'Malament, Cynthia B', 'Robinson, Leonard C.', 'Walker, Elsie M', 'Underwood, Jennifer D', 'Bolton, Joshua P', 'English, Christopher W', 'Hughes, Leslie', 'Sessoms, Chalarra A', 'Nam, Taehyun', 'Derrick, Patricia L.', 'Dwyer, Linda E', 'Roser, James Stephen', 'Wright, James Lonnie', 'Johnson, Jennifer Hoi Yin', 'Shuhy, David E.', 'Carlson, Kristin L.', 'Laws, Alexis Rachel', 'Poddar, Amit', 'Castelow, Hannah Elizabeth', 'Parsons, Catrice L', 'Lanious, Patricia E', 'Hoffman, Adam H.', 'Song, Hongzhuan', 'Foutz, Brittany Lee', 'Stratton, Casey M', 'Sturgis, Jaclyn K', 'Agarwal, Vinita', 'Shetty, Shekar T.', 'Smith, Betty Lou', 'Johnson, Aaron C', 'Malkus, Milton M', 'Han, Eun-Jeong', 'West, Laura L', 'Urban, Molly B', 'Smith, Rachel N', 'DiBartolo, Gerard R.', 'Spickler, Donald E.', 'Adams, Roberta A.', 'Acton, Robin Lilley', 'Parker, Janelle Ingrid', 'Dyer, Nikki Allen', 'Bugdal, Melissa Elizabeth', 'Hamill, Thomas Joseph', 'Byrne, Katherine Marie', 'Sporer, Ryan A', 'Dobos, Lilia Caroline', 'Nanez, Martin Audelo', \"Daniels, Maria La'Trice\", 'Wesolowski, Sarah C', 'Klug, Robert', 'Smith, Kenneth J.', 'McNabb, Morgan L', 'Nein, Matthew Ammon', 'Hurley, Deanne Lauren', 'Lewis, Lindy Ladd', 'Holdai, Veera', 'Kotlowski, Dean J.', 'Marasco, Laura L.', 'Hartman, Nathaniel Thomas', 'Lovelace, Kaitlin Marie', 'Neville, Barry P.', 'Perry, Melissa Kaye', 'Monico, Evgeniya Y', 'Laird, Krispen Louise', 'Acocella, Cecilia M.', 'Monroe, Lauren Nicole', 'Waldron, Mia King', 'Fitzgerald, Kevin M', 'Kishen, Aarti', 'Upshaw, Mary-Tyler Embrey', 'Junkin, Vanessa Dawn', 'Gray, Latrice Kiane', 'Sum, Vichet', 'Smith, Diane June', 'Carayon, Celine', 'McFarland, Marie Anne', 'Ferger-Hill, Robin Lynn', 'Jones, Catherine Jane', 'Ettinger, Thomas J', 'Osman, Suzanne L.', 'Southerland III, Wallace', 'Arvi, Leonard', 'Sacco, Francesco G', 'Oby Jr, Stephen J', 'Oleszewski, Ashley M', 'Short, Brenda Figgs', 'Weber, David P', 'Schoyen, Jeffrey G.', 'Leonel Jr, Ronei', 'Simmons, Teresa Fields', 'Yonker, Shawn R', 'Weldon, William W', 'Morris, Paula T.', 'Dockins-Mills, Lawanda', 'DoDoo, Soraya Laaouad', 'Messner, Jacqueline Rose', 'Barse, Ann M.', 'Soule, Jessica L', 'Ference, Gregory C.', 'McNeil, Ashley Rae', 'Lewis, Ann C.', 'Hirko, Daniel Martin', 'Mead, Terry M', 'Stitcher, Thomas P.', 'Norman, Matthew James', 'Rouse, Kelley J', 'Burt, Jane B', 'Malone, Ruth Frances', 'Laird, Johanna W.', 'Schreibman, Janice Suzette', 'Colonna, C. Ann', 'Pepper, Jennifer Lynne', 'Endicott, Seth Andrew', 'Becker, Larence', 'McCarty, Susan M', 'Yarmo, Leslie T.', 'Tribull, Douglas F', 'Truitt, Debra L.', 'Gill, Kathryn Estelle', 'Gilkey, Theodore R.', 'Oh-Howard, Jeongju G', 'Daugherty-Ball, Debra Blair', 'Bowen, Brianna Ebony', 'Grudis, Suzanne Parker', 'Flores, Brian M', 'Johnson, Karin E.', 'Morrison, Jody D.', 'Schlehofer Copper, Michele M', 'Schloemer, Deanna Lea', 'Munday, Nicole M.', 'Jesien, Roman V.', 'Craddock, Christine Lee', 'Vassallo-Oby, Christine Frances', 'Munasinghe, Kumudini A', 'Fielder, Erica Zoe', 'Harvey, Kellie Anne', 'Elder-Correa, Alaina Jane', 'Rodriguez, Josefina Esther', 'Jacobs, Richard J', 'Juncosa Jr, Jose I', 'Franzak, Judith K', 'Adams, Stephen B.', 'Jones, Kasey Chandler', 'Chen, Xuan', 'Ginnavan, Amy Pauline', 'Greenwood, Tina M.', 'Shockley, Beth Ellen', 'Shakur, Asif M.', 'Kulavuz-Onal, Derya', 'Hohman, Tyler James', 'Hahn, Kristen Deanne', 'Padgett, Stephen M', 'Hannon, Claudia A.', 'Anderson, Louise L', 'Kingan, Michael J', 'Mason, Elisabeth Raquel', 'Kim, Jinchul', 'Warfield, Bradley Scott', 'Richards, Patricia O.', 'Dalrymple, Deborah K', 'Fadl Alla, Tegwa', 'Conrath, Ryan C', 'Delauter, Adam Karl', 'Samis, Gail Marie Johnston', 'Leibu, Beth Louise', 'Truong, Hoai-An', 'Schultes, Carla N', 'Burneston, Kristin Lynn', 'Muller, Susan M.', 'Hatley, James D.', 'Stiegler, Brian N.', 'Hall, Shane D', 'Decker, Wayne H.', 'Quillin, Kimberly Johnson', 'Lombardo, Cynthia M.', 'Burns, David Paul', 'Nash, Naomi Ruth', 'Harlin, Margaret Mary', 'Manns, Theresa A', 'May Jr, Everette L', 'Satterfield, Danny E', 'Melczarek, Arnold N', 'Campbell, Robert S.', 'Li, Ning', 'Tolley, Leslie Ann', 'Goyens, Tom', 'Duval, Marion S', 'Callaghan, Renee Garcia', 'Scott, Samantha L', 'Patterson, Meredith M.', 'Johnson, Lindsay Nicole', 'Waggoner, Kyle Michael', 'Hetzler, Steven M.', 'Porter-Long, Caroline T', 'Geraldes, Heather April', 'Ginta Martin, Alexandra Florina', 'Skeeter, Brent R.', 'Donoway, Jennifer Gordy', 'Prievo, Holly M', 'Pretl, Michael A', 'Barahouei Pasandi, Faezeh', 'Verzi, Diane W', 'Jones, Sarah Alexandra', 'Rieck, David F.', 'Donoway, Keith Allwin', 'Scott, Michael S.', 'Kottemann, Jeffrey E.', 'Gootee-Ash, Amy L', 'Kubina, Mackensey K', 'Woodward, Lisa Ann', 'Hwang, Jee Hyun', 'Ackerson, Kenneth Wayne', 'Kelly, Clare Kathleen', 'Jones, Amy Markley', 'Jones, Susan Margaret', 'Phillips, Scott P', 'Johnson, Malkia Lashan Brown', 'LeBaron, David N.', 'Nan, Wenxiu', 'Johnson, Robert A.', 'Gabbard, Theresa Tovornik', 'Patel, Shruti A', 'Williams-Crawford, Frances H', 'Palakanis, Kerry C', 'Felix, Alexis Faye', 'Darnell, Lynnette', 'Baker, Robert A', 'Persad-Clem, Reema A', 'Burgasser, Connor Langan', 'Planes, Cyril', 'Panjwani, Narendra K', 'Rommel, Luke A', 'Young, Jacey Megan', 'Calafiura, Marie B', 'Cho, Hajin', 'Beise, Catherine M.', 'Cabrera, Katelynn C', 'Leonard, Steven Edward', 'Carlander, Jay R.', 'Yingling, Craig Douglas', 'Weer, Christy H.', 'Penuel, John Sherwood', 'Wiersberg, Gerrie Lynn', 'Sage, Marisa I', 'Zhang, Yinghao', 'Chambers, Silvana S', 'Gering Jr, George W', 'Suk, Andrea L', 'Weaver, Ryan V', 'Trantin, Kathryn Marie', 'Austin, Homer W.', 'Pollock, Barbara J.', 'Trenary, Shelby Elizabeth', 'Kew, Claire F.', 'Pappas Jr, Allan', 'Giska, Alison Lin', 'Simulis, Anna M', 'Thomas, Melissa D.', 'Habashy, Noel B', 'King, Carolyne Marie', 'Dreany-Pyles, Laura C.', 'de Socio, Mark J', 'Walsh, Catherine M.', 'Erdie, Paula Martin', 'Winter, Dorothea M', 'Mosman, Nicole R', 'Chillingworth-Shaffer, Anne', 'Long, Robert P.', 'Bergner, Jennifer Anne', 'Scahill, Andrew D', 'Kim, Koomi J', 'Losoya, Joshua David', 'Harrison, Nicole Elizabeth', 'Carter, Carolyn M', 'Kimlel, Lauren M', 'Morris, Morgan D', 'Dean, Brian J.', 'Brotman, Gary B', 'Miriel, Victor A.', 'Biscoe, Melanie Ann', 'Younker, Jennifer M', 'Bruce, Sean M', 'Phillips, David S', 'Garcia Luque, Silvia', 'Lipka, Pamela', 'Garcia, Mark J', 'Johnson, Jennifer Mills', 'Workman, Rachael Marie', 'Snavely, Susan L', 'Kraemer, Laura Davis', 'Frana, Mark F', 'Cashman, Kristin Lee', 'Howard, Stephanie Erin', 'Dover, Howard F.', 'Villalobos Fiatt, Laura', 'Ananou, Teko S', 'Cohey, Maribeth Lorraine', 'Lebel, Phillip G.', 'Cowall, Cynthia C.', 'Nichols, Edward Yancey', 'Layton, C. Rodney', 'McKay, Meghan E.', 'Zak, William F', 'Cox, Jennifer B', 'Chaudhry, Vishal', 'Harrington, Gary M.', 'Krisulevicz, Ronald J', 'Stribling, Judith M.', 'Mirarchi, Sarah L.', 'Gilmore, Jordan A', 'Groth, Randall E.', 'Melstrom, Tina M', 'Murray, Laura', 'Krempel, Lee Michael', 'Belloso, Leslie M', 'Erickson, Patti T', 'Sodhi, Mininder K', 'Ralph, Gary D', 'Jaspers, Lea H', 'Gittelman, Julie E', 'Jansson, Sarah', 'Collins, Khalilah V.', 'Peng, Yuqi', 'Camillo, Michael William', 'Tremaine, Marianne G', 'Cammarano, Cristina', 'Keenan, Anastacia J', 'Dunn, Timothy J.', 'Elburn, Sara J.', 'Chappell, Charisse D.', 'Gibbons, Jenna', 'Fleischer, Priscilla R', 'Turner, Anne Cheryl', 'Howie, John K', 'Colon, Jonathan', 'Garfield, Lisa E', 'Madden, Joshua E', 'Wills, Jennifer H.', 'Hall, Rhonda Michele', 'Ubi, Jaan', 'Lebbie, Sahr Momaqua', 'Cumming, Danielle M.', 'Mero, Kendra Dawn', 'Berkman, James J.', 'Dugan, Marissa K', 'Corfield, Jeremy', 'Paskova-Anderson, Lyubov A', 'Keifer, David Z', 'Desper, Jason M', 'Thompson, Andrew Wade', 'McInerney, Jessica', 'Anderson, Joseph T', 'Hickman, John N.', 'Yesiltas, Chandra L', 'Eutsler, Madysen Marie', 'Berezowski, Brooke Ann', 'Farlow, Michael William', 'Brooks, LaToya', 'Pandey, Anjali', 'Ramey, Cathy Marie Watson', 'Murasugi, Sachiho C', 'Farr, Ashley Suzanne', 'Stull, Eric Stephen', 'Bivens, Gaylena Addie', 'Goldstein, Arthur G', 'Sabzi, Hadis', 'Zellers, Dana Bethsabe', 'Clarke, James C', 'Genvert, Margaret Fisk', 'Thomas, Christopher Lamont', 'Proudfoot McGuire, Peggy J.', 'Benato, Chiara Elena', 'Noble, Megan Kolby', 'Horton, Thomas W', 'Taylor, Susannah W', 'Chen, Chin-Hsiu', 'Goode, Jeffrey H.', 'Basko, Aaron M', 'Morrison-Parker, Claudia', 'Bernhard, Stephanie R', 'Silaphone, Keota S.', 'Scovell, Paul E.', 'Bowers, Natalie P', 'Beteck, Rose N', 'Dudley-Eshbach, Janet E.', 'Anderson, Thomas E', 'Bernier, Lindsey R', 'Parsons, Mary T.', 'Rangel, Adriana G', 'McElroy, Honor Braden', 'Longer, David', 'Thompson, Melissa M', 'Overholt, Charles M', 'Kowalski, Luanne L', 'Gibson, Abby S', 'Hollis, Matthew J', 'Fafoutis, Dean J.', 'Fleekop, Matthew Joseph', \"O'Loughlin, Michael G\", 'Young, Brian K', 'Park, Sang-Eon', 'Guckes, Kirsten Raquel', 'Bouchelle, Jamie L', 'Tabor, Nancy R', 'Krebs, Stacey Elizabeth', 'Nagel, William J.', 'Hill, Marjorie R', 'Buss, James J', 'Zockoll Jr, Brian Mark', 'Franco, Megan G.', 'Holland, Mark A.', 'Ivey, Raymond', 'Hoffman Jr, Robin Jay', 'Givans, Sylvia A', 'Goodman, Gary Cornelius', 'Wilkin, Claire Gabrielle', 'Jarosinski, Judith M.', 'Logan, April C', 'Engelman, David Robert', 'Cashman, Burton Roger', 'Crane, Sherry', 'Emmert, Elizabeth A. B.', 'Goodwin III, William V', 'Mills, Jeanne T', 'Poe, Jennifer M', 'Carstens, James F.', 'Wenke, John P.', 'Trice, Kathryn Carol', 'McCabe, Tiffany Michelle', 'Hua, Jennifer', 'Stewart, Margaret A', 'Rogers, Susan', 'Morris, Lorna N', 'Arban, Kathleen S', 'Vazquez, Gerardo A', 'Carr-Phebus, Alissa A', 'Ebert, John Malcolm', 'Carpenter, Eric J.', 'Bunting, David Orlando', 'Bown, Carolina D.', 'Cyryca, Paul Anton', 'Hill, James K.', 'Kolahdouzipour, Aysan', 'Sokoloski, Joshua E', 'Vennos, Amy Demetra-Geae', 'Twilley, Brian', 'Fineran, MaryLouise M', 'Mohammadioun, Jamshyd', 'Schneider, Gustavo', 'Van Dyke, Debra J', 'Matthews, Meghan Aileen', 'Naleppa, Margaret M', 'Joyner, Lisa R.', 'Jackson, Catherine Ann', 'Parthum, Bryan M.', 'Siers Jr, Ronnie R.', 'Gregory Jr, James E.', 'Campbell, William T.', 'Barrett, George D', 'Charlton, Michael James', 'Gang, KwangWook', 'Bunting, Jaime R', 'Murphy, John P', 'Goldberg, Rachel M', 'Beardsley, Corinne R', 'Robinson, Timothy F.', 'Wilkens, Richard T', 'Miller, Elwood M', 'Crowell, Katherine', 'Willey, Amanda J', 'Cosgrove, Kristina Lindes', 'Miller, Timothy S.', 'Devine, Jane M', 'Vickers, Christina Lea', 'Ball, Marchan Rawlins', 'Wang, Xiaohong', 'LaManca, John J.', 'Gladden, David L.', 'MacDonald, Elisheva D', 'Cerda, Rodrigo Javier', 'Danieli, Raymond F', 'Beamer, Jennifer G', 'Sise, Michael J', 'Packey, Daniel Jack', 'Thomas Jr, Lynn Brinsfield', 'Bienstock, Arnold L', 'Larson, Rebecca S', 'Halowich, Joseph H', 'Wilson, Brittany Nicole', 'Hogg, Suzanne Elizabeth', 'Young, Shawn W', 'Root, David G.', 'Lopez, David F', 'Nutter, Kimberly M', 'Ostrom, Todd Michael', 'Robeck, Edward C.', 'Bowers, Venessa A', 'Whitcomb, Valerie J', 'Selway, Christie C', 'Bleil, Robert R.', \"Batson, Ra' Sheeda D\", 'Elphick, Keith John', 'Rasmussen, Victoria', 'Shea, Michael P', 'Rodriguez Vallejo, Rocio', 'Kunciw, Bohdan G', 'Moore, Christopher Taylor', 'Cranbury, Sarah E', 'Allen, Kimberly Dawn', 'Papke-Shields, Karen E.', 'Bronson, Adam P', 'Duong, Hong K', 'Mitchell, Diane K', 'Halperin, Alexander D', 'Birch, Kevin E.', 'Lowrie, Bridget M', 'Dronenburg, Kristina Marie', 'Byrd, Cynthia A', 'Murphy, Megan A', 'Devine, Michael C.', 'Witkofsky, Alfred A.', \"O'Keefe, Jessica M\", 'Anderton, Jeanne L', 'Boggio, Pamela J.', 'Bergonia, Aimee Suzanne', 'Suprunova, Anastassiya', 'MacSorley, Paige B.', 'Ortlip, Austin Tyler', 'Meekins, Amy S.', 'Carlos, Michael D', 'Wood, Bob G.', 'Poe, Preston D.', 'Erickson, Floyd L.', 'Haddock, Carey E', 'Binz, Steven M', 'Miller, Megan Carey', 'Cawthern, Thomas R', 'Drechsler, Terry Ann', 'Pryor, Michelle RenÃ©e', 'Barja Cuyutupa, Ethel Mylene', 'Adkins, Patricia A', 'Craig, Dylan Keith', 'Cristea, Haley J', 'Lelic, Emin', 'Wolff, William A', 'Pautz, Stefani Nicole', 'Royer, Regina D.', 'Houser, Linda Denise', 'McCarty, Heather R', 'Ludwick, Kurt E.', 'Quan, Jing', 'LaMade, Jennifer Cottle', 'Ritenour, Donna M', 'Willey, Vicki C', 'Luttrell Jr, Robert D.', 'Connery, Mary Cathrene', 'Mahn, Thomas E.', 'Surak, Sarah M', 'Lee, Kyung Min', 'Jimenez, Zulma Azucena', 'Parker, Chasta Louise', 'Ashton, Linsey Marie', 'French, Kara M', 'Chakraborty, Moushumi', 'Schaire, Harrison William', 'Holmes-Wiggins, Kimberly N', 'Houghtaling, Cynthia Ann', 'Slye, Natalie T.', 'Altunc, Arife B', 'Franchi, Giulia', 'Hargrove, Alex R', 'Pereboom, Maarten L.', 'Giudice, Shauneen', 'Jordan, Michell M', 'Kaul-Black, Mary E', 'Weaver, Starlin D.', 'James, Shannon Marie', 'McFadden, Jenny Lynn', 'Serrano Garcia, Jose V', 'Chown, Gregory A', 'Hinderer, Katherine A.', 'Davis-Hayes, Diane L', 'Wheeler, Stacy Lynn', 'Leasure, Thomas R', 'Tardiff, Robert M.', 'Norris, Daniel', 'Patt Jr, Gerald D.', 'Timmons, Katherine Grace', 'Smith, David S', 'Moore, Heidi Lynne', 'Ets, Hillevi K', 'Blackmon, Jonathan C', 'Cooper, Sean G', 'Harris, Timothy Blake', 'Kane, Mary K.', 'Anthony, Rebecca S', 'Kerrigan, Sandra Lehman', 'White, Iven E.', 'Campbell, Matthew J', 'Sayres, Mary L', 'Hesser, Phillip', 'Milner, Kenneth R.', 'Steele, Rachel R', 'Hopson, Natalie W.', 'Harper, Christina Bantz', 'White, Kylie M.', 'Culotta, Jordan Jeffrey', 'Lawler, Ellen M.', 'Lew, Chooi-Theng', 'Horsman, Phoebe W', 'Donovan, Joanne W', 'Bailey, Matthew A.', 'Detwiler, Louise A.', 'Bearman, Karen', 'Moore, James B', 'Cojoca, Ecaterina', 'Owens-King, Allessia P', 'Robinson, Kacie Lynn', 'Leroy, Lucie', 'Kock, Stacia L', \"O'Driscoll, Daniel P.\", 'Bracken, Michele I.', 'Prisco, Breanne Jennifer', 'Bressman, Noah R', 'Anderson, Averill Vernon', 'Whitt, Hunter Charles', 'Tovornik, Mary R', 'Mancuso, Martha Clare', 'Wright, Cadence Anne', 'White, Bryan K', 'Illgen, Katherine Marie', 'Olson, Sherryl K.', 'Webb, Kawana Laya', 'Nutt, Rita J.', 'Woodis, Lena A', 'Soto Ramirez, Nina', 'Manakyan, Herman', 'Pelot, Kimberly Gleason', 'Ferraro, Elaine Michelle', 'McCrobie, James N', 'Rittinger, Eric R', 'Lenox, John F', 'Gold, Jeffrey Derek', 'Micciche, Amanda Erin', 'Smith, Erin M', 'Barber, Robert', 'Yoon, David J', 'Willis, William Anthony Volney', 'Edwards, Victoria Venable', 'Alessandrini, Erica A.', 'Jones, Jerilisha L', 'LaCurts, Carvel Lee', 'Namwamba, Fulbert L', 'Morrow, Julie A', 'Shaup, Karen L', 'Tirab, Mohamed', 'Thompson, G Ray', 'Anderson, Laura E.', 'Heim, Sara S', 'Viggato, Vincent T', 'Charlton, Heather Giles', 'Randall, Kelli V.', 'Simmons, Haven P.', 'Bonsteel, Adriana C', 'Herrera Angel, Rafael', 'Briddell, April Shanell', 'Silva, Anne Marie', 'You, Soeun', 'Meenan, Kevin Patrick', 'Selzer, Christine M', 'Kiser, John W.', 'Bowles, Daniel A', 'Wellman, Brittany Taylor', 'Zimmerman, Katrina Leesa', 'Herwig, Stephen M.', 'Larrea, Edgar Luis', 'Pike, Judith E.', 'Werner, Timothy J', 'Norris, Gary J', 'Banks, Troy V.', 'Burton, James A', 'Berke, Andrew M', 'Truitt, Jami Lynn', 'Spillson, Christine A', 'Armstrong, Grady', 'Guerriero, Alexandra', 'Maier, Karl J', 'Kalb, John D.', 'Woodis, Adam L', 'Egan, Christine N.', 'Hellsten, Catherine Y', 'Perret, Arnaud P', 'Hall, Michael F', 'Stoner, Alexander M', 'Davis, Jeni R', 'Pennerman, Althea J.', 'Brown, Anita R.', 'Glanz, Julia Anne', 'Parker, David L.', 'Battistoni, Susan Boyer', 'Bloodworth, Gina', 'Sinclair, Leslie A', 'Badros, Karen Kathryn', 'Olson, Melissa Dawn', 'Jin, Xiwen', 'Kirsch, Robert E', 'Chism, Alison L.', 'Pica, Andrew J.', 'Diriker, Veronique', 'Feusahrens, Lauren Ross', 'Outten, Clara Louise', 'Cipolla, Charles E.', 'Goodberry, Benjamin Nathaniel', 'Marquette, Lisa M', 'Walter, Jessica Marie', 'Cashman, Gregory E.', 'David, Satjajit Samuel', 'Tignor, William Charles', 'Illig, Diane S.', 'Suber, Jordan C', 'Jones, Katherine Louise', 'Canfora, Susan A', 'Ortega, Leticia E.', 'Harr, Christopher T', 'Hellane, Sally A', 'Bunting, Jocelyn C', 'Megargee, Mary Kathryn', 'Cha, Hoon S.', 'Ewers, Charles J', 'Jester, Wanda J', 'Thompson, Klaudia M.', 'Forte, Natalie T', 'Hudson, Donna', 'Ford, Rhonda Lee', 'Record, Raffaela N', 'Horsey, Allison Burdo', 'Good-Malloy, Nicholas', 'Moore, Stephanie C', 'Venso, Elichia A.', 'Silverstrim, Karen F.', 'Martin, Heather Lynn', 'Heimann, Jessica E', 'Pfeiffer, Thomas P', 'Perkins, Wanda S.', 'McGorry, Bernard F.', 'Brown, Paula Danielle', 'Schnitzer, Dorothy Claire', 'Tomanek, Veronica M', 'Briand, Christopher H.', 'Otto, Catherine N', 'Melstrom, Richard T', 'Curtin, Jason Ernest', 'Weisgerber, Amanda Leigh', 'King, Patricia K', 'Sutton, Sherry R', 'Wang, Shuangquan', 'Tabor, Jerry N.', 'Gunther, Mary Roman', 'Kendall, Frances L.', 'Munemo, Jonathan', 'Walstrum, Amy Lynn', 'Ringle, Tony W', 'Clement, Grace A.', 'Hall, Renee Richardson', 'Curtis, William J', 'Olszewski, Pamela J.', 'Tate, Stacie L', 'Cassels, Angela Pantinella', 'Folger, William M.', 'Willis, Aaron C', 'Endicott Jr, Russell C.', 'Rice, Philip Thomas', 'Ozoke, Vitus A.', 'Cashio, Victoria K', 'Oldson, James D.', 'Mears, Jennette Altvater', 'McCartney, Jason S.', 'Fagan, Kathleen H', 'Maners, Jamie L', 'Furman, Seth M', 'Parnell, Janique L', 'Fritz, Aaron Matthew', 'Gorrow, Teena R.', 'Cottingham, Carl L', 'Thompson, Gregory', 'Austin, Jathan W', 'Moriarty, Thomas A.', 'Alves, Helena Lucas', 'Carter, Edward Davis', 'Shufelt, Catherine Armetta', 'Jorden, Brenda J', 'Wiencek, Barbara J', 'Solano, Gina L', 'Lipka, Matthew J', 'Thaqi, Vendim', 'Dunn, Alison Michele', 'McCarty, Michael B', 'Gibb, Paula Ruark', 'Saltzberg, Matthew A', 'Cathcart, Donald C', 'Pope, Michelle Renee', 'Nolan, Margo Joyce', \"Sha'ar, Ahmad A\", 'Brown, Sedonna M', 'Hines, Daniel Joseph', 'Brower, Lori Ann', 'Taylor, Ryan C.', 'Bullock, Emily Klein', 'Clark, Jessica K', 'Habeger, Amy Diane', 'Easterling, Debbie S.', 'Mapp, Charisse M.', 'Buzard, Lindsey Rachel', 'Hart, Jennifer A', 'Cooper, Sue A', 'Rogers, Travis Paul', 'Webb, Cynthia Marie', 'Fanjoy, Keith R', 'Day, Jacob Randall', 'Schneider, Lisa M', 'Nobiling, Brandye D.', 'Johnson, Kenneth Albert', 'Schneider, Traci Bozman', 'Rogers, William Brooke', 'Andes, Laurie A.', 'Ransom, Tami S', 'McEntee, Shawn', 'Chen, Yi-Ju', 'Settle, Robert B.', 'Garlock, Marc Richard', 'Berry, Robert Alan', 'Stoner, Melissa A', 'Wilson, Cassandra Elaine', 'DeWitt, Douglas M.', 'Petrolino, Ashley V', 'Bennett, Tom Richard', 'Teller, Kyle G', 'Bradford, Randy Lee', 'Deysher, Ryan Michael', 'Kim, Sook Hyun', 'Mahmood, Sehba', 'Mathias, Keith Wayne', 'Bishop, Nicole Roxanne', 'Shives, Amy Lynn', 'Harris, Daniel W.', 'Ritchey, Kenneth James', 'Webber, Michael Allan', 'Van Aken, Rebecca Elizabeth', 'Williamson, Thea C', 'Campbell-Hanson, Trisha N', 'Finch, Maida A', 'Hipszer, Amanda M', 'Campbell, Rita G', 'Royer, Jeffrey A', 'Maddux, Lindsay Elizabeth', 'Brown, Voncelia S.', 'Welch-Hamill, Amanda Sue', 'Kiefer, Stephanie Ellen', 'Brown, Niya R.', 'Wolinski, John T.', 'Reid, Tina P', 'Mukhopadhyay, Aparajita', \"Vila', Joaquin S.\", 'Arenella, Laura Anne', 'Carson, Christopher Dean', 'Silliman, Jennifer L', 'East, Meghan S', 'Sroka, Matthew', 'Karimzad Sharifi, Farzad', 'Heimdal, Justina N', 'Jung, Kyoung Rae', 'Venosa, Joseph L', 'Price, Eric M', 'Williams, Tracey L.', 'Reynolds, Leah M', 'Felizzi, Marc V', 'Robinson, Yolanda M', 'Park, Susan E', 'Gilbert, Kristin N', 'Case, Sarah H.', 'Williams, Natasha M', 'Newton, Darrell M.', 'DeCock, Paul C', 'Rising, Lynn T.', 'Whall, Raymond A.', 'Holloway, Susanne A', 'Brahosky, Adam C', 'Cross, Jessica E', 'Mathers, Ani M', 'Meehan, Mattie R', 'Strittmatter, Dennis A', 'Vazquez Diaz, Ricardo', 'Diehl, Heather S', 'Abresch, Bruce Anthony', 'Huston, Sharon P.', 'Ruffo, Nicholas S', 'Folkoff, Michael E.', 'Pass, Victoria R', 'Gambini de Secondi, Vanina', 'Hall, Logan Miller', 'Smith, Nancy Marie', 'Bodley, Dana', 'Taylor-Thoma, Marcie', 'McGee, Pamela Linda', 'Doyle, Melissa S', 'Madachik, Mark Robert', 'Rees, Jon M', 'Guillemart, Nancy Crabson', 'Bulka, Yuriy', 'Bell, Joyce A.', 'Obeng, Efua B', 'De Fino, Dean', 'Basham, Viktoria Valerieva', 'Batts, Lori Landon', 'Miller, Katherine R.', 'Tomcho, Thomas J.', 'Masenior, Nicole F', 'Money, Jessica Paige', 'Bauer II, Keith W', 'McDowell, Reed D.', 'Hornstein, Anna Michele', 'Powers, Emma-Ann', 'Hill, Lauren R', 'Wheatley, Laura Marie', 'Sharma, Andrew', 'McKittrick, Jamie Lauren', 'Edwards, Jossolyn R.', 'Trout, Kristin G.', 'Ennis, Eleanor M', 'Garner, Robert M.', 'Koval, Michael R.', 'Hesen, James Robert', 'Benyish Jr, Joseph W', 'Gathagan, Bryan K', 'Senkbeil, Edward G.', 'Smith, Alison Jean', 'Avara, Mary V', 'Shipper, Frank M.', 'Ananou, Regine C', 'Flexner, Paul Q.', 'Shary, Timothy M', 'Welty, Phillip Warren', 'Hentschel, Sandy E', 'Ratti, Manav', 'Rittling, Nicole Angela', 'Richerson III, William R', 'Anderson, Matthew Lee', 'Jett, Monica K', 'Goblinger, Jean L', 'Hoffman, Robert', \"O'Sullivan, Shannon E.M.\", 'Mitra, Deeya', 'Follmer, Kayla B', 'Toonstra, Jenny L', 'Anderson, Philip D', 'Walter-Nusberger, Julian', 'Sammons, Jason Harry', 'Siegert, Nancy L', 'Metzger, Gayle Elizabeth', 'Hanley, Yvonne D', 'Holmes, Stephen M.', 'Hylton, Mary E', 'Wood, Cara L', 'Conder, Cory J', 'Buono, Alexia Jordan', 'Pope, Katherine C', 'Given, Kirby Mae', 'Camillo, Christina G', 'Street, Vera L.', 'Sterling, Amy M', 'Smith, Renee Ashley', 'Nelson, William A', 'Wainwright, Barbara A.', 'Leoutsakas, Dennis A.V.', 'Kang, Ellen K', 'Culler, Kirsten A', 'Nally, Elizabeth Anne', 'Martin, Joni Pittenger', 'Elzey Jr, Guy S.', 'Offen, Keith', 'Snee, Aric Donald', 'Hansen, Toran J', 'Singh, Nitya P', 'Lewis, Michael L.', 'Small, Clara L.', 'Larsen, Erik J', 'Hubbard, Jennifer M', 'Everhart, Sarah M', 'Chambers, Dustin L.', 'Cai, Jiacheng', 'Echols, Brittany Ayanna', 'Nolte, Sharon E.', 'Chaikel, Joseph', 'Lee, Yerin', 'Phillips, Ashley Nicole', 'Merkel, Joshua Anthony', 'Lucas, Timothy D', 'Adams, Geralyn M', 'Anderson, Katelin Elizabeth', 'Kushner, Margo A.', 'Boroughs, Jon J', 'Danderson, Mark L', 'Scheid, David T', 'Busko, Nicholas', 'Rue, Constance', 'Forte, James A.', 'Timmons, John Adam', 'Keeton, Thomas George', 'Michalski, Jeremy B', 'Salimian, Fatollah', \"O'Neal, Elaine Marie\", 'Gladden, Tara C', 'Marzocchi, Jaime Michel', 'Ellis, William C', 'Jones, Shauna-Kaye Olivia', 'Lee, Misuk', 'Kellerman, Shelby M', 'Corrigan, Matthew J.', 'Insley III, Carlton R', 'Gostomski, Alaina Marie Macdonald', 'Jenne, Joel T.', 'Hughes, Peggy A', 'Talbert, Bart R', 'Bievenour, Michael George', 'Biasotto, Marylou F', 'Wenke, Joseph Hughes', 'Grande, Christopher Michael', 'Martin, John W', 'Shakley, Sherri-Lynn', 'Mazzetti, Heather Morea', 'Ervin, Danny M.', 'Hammond, Walter D.', 'Engberg, Kathryn Ann', 'Zlabek, Katherine M', 'Champagne, Carole A.', 'DiBartolo, Mary C.', 'Price, Dana Leigh', 'Bowden, Derek Thomas', 'Long-White, Deneen N', 'La Chance Jr, Robert Michael', 'Kearney-Edwards, Katina B', 'Kiernan, Kathleen', 'Evans, Timothy V', 'Foster, Joseph K', 'Kim, Yun-Kyoung', 'Miller, Brian K', 'Kaylor, James Richard', 'Carmack, Lori A', 'Howard, Tiffany', \"O'Donnell, James Joseph\", 'Holt, Susan M', 'Randall, Susannah Marie', 'Hawbaker, Beth L', 'Koko, Jacques L.', 'Fiala, Kelly A.', 'Weir, Adam W', 'Warner-Chaibou, Keoshia Danielle', 'Choi, Yoojin', 'Birch, Sharon McMahon', 'Lei, Shan', 'Johnson, David T', 'Betterton, Erika', 'Imirie, Olivia Nicole', 'Hogue, Aaron S.', 'Gehnrich, Stephen C.', 'Deminne, Kevin Francis', 'Dotterer, Ronald L.', 'Arvi, Marie S', 'Reid, Melissa Moore', 'Borden, Lizzette Melisa Aristizabal', 'Kolstoe, Sonja H', 'Bergo, Conrad H', 'Jester, Otto Frederick', 'Hoenigmann-Lion, Natalia M.', 'Searcey, Marilyn Gail', 'Wight, Charles A', 'Friese, Seth J.', 'Genareo, Vincent R', 'Youmans, Daniel Charles', 'Bing, John R.', 'Peterson, Dean A.', 'Henry, Candace N', 'Wynkoop, Sean Gregory', 'Berns, Chelsea M', 'deWitt, Wallace', 'Harrington, Aliscia Dawn', 'DeWitt, Lori J.', 'Parnell, Darren B.', 'Boudreau, Thomas E.', 'Waters, Amy Hollis', 'Summers, George F.', 'Townsend, Zachary Michael', 'Morgoch, Meredith', 'Ireland, Holly', 'Premo-Hurt, Joran Premo', 'Massey, Kevin Dale', 'Pasirayi, Simbarashe', 'Dorman, Mary Amber', 'Towle, Beth A', 'Hunter, Richard B.', 'Corn, Jennifer Helen', 'Hade, Eden Terry', 'Madden, Meredith J', 'Silver, Patrice Marie', 'Leaver, Echo E', 'Okubo, Yuki', 'Bowden, Kara Lewis', 'Boog, Melissa M.', 'Fabian, Chelsea Nicole', 'Liston, Jennifer M', 'Grecay, Paul A.', 'Chen, Xingzhi Mara', 'Hyman, Batya', 'Keen, Charles', 'King, James S.', 'Emmert, Jeffrey W.', 'De Ridder, Jerome J', 'Christensen, Pearl H', 'Jack, Carlotta Gray', 'Jenkins, Ginger A', 'Baker, Ali Elizabeth', 'Kane, Francis I.', 'Gibbs, Lincoln A', 'Wu, Ying', 'Miller, Heather Rose', 'Wallace, David S.', 'Tuske, Joerg P', 'Ravizza, Dean M.', 'Whitehead, George I.', 'Thamert, Amy Nicole', 'Pahl, Jennifer Anne', 'Berg Jr, R Clifford', 'Waddy, Angela M', 'Perez, Brian', 'Payne, Cynthia Ann', 'Quintana Wulf, Isabel', 'Horikami, Bryan K.', 'Howard, Joseph W.', 'Daly, Elizabeth Susanne', 'Raley, John L', 'Zaprowski, Brent J.', 'Newman, Stacey Virginia', 'Sullivan, Breck Maura', 'Morrison, Eddy', 'Keyser, Bethany J', 'Murphy, Janice', 'Johnson, Katherine Joanne', 'Roche, Olivier P.', 'Fahey, R. Sean', 'Arausa, Christopher Jefford', 'Hurley, Timothy R.', 'Hahn, Eugene D.', 'Durow, Steven L', 'Lattimore, Somiah M', 'Nibblett, Nancy Taylor', 'Goldberg, Martin D', 'Bivens, Bruce T', 'Cottman, Tracey Meshelle', 'Eagle, Harlan A.', 'Pope IV, Alexander', 'Howard, Julia Wright', 'Gasior, Paul L', 'Kilian, Lisa M', 'Burke Jr, William P.', 'Riley, Cindy L', 'Liswell, Mary', 'Holmes-Kriger, Jamie Lynn', 'Willey, Jessie Ellen', 'Chandrashekar, Sumathy X.', 'McComb, Ellis Wellington', 'Eksi Linderkamp, Ayse Asli', 'Young, Kevin', 'Hartman, Laura Elizabeth', 'James, Katherine Elaine', 'Chopra, Sehmina Jaffer', 'Hindman, Jessica Anne', 'Gonzalez, Aston A', 'English, Helene D', 'Mister, Brenda J.', 'Holmes, John Matthew', 'Wessel, Michael J', 'Catlin, Amanda Susan', 'Bonsteel, Jennifer Marie', 'Dean, Patricia K.', 'Hedlesky, Michael R', 'Cox, Jeremy G', 'Schiavone, Aubrey Vera Joan', 'Jones, Thomas W.', 'Meyer, Amber L', 'Curtin, Katherine M', 'Bemis, Rhyannon H', 'Headrick, Matthew Dennis', 'Caballero, Guillermo A', 'Gilkey, Mary Ann', 'Tabb, Christopher W', 'Calo, Thomas J', 'Tossey, Lisa Danielle', 'Cao, Wen', 'LaChance, Haleigh N', 'Reyes Jacobo, Edgar E', 'Karten, Karen Joy', 'Guevara, Jacqueline J', 'Boso, Christian Makafui', 'Merrick, Lutisia Jeanene', 'Patterson, Michael C', 'Morrison, Robert L', 'Nieves, John A', 'Martin, Sara R', 'Berry, Dagmar', 'Hill, Brian C.', 'Ebanks, Davin K', 'Burrows-McElwain, Cicely Kenda', 'McCabe, Douglas W', 'Gibson, Noah McGarrity', 'Kantzes, Robert', 'Morgan, Brian Andrew', 'Kruglinski, Jennifer M', 'McCabe, Karen L', 'Dean, Mimi F', 'Emerson, Jamie D.', 'Winchell, Katharine A', 'Engle, Lori R', 'Basehart, H Harry', 'Gutberlet Jr, Ronald L.', 'Tu, Junyi', 'Lin, Wei-Ting', 'Mazzetti, Scott A.', 'Mosher, John C', 'Sommer, Kylie Renee', 'Gilchrest, Wayne T', 'Price, Tonya A', 'Beegle, Gwen P.', 'Myers, Helen M', 'Geleta, Samuel B.', 'Park, Ki Ho', 'Creese, Charlene Matthews', 'Mitchell-Ebert, Nancy', 'Ballard, Adrian John', 'McGlone-Smith, Morgan Elizabeth', 'Shannon-Ramsey, Vivian R.', 'Paik, Seung Joon', 'Conners, Keith J.', 'Khazeh, Khashayar', 'Sessoms, Diallo D.', 'Deal, Aaron Darnell', 'Saunders, Jennifer Alden', 'Catron, Arielle Genevieve', 'Stutelberg, Erin B', 'Hill, Amanda Gail', 'Wilson, Logan Alexander', 'Hamilton, Stuart E', 'Hatton, Holly D', 'Knier, Lawrence E', 'Rexrode, Tiffany L', 'Shuster, Eric T', 'Young, Peter Ivan', 'Rigsby, Marcia E', 'Trolio, David P', 'Ehrhardt, Ursula M.', 'Twigg, Claire Louise', 'Mallow, Suzanna Lynette', 'Poore, Alesha R', 'Lutz, Melissa Faith', 'Robbins, Elbert M', 'Kyereme, Kofi', 'Anderson, Ellen J', 'Perret, Sally A', 'England, Charlotte P', 'Greenwood, Elena Rose', 'Walls, Tasha Renee', 'Kauffman, Elizabeth C', 'Emerson, David J', 'Rahimlabaf-Zadeh, Sanaz', 'Clark-Shaw, Kimberly D.', 'Seldomridge, Elizabeth A.', 'Siers, Stacie E', 'Boyer, Jefferson Parks', 'Jarrett, Brian N.', 'Troup, Nicholas W', 'Hatfield, Lindsey Marie', 'Cicero, Carrianne Marena', 'Flagg, Mary B', 'Brace, Andrea M', 'Van Vulpen, Kimberly Searcey', 'Cockey, Linda E.', 'Leffew, Erin Ashley', 'Seneviratne, Buddhakoralalage Leelange', 'Curtin, Elizabeth H.', 'Davis, Daniel Joseph', 'White, Katie Corbin', 'Whitney, Jeanne E.', 'Presotto, Andrea', 'Michaud, Matthew J', 'Thompsen, Pamala Dawn', 'Horvath, Todd M', 'Wagner, Diana Mae', 'Manizade, Kathrine F', 'Polkinghorn, Brian D.', 'Stanfield, Kellie P', 'Mahoney, Susan L', 'Freeman, Angela Rose', 'Stewart, Christina Marie', 'Stock, Timothy E.', 'Block, Megan Hopper', 'Street, Marc D.', 'Wood, Adam H.', 'Bond, Ernest L.', 'Nyland, Jennifer F', 'Davis, Daniel James', 'Mayhew, Heather Aubree', 'Crowley, Caitlyn Howard', 'McHenry, Patrick R', 'Willey, Jeffrey Allan', 'Cervantes, Mason', 'Barnes, Marita A', 'Cone, Randall E', 'Lee, John C', 'Muller, Mark W', 'McDermott, Edward Patrick', 'Gomez Roman, Alejandro', 'Cook, Brian Joseph', 'Michelson, Nancy L.', 'Shifler, Ryan Michael', 'Lycett, Kristen A', 'Lembo, Arthur J.', 'Nastvogel, LaShawn D', 'Mitchell, Miguel O.', 'Seth, Douglas', 'Ennis, Michele Lynn', 'Boone, Laverne M', 'McCombs, Alexandria G', 'Matthews, Heather Joy', 'Miller, Ryan Scott', 'Jones, Edward F.', 'Endicott, Kelsie', 'Farrow, Julia Thompson', 'Marshall, P Douglas', 'Owen, Gwendolyn Rachel', 'Webster, Debra A.', 'Long III, Creston S.', 'Karli, Matthew Todd', 'Keough, Gerard E.', 'Rockelli, Laurie Ann', 'Mitchell, Rhonda Marie', 'Tossey, Marvin G.', 'Sweadner, Stefany K', 'Carroll, Karen Morris', 'Habay, Stephen A.', 'Adeniran, Adeniyi Adeleke', 'Marinaro, Laura Marie', 'Milligan, Teena M', 'Clatterbuck, Jessica Paige', 'Schuldt, Michael A.', 'Johnson, Jennifer A', 'Cardillino, Theresa Ann', 'Johnston, Eric William', 'Valenti, Sherry Dutrow', 'Trenary, Melany C.', 'Marquez, Loren L.', 'McBrien, Robert J', 'Vilmar, Christopher S.', 'Johnson, Robert L.', 'Moeder, Michael D.', 'Root, Vicki J', 'Pica, Dorinda F', 'Makuchal, Penny Justice', 'Bratten, Sylvia Theresa', 'Bhuvanesh, Abhinesh', 'Fritz, Heidi L', 'McCray Jr, Jacquis Allen', 'Schaefer-Salins, Ellen', 'Lauterburg, Steven T', 'Carlisle, Gregory Jason', 'Chasse, Robert', 'Dziwenka, Ronald J', 'Pearl, Kendall Mayson', 'Douglas, Katherine Brooks', 'Song, Yujia', 'Dotson, Debra Ann', 'McGrath, Stacy Marie', 'Beckman, Pamela Joy', 'Blickle, Carl W', 'Passyn, Kirsten A.', 'Jones, James A.', 'Joyner, Robert L.', 'Shoop, Steven R', 'Romanowski, Autumn D', 'Ross, Michael C', 'Garmon, Lance C.', 'Arbona, Lourdes Tarin', 'Abel, Robert Frank', 'Fossell, Jennifer E', 'Halfpap, Randal B.', 'Diriker, Memo F.', 'Weber, Rachel Lynn', 'Tusing, Jennifer Leigh', 'Hilton, David N', 'Skeeter, Cheryl Strickler', 'Claggett Jr, Ellwood Tyler', 'Bowler, Richard C.', 'Billups, Marion Judith', 'Perreault, Melanie L.', 'Liebgold, Eric B', 'Daly, Terence F', 'Brightful, Erika Dar', 'Hall, Nicole Julienne', 'Roche, Oliver P.', 'Seth, Allison B', 'Robinson, Rashid V', 'Sullivan, Kyle G', 'Howard, Matthew A', 'Story, Emily F.', 'Castellano, Krystle Lynn', 'Ahlstrom, Christopher L', 'Garner, Robert Scott', 'McCracken, Angela Dawn', 'Howard, Emilee D', 'Reagin, Timothy Mitchell', \"Lelic', Emin\", 'Harper, Julia Nelson', 'Nina, Alketa', 'Kaiser, Kelly Lynn', 'Fedorko, Brent F', 'Schock, Melissa English', 'Burgess, Claudia R.', 'Wolfe, Ira S', 'Jing, Yaping', 'Adeogun, John Olufemi', 'Pautler, Charles P.', 'Canter, Miranda M', 'Gebel, Doris J', 'Hoffman IV, Richard C.', 'McAneny, Kathleen M', 'Fennell, Patrick B', 'Vogelsong, Candace Scott', 'Soosaipillai, Ignaciyas K', 'Lu, Enyue', 'Williams, Lauren A', 'Campbell, Jesse Alan'}\n"
     ]
    }
   ],
   "source": [
    "official_faculty_set: set[str] = set(official_professor_names_and_dept.keys())\n",
    "print(official_faculty_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from My_Data_Classes import CategoryInfo\n",
    "import pickle\n",
    "\n",
    "\n",
    "def open_dict_pkl(file_path: str):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict = open_dict_pkl(\"./category_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Curry, Caitlin', 'Hamilton, Stu', 'Kim, Yun-Kyoung', 'Freeman, Angela R.', 'Bressman, Noah', 'Disbennett, Mackensie', 'Joyce, Ryan Patrick', 'Ramanathan, Gurupriya', 'Silaphone, Keota', 'Muller, Mark W.', 'Presotto, A.', 'Irons, Jonathan', 'Auerbach, Anna Jo J.', 'Revel, Cody B.', 'Porter, Heather', 'Freda, Kaynabess', 'Clark, Jessica', 'King, Carolyne M.', 'Karimzad, Farzad', 'Choi, Yoojin', 'Lei, Shan', 'Talbot, John', 'Nein, Matthew A.', 'Yao, Hong', 'Jeon, Kwonchan', 'Kolstoe, Sonja', 'Hogue, Aaron S.', 'Weer, Christy H.', 'Mitchell, Kaitlyn', 'Hensiek, Sarah', 'Brennan, Anna', 'Brannock-Cox, Jennifer', 'Lycett, Kristen A.', 'Maloof, Joan E.', 'Juncosa, Jose I.', 'Lanehart, Emily', 'Naing, Yadanar Than', 'Townsend, Zachary', 'Jauregui, Jean E.', 'Barnes, Annette', 'Friese, Seth J.', 'Chappell, Charisse', 'Cusic, Rachel M.', 'Small, Hannah G.', 'Boster, Charles R.', 'Pellinger, Thomas K.', 'Padgett, Stephen M.', 'Pasirayi, Simba', 'Chaves, Alec B.', 'Lane, Halle L.', 'Sargent, Sheridan', 'Rost-Nasshan, Aerin', 'Halperin, Alexander', 'Nalesnik, Allison', 'Moghaddam, Masoud', 'Walter, Mark I.', 'Marr, Alissa N.', 'Jermain, Madison', 'Venable, Victoria', 'Smith, Kenneth', 'Evans, Amanda S.', 'Petrolino, Ashley V.', 'Schneider, Sidney R.', 'Walsh, Catherine M.', 'Dolezar, Gina M.', 'Manchak, Randi', 'Barrett, G. Douglas', 'East, Meghan', 'Pike, Judith E.', 'Brady, Alyssa', 'Paco, Paola J.', 'Morningred, Connor', 'Seho-Ahiable, Gloria E.', 'Restein, Joseph', 'Pellinger, Thomas', 'Bachran, Karsin', 'Pasirayi, Simbarashe', 'Mazzetti, Scott', 'Comfort, Joshua', 'Eun, Jihyun', 'Phillips, Robert A.', 'Vennos, A.', 'Schwartz, Joseph', 'Quillin, Kim', 'Okubo, Yuki', 'Leaver, Echo', 'Stott, Mia', 'Stutelberg, Erin B.', 'Wesolowski, Sarah C.', 'Maykrantz, S. A.', 'Stoner, M.', 'Treuth, Margarita S.', 'Hoffman, Richard', 'Schneider, Lisa', 'Chen, Xingzhi Mara', 'Grecay, Paul A.', 'Chen, Mara', 'Eksi, Asli', 'Corfield, Jeremy R.', 'Habermeyer, Ryan', 'Price, Dana L.', 'Wu, Yun', 'Werner, Tim', 'Caviglia-Harris, Jill L.', 'Hamilton, Stuart', 'Morgan, Brian', 'Warfield, Rebecca', 'Wu, Ying', 'McCartney, Jason', 'Green, Daniel C.', 'Garcia, Mark J.', 'Ravizza, Dean M.', 'Tibbo, Morgan', 'Whitehead, George I.', 'Stribling, Judith M.', 'Kotlowski, Dean', 'Jung, Kyoung-Rae', 'Groth, Randall E.', 'Liebgold, Eric B.', 'Guarnera, Samantha R.', 'Coss, Derek A.', 'Hartlove, Nathan B.', 'Long-White, Deneen', 'Geleta, S. B.', 'Ennerfelt, Hannah', 'Cha, Hoon S.', 'Miller, Stephanie', 'Follmer, Kayla B.', 'Demond, Whitney', 'Richardson, Gwynne', 'Cave, Jason', 'Roose, Jordan J.', 'Roberts, Paige M.', 'Hawkins, Ashley E.', 'Walter, Jessica', 'Wathen, Bailee', 'Webster, Debra', 'Miller, Jenna', 'Peng, Yuqi', 'Grant, Alexa H.', 'Hahn, Eugene D.', 'Cammarano, Cristina', 'Fox, James T.', 'Short, Laura', 'Scanlon, Jaycee', 'Rosette, Demetri', 'Werner, Timothy', 'Clements, Paul', 'Kalita, Diane', 'Masteran, Conner J.', 'Duong, Hong', 'Austin, Jathan', 'Surak, Sarah', 'Briand, Christopher H.', 'Meister, Ben M.', 'Nan, Wenxiu (Vince)', 'Maier, Karl J.', 'Vicens, Belen', 'Hanley, Yvonne Downie', 'Miller, Jerome A.', 'Warman, Stephanie', 'Miao, Chao', 'Wang, Shuangquan', 'Shepherd, Meghan', 'Teller, Kyle G.', 'Munemo, Jonathan', 'Allen, Kim', 'Sporer, Ryan Alan', 'Genareo, Vincent R.', 'Follmer, Kayla', 'Fennell, Patrick', 'Jewell, Mollie Anne', 'Anderson, Philip D.', 'Fedorko, Brent', 'Molina, Michael', 'Tchienga, Ines', 'Conrath, Ryan', 'Lamb, Stephanie M.', 'Mrozinski, Abigayle C.', 'Chapman, Hayley A.', 'Schlehofer, Michele M.', 'Olortegui, Ashley', 'Pandey, Anjali', 'Park, Minseok', 'Wilson, Abigail', 'Singh, Nitya', 'Miller, Lien', 'Mathers, Ani Manakyan', 'Benner, Jeffrey', 'Baker, Keirsten T.', 'Ali, Bakr', 'Naboulsi, Amar L.', 'Logan, April C.', 'Singh, Nitya P.', 'Bones, Lela', 'Willey, Amanda', 'Sen, Argha', 'Mitchell, Kelsey M.', 'Bush, Domonique', 'Whitehead, George, I', 'Emmert, Jeffrey', 'Caballero, Guillermo', 'Joyner, Robert L., Jr.', 'Emmert, Elizabeth', 'Leonel, Ronei', 'Cottingham, Andrea', 'Hart, Jennifer', 'Simons, Patrick', 'Stanfield, Kellie', 'Taylor, Ryan C.', 'Marinaro, Laura', 'Ortlip, Austin T.', 'Ramseyer, Craig A.', 'Bressman, N. R.', 'Hoffman, Richard C.', 'Ransom, Tami', 'Taylor, Ryan', 'Maykrantz, Sherry A.', 'Hassan, Tehzeeb', 'Nobiling, Brandye D.', 'Ludovici, Rosalind J.', 'Boyd, Gerard', 'Emerson, David J.', 'Wooleyhand, Sean', 'Tomcho, Thomas', 'French, Kara M.', 'Habay, Stephen', 'Tu, Junyi', 'Martino, Andrew', 'Van de Wide, Aurelie', 'Bradley, Christina J.', 'MacDougall, Madison', 'Falcone, Gianna', 'Seldomridge, Lisa A.', 'Coyle, Krystina', 'Lamanca, John', 'Davis, Jeni', 'Schwartz, Alison C.', 'Barnes, Samuel', 'Emerson, Jamie', 'Kim, Sook Hyun', 'Geleta, Samuel B.', 'Harris, Daniel W.', 'Silaphone, K.', 'Fountain, William A.', 'Owens-King, Allessia P.', 'Poddar, Amit', 'Richerson, Rob', 'Rosette, Vincent D.', 'Jarosinski, Judith M.', 'Walker, Elsie', 'Breon, Kathryn', 'Emmert, Elizabeth A. B.', 'Koehler, S.', 'Timmons, Miranda', 'Agarwal, Vinita', 'Clark, Jessica Kennett', 'Berns, Chelsea M.', 'Billups, M. Judith', 'Martin, Jennifer M.', 'Vance, Morgan', 'Han, Eun-Jeong', 'Gimblet, Colin', 'Anthony, Becky', 'Nyland, Jennifer F.', 'Hong Duong', 'Allen, Kimberly', 'Phillips, David S.', 'Tuske, Joerg', 'Villalobos, Laura', 'Carlini, Nicholas A.', 'Nutt, Rita', 'Steele, Rachel R.', 'Kim, Yun Kyoung', 'Bolton, Joshua P.', 'Fox, James', 'Schneider, Gustavo', 'Smith, Kenneth J.', 'Rojas, Anthony J.', 'Bushera, Hakeem', 'Rowe, Emily', 'Peixoto, Sinelia', 'Bergner, Jennifer', 'Hill, Brian', 'Holdai, Veera', 'Carter, Michael S.', 'Wesolowski, S.', 'Kotlowski, Dean J.', 'Cheng, Yijie', 'Gang, KwangWook', 'Follmer, D. Jake', 'Arban, Kathleen', 'McCarty, Michael', 'Daly, E. Susanne', 'Hunter, Kimberly L.', 'Perret, Sally', 'Thomas, L.', 'Mann, Hunter', 'Harbaugh, Jessica', 'Keethaponcalan, S. I.', 'Miller, Timothy S.', 'Erickson, Les', 'Presotto, Andrea', 'Folkoff, Michael E.', 'Carayon, Celine', 'Howell, Julia', \"Evans, La'Tier\", 'Troup, Nicholas', 'Wang, Xiaohong', 'Ancalmo, Juliana', 'Shakur, Asif', 'Whittaker, Amber', 'Osman, Suzanne L.', 'Kuszmaul, Dillon J.', 'Albright, Savannah G.', 'Polkinghorn, Brian D.', 'Cronin, Andrew', 'Thomas, John Z.', 'Hall, Nicole', 'Harris, Mercedes E.', 'Fletcher, Michelle N.', 'Kim, Chan', 'Jewell, Jennifer R.', 'Flint, Carl', 'Othman, Yasmeen', 'Franzak, Judith K.', 'Purohit, Abhishek', 'Schlehofer, Michele M. M.', 'Fritz, Heidi L.', 'Rivera, Allyson', 'Cervantes, Mason', 'Montgomery, Chandini B.', 'Wille, Brendan', 'Stutelberg, Erin', 'Stafford, Kathleen', 'Morris, Paula T.', 'Hopkins, Emily', 'Koval, Michael R.', 'Weaver, Starlin', 'Hamilton, Stuart E.', 'Allen, Samantha', 'Kramer, Michael E.', 'Hatley, James', 'Ference, Gregory C.', 'Hagadorn, Mallory A.', 'Roche, Olivier', 'Scott, Michael', 'Marquette, Lisa', 'DiBartolo, Mary', 'McElroy, Honor', 'Egan, Chrys', 'Pope, Alexander', 'Lahay, Amber R.', 'Staudmyer, Timothy', 'Petersen, Courtney', 'Chambers, Dustin', 'Yoon, David J.', 'Glass, Noah', 'Mirza, Ateeb M.', 'Mcelroy, Honor B.', 'Ratti, Manav', 'Fowler, Garrett', 'Habay, Stephen A.', 'Bhatti, Khadija', 'Sokoloski, Joshua E.', 'Mills, William', 'Ottoni Santiago, Thais Muniz', 'Schuldt, Michael A.', 'Koh, Bibiana D.', 'Johnson, Aaron', 'Gallo, Silviya M.', 'Miller, Julia M.', 'Rocker, Amanda', 'Rohde, Jacki', 'Boyd, Marshall', 'Sarrett, Austin', 'Reid, Tina P.', 'Erickson, Patti T.', 'Hill, Amanda', 'Krach, Noah', 'Hammond, Courtney Nicole', 'Joyce, Ryan', 'Caviglia-Harris, Jill', 'Adams, Stephen B.', 'Kramer, Christian F.', 'Seldomridge, Lisa', 'Quarantillo, Michael E.', 'Nyland, Jennifer', 'Chen, Xuan', 'Boster, Charles', 'Lynch, Kerry E.', 'Rickards, Megan', 'Lee, Byung Ho', 'Rose, Chelsi M.', 'Rittinger, Eric R.', 'Locklear, Robert', 'Kulavuz-Onal, Derya', 'Keifer, David', 'Norman, Brandon', 'Quan, Jing', 'Woodis, Lena A.', 'Green, Daniel', 'Bennett, Harmon', 'Laaouad-dodoo, Soraya', 'Fillebrown, J.', 'Cooper, Sue A.', 'Stoner, Alexander M.', 'Adler, Laurie', 'Nibblett, Kaitlyn', 'Song, Yujia', 'Tiberio, Layla', 'Billups, M. J.', 'Wenke, John', 'Williams, E. Eugene', 'Naumann, Madeline', 'Naing, Yadanar', 'Shifler, Ryan M.', 'Chaudhry, Bilal', 'Muller, Susan M.', 'Cai, Jiacheng', 'Franchi, Giulia', 'Talbert, Bart', 'Hughes, Ryan', 'Lynch, Colleen E.', 'Manole, Denise', 'Li, Ning', 'Bowler, Matthew M.', 'Goyens, Tom', 'Emerson, David', 'Harrington, Gary', 'Hinderer, Katherine A.', 'Insley, Carlton R., III', 'Johnson, David T.', 'Cimiluca, Mark', 'Barry, James', 'Voithofer, Gabrielle', 'Surak, Sarah M.', 'Nieves, John A.', 'Walton, Kristen Post', 'Patel, Shruti', 'Rexroth, Kayla S.', 'Robbins, Ethan', 'Briand, C. H.', 'Groth, Randall', 'Winter, Dorothea M.', 'Franzak, Judith', 'Brown, Voncelia', 'Wulf, Isabel Quintana', 'Scott, Michael S.', 'Wehlan, T.', 'Jing, Yaping', 'Wilhite, Kyle O.', 'Lindner, Nicole C.', 'Bowler, Richard', 'Kobisk, Ashley', 'Labb, Samantha A.', 'Chaudhry, Eaqan A.', 'Kim, Koomi', 'Finch, Maida A.', 'Hylton, Mary', 'Naing, Yadanar T.', 'Bemis, Rhyannon H.', \"O'Sullivan, Shannon E. M.\", 'Ortlip, Austin', 'Williamson, Thea', 'Gonzalez, Aston', 'Grubb, Brittany', 'DiBartolo, Mary C.', 'Valenti, Zackary J.'}\n"
     ]
    }
   ],
   "source": [
    "unique_fac_set: set[str] = set()\n",
    "\n",
    "for category, category_info in cat_dict.items():\n",
    "    for faculty in category_info.faculty:\n",
    "        unique_fac_set.add(faculty)\n",
    "\n",
    "print(unique_fac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process\n",
    "from itertools import combinations\n",
    "import collections\n",
    "\n",
    "name_groups = collections.defaultdict(list)\n",
    "for name in unique_fac_set:\n",
    "    if name == \"\":\n",
    "        continue\n",
    "    if \", \" in name:\n",
    "        last_name, remainder = name.split(\", \", 1)\n",
    "        first_initial = remainder[0] if remainder else \"\"\n",
    "    else:\n",
    "        # If there's no comma, split by the first space.\n",
    "        parts = name.split(\" \", 1)\n",
    "        if len(parts) == 2:\n",
    "            last_name, remainder = parts\n",
    "            first_initial = remainder[0] if remainder else \"\"\n",
    "        else:\n",
    "            # Handle case where there's no space (single-word name)\n",
    "            last_name = name\n",
    "            first_initial = \"\"\n",
    "    group_key = (last_name, first_initial)\n",
    "    name_groups[group_key].append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check similarity\n",
    "count = 0  # Initialize count at the global scope\n",
    "count2 = 0\n",
    "\n",
    "\n",
    "def check_similarity(names):\n",
    "    global count  # Declare count as global to modify the global variable inside this function\n",
    "    pairs = combinations(names, 2)\n",
    "    for item1, item2 in pairs:\n",
    "        # Extract the first names and consider only up to the first space (if present)\n",
    "        first_name1 = item1.split(\", \")[1].split(\" \")[0] if \", \" in item1 else \"\"\n",
    "        first_name2 = item2.split(\", \")[1].split(\" \")[0] if \", \" in item2 else \"\"\n",
    "\n",
    "        # Check if the first letters of the first names match and compare the lengths of the first names\n",
    "        if first_name1[0] == first_name2[0] and len(first_name1) == len(first_name2):\n",
    "            similarity = fuzz.ratio(item1, item2)\n",
    "            print(f'Names \"{item1}\" and \"{item2}\" have a similarity of {similarity}%')\n",
    "            count += 1\n",
    "\n",
    "\n",
    "def check_similarity_3(names):\n",
    "    global count2  # Declare count as global to modify the global variable inside this function\n",
    "    pairs = combinations(names, 3)\n",
    "    for item1, item2, item3 in pairs:\n",
    "        # Extract the first names and consider only up to the first space (if present)\n",
    "        first_name1 = item1.split(\", \")[1].split(\" \")[0] if \", \" in item1 else \"\"\n",
    "        first_name2 = item2.split(\", \")[1].split(\" \")[0] if \", \" in item2 else \"\"\n",
    "        first_name3 = item3.split(\", \")[1].split(\" \")[0] if \", \" in item3 else \"\"\n",
    "\n",
    "        # Check if the first letters of the first names match and compare the lengths of the first names\n",
    "        if (\n",
    "            first_name1[0] == first_name2[0]\n",
    "            and len(first_name1) == len(first_name2)\n",
    "            and first_name1[0] == first_name3[0]\n",
    "            and len(first_name1) == len(first_name3)\n",
    "        ) or (\n",
    "            first_name2[0] == first_name3[0] and len(first_name2) == len(first_name3)\n",
    "        ):\n",
    "            similarity = fuzz.ratio(item1, item2)\n",
    "            similarity2 = fuzz.ratio(item2, item3)\n",
    "            print(f'Names \"{item1}\" and \"{item2}\" have a similarity of {similarity}%')\n",
    "            print(f'Names \"{item2}\" and \"{item3}\" have a similarity of {similarity}%')\n",
    "            count2 += 1\n",
    "\n",
    "\n",
    "# Iterate over the groups and check similarity within each group\n",
    "for group in name_groups.values():\n",
    "    if (\n",
    "        len(group) > 1 and len(group) < 3\n",
    "    ):  # Only compare if there are at least 2 names in the group\n",
    "        # print(f\"{group}\")\n",
    "        check_similarity(group)\n",
    "    elif len(group) > 2:\n",
    "        print(f\"\\n{group}\")\n",
    "        check_similarity_3(group)\n",
    "print(\n",
    "    f\"\\nNumber of pairs that share the same last name, same first initial, and same first name length: {count}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of triplets that share the same last name, same first initial, and same first name length: {count2}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_name_a_group_key(name, name_groups):\n",
    "    # Reformat name input so it's just the lastname and first initial in order to match the key format in name_groups\n",
    "    if \", \" in name:\n",
    "        last_name, remainder = name.split(\", \", 1)\n",
    "        first_initial = remainder[0] if remainder else \"\"\n",
    "    else:\n",
    "        # If there is no comma split by the first space\n",
    "        parts = name.split(\" \", 1)\n",
    "        if len(parts) == 2:\n",
    "            last_name, remainder = parts\n",
    "            first_initial = remainder[0] if remainder else \"\"\n",
    "        else:\n",
    "            # For the case a name is a single word\n",
    "            last_name = name\n",
    "            first_initial = \"\"\n",
    "\n",
    "    # create a key that matches the key formats in name_groups\n",
    "    group_key = (last_name, first_initial)\n",
    "\n",
    "    # check if the key exists in name_groups\n",
    "    if group_key in name_groups:\n",
    "        return True, name_groups[group_key]\n",
    "    else:\n",
    "        return False, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "unique_fac_set: set[str] = set()\n",
    "pattern = r\"^[A-Za-z]+,\\s[A-Za-z]+\\s[A-Z]\\.$\"\n",
    "\n",
    "\n",
    "for category, category_info in cat_dict.items():\n",
    "    for faculty in category_info.faculty:\n",
    "        unique_fac_set.add(faculty)\n",
    "\n",
    "refined_unique_fac_set: set[str] = set()\n",
    "for name in unique_fac_set:\n",
    "    a_group_key = is_name_a_group_key(name, name_groups)\n",
    "    if a_group_key[0]:\n",
    "        values = a_group_key[1]\n",
    "        desired_value: str = \"\"\n",
    "        for value in values:\n",
    "            if re.match(pattern, value):\n",
    "                desired_value = value\n",
    "            else:\n",
    "                desired_value = max(values, key=len)\n",
    "            refined_unique_fac_set.add(desired_value)\n",
    "\n",
    "# print(unique_fac_set)\n",
    "# print(refined_unique_fac_set)\n",
    "print(f\"Num Elements in UniqueFacSet: {len(unique_fac_set)}\")\n",
    "print(f\"Num Elements in RefinedFacSet: {len(refined_unique_fac_set)}\")\n",
    "\n",
    "new_set: set[str] = unique_fac_set - refined_unique_fac_set\n",
    "# print(f\"Num Elements in NewSet: {len(new_set)}\")\n",
    "\n",
    "new_set_list: list[str] = list(new_set)\n",
    "\n",
    "last_names = {name.split(\", \")[0] for name in new_set}\n",
    "\n",
    "refined_unique_fac_list = list(refined_unique_fac_set)\n",
    "\n",
    "print(\"\\n\")\n",
    "best_matches: list[str] = []\n",
    "for last_name in last_names:\n",
    "    best_match, score = process.extractOne(last_name, refined_unique_fac_list)\n",
    "    best_matches.append(best_match)\n",
    "\n",
    "best_matches.sort()\n",
    "new_set_list.sort()\n",
    "\n",
    "for i in range(len(max(best_matches, new_set_list))):\n",
    "    print(f\"{best_matches[i]}: {new_set_list[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in refined_unique_fac_list:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

            ```

            src/other/testing/wos_parser_testing.ipynb:
            ```
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"wos_records.xls\")\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "df.to_csv(\"wos_records_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the wos lite api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception when calling IntegrationApi->id_unique_id_get: (403)\n",
      "Reason: Forbidden\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 18 Mar 2024 16:51:27 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '49', 'Connection': 'keep-alive', 'vary': 'Origin', 'Correlation-Id': 'b0792985-d56f-427b-8852-5d0d21acef97#76367682', 'X-Kong-Response-Latency': '16'})\n",
      "HTTP response body: {\n",
      "  \"message\":\"You cannot consume this service\"\n",
      "}\n",
      "\\n\n",
      "Exception when calling SearchApi->root_get: (403)\n",
      "Reason: Forbidden\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 18 Mar 2024 16:51:27 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '49', 'Connection': 'keep-alive', 'vary': 'Origin', 'Correlation-Id': 'b0792985-d56f-427b-8852-5d0d21acef97#76367691', 'X-Kong-Response-Latency': '0'})\n",
      "HTTP response body: {\n",
      "  \"message\":\"You cannot consume this service\"\n",
      "}\n",
      "\\n\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import woslite_client\n",
    "from woslite_client.rest import ApiException\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure API key authorization: key\n",
    "configuration = woslite_client.Configuration()\n",
    "configuration.api_key[\"X-ApiKey\"] = \"https://api.clarivate.com/apis/wos-starter/v1\"\n",
    "\n",
    "# create an instance of the API class\n",
    "integration_api_instance = woslite_client.IntegrationApi(\n",
    "    woslite_client.ApiClient(configuration)\n",
    ")\n",
    "search_api_instance = woslite_client.SearchApi(woslite_client.ApiClient(configuration))\n",
    "database_id = \"WOS\"  # str | Database to search. Must be a valid database ID, one of the following: BCI/BIOABS/BIOSIS/CCC/DCI/DIIDW/MEDLINE/WOK/WOS/ZOOREC. WOK represents all databases.\n",
    "unique_id = \"WOS:000270372400005\"  # str | Primary item(s) id to be searched, ex: WOS:000270372400005. Cannot be null or an empty string. Multiple values are separated by comma.\n",
    "usr_query = \"TS=(cadmium)\"  # str | User query for requesting data, ex: TS=(cadmium). The query parser will return errors for invalid queries.\n",
    "count = 1  # int | Number of records returned in the request\n",
    "first_record = 1  # int | Specific record, if any within the result set to return. Cannot be less than 1 and greater than 100000.\n",
    "lang = \"en\"  # str | Language of search. This element can take only one value: en for English. If no language is specified, English is passed by default. (optional)\n",
    "sort_field = \"PY+D\"  # str | Order by field(s). Field name and order by clause separated by '+', use A for ASC and D for DESC, ex: PY+D. Multiple values are separated by comma. (optional)\n",
    "\n",
    "try:\n",
    "    # Find record(s) by specific id\n",
    "    api_response = integration_api_instance.id_unique_id_get(\n",
    "        database_id, unique_id, count, first_record, lang=lang, sort_field=sort_field\n",
    "    )\n",
    "    # for more details look at the models\n",
    "    firstAuthor = api_response.data[0].author.authors[0]\n",
    "    print(\"Response: \")\n",
    "    pprint(api_response)\n",
    "    pprint(\"First author: \" + firstAuthor)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling IntegrationApi->id_unique_id_get: %s\\\\n\" % e)\n",
    "\n",
    "try:\n",
    "    # Find record(s) by user query\n",
    "    api_response = search_api_instance.root_get(\n",
    "        database_id, usr_query, count, first_record, lang=lang, sort_field=sort_field\n",
    "    )\n",
    "    # for more details look at the models\n",
    "    firstAuthor = api_response.data[0].author.authors[0]\n",
    "    print(\"Response: \")\n",
    "    pprint(api_response)\n",
    "    pprint(\"First author: \" + firstAuthor)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling SearchApi->root_get: %s\\\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.clarivate.com/apis/wos-starter/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import clarivate.wos_starter.client\n",
    "from clarivate.wos_starter.client.rest import ApiException\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "configuration = clarivate.wos_starter.client.Configuration(\n",
    "    host=\"https://api.clarivate.com/apis/wos-starter/v1\"\n",
    ")\n",
    "configuration.api_key[\"ClarivateApiKeyAuth\"] = os.getenv(\"WOS_STARTER_API_KEY\")\n",
    "\n",
    "with clarivate.wos_starter.client.ApiClient(configuration) as api_client:\n",
    "    api_instance = clarivate.wos_starter.client.DocumentsApi(api_client)\n",
    "    q = \"OG=(Salisbury University) AND PY=2018-2023 AND DT=(Article)\"\n",
    "    db = \"WOS\"\n",
    "\n",
    "    try:\n",
    "        api_response = api_instance.documents_get(q, db=db)\n",
    "        response_dict = api_response.to_dict()\n",
    "        response_json = json.dumps(response_dict, indent=4)\n",
    "        with open(\"api_response.json\", \"w\") as f:\n",
    "            f.write(response_json)\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling DocumentsApi->documents_get: %s\\n %e\")\n",
    "    except AttributeError:\n",
    "        print(\"The response object cannot be converted to a dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wos_api_response.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"formatted_response.txt\", \"w\") as f:\n",
    "    for hit in data[\"hits\"]:\n",
    "        f.write(\"PT J\\n\")\n",
    "        authors = hit.get(\"names\", {}).get(\"authors\", [])\n",
    "        author_names = \", \".join(\n",
    "            [author.get(\"displayName\", \"Unknown author\") for author in authors]\n",
    "        )\n",
    "        f.write(f\"AU {author_names}\\n\")\n",
    "        title = hit.get(\"title\", \"No title available\")\n",
    "        f.write(f\"TI {title}\\n\")\n",
    "        # Assuming 'abstract' is directly under 'hit', adjust if it's nested differently\n",
    "        abstract = hit.get(\"abstract\", \"No abstract available\")\n",
    "        f.write(f\"AB {abstract}\\n\")\n",
    "        f.write(\"ER\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m api_response_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mDocumentsList(metadata=Metadata(page=1, limit=10, total=2569), hits=[Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1973R217300120\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNONLINEAR EIGENVALUES\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeeting Abstract\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOTICES OF THE AMERICAN MATHEMATICAL SOCIETY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1973, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA658-A658\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA658\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA658\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=1)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAY, EL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAY, EL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDFT-8752-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973R217300120&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=None, related=None), citations=[], identifiers=DocumentIdentifiers(doi=None, issn=None, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1973Q729400021\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROGRESS IN GROUP AND FAMILY THERAPY - SAGER,CJ AND KAPLAN,HS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBook Review\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPERSONNEL AND GUIDANCE JOURNAL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1973, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m52\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m64-65\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m65\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=2)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMASUCCI, MJ\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMASUCCI, MJ\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFLQ-5931-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973Q729400021&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973Q729400021&DestLinkType=CitedReferences&DestApp=WOS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, related=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973Q729400021&DestLinkType=RelatedRecords&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), citations=[], identifiers=DocumentIdentifiers(doi=None, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0031-5737\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1973P706700010\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOUND OF SILENTS - EARLY SHREW\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENGLISH JOURNAL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1973, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m62\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m754-&\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m754\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=0)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWELSH, JM\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWELSH, JM\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEDC-8023-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973P706700010&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973P706700010&DestLinkType=CitedReferences&DestApp=WOS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, related=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973P706700010&DestLinkType=RelatedRecords&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), citations=[], identifiers=DocumentIdentifiers(doi=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10.2307/814288\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0013-8274\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1973P011500004\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALGORITHM FOR COMPUTER REGISTRATION\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOLLEGE AND UNIVERSITY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1973, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m48\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m87-89\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m87\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m89\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=3)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKUNDELL, FA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKUNDELL, FA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFIW-7790-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973P011500004&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=None, related=None), citations=[], identifiers=DocumentIdentifiers(doi=None, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0010-0889\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1974T132600033\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREGIONAL DEVELOPMENT IN BRITAIN - MANNERS,G, KEEBLE,D, RODGERS,B AND WARREN,K\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBook Review\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROFESSIONAL GEOGRAPHER\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1974, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m26\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m232-232\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m232\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m232\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=1)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROSING, RA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROSING, RA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDPZ-8554-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974T132600033&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974T132600033&DestLinkType=CitedReferences&DestApp=WOS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, related=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974T132600033&DestLinkType=RelatedRecords&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), citations=[], identifiers=DocumentIdentifiers(doi=None, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0033-0124\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1974V479900005\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mATTITUDES ABOUT SEX-ROLES, SEX, AND MARITAL-STATUS OF ANTI-NIXON DEMONSTRATORS, COMPARISON OF 3 STUDIES\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPSYCHOLOGICAL REPORTS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1974, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m35\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1049-1050\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1049\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1050\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=2)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOESTING, J\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOESTING, J\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFBF-6821-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOESTING, R\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOESTING, R\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEZQ-1244-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V479900005&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V479900005&DestLinkType=CitedReferences&DestApp=WOS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, related=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V479900005&DestLinkType=RelatedRecords&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), citations=[], identifiers=DocumentIdentifiers(doi=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10.2466/pr0.1974.35.3.1049\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0033-2941\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1974V129000008\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLACK COLLEGE-STUDENTS RESPONSES TO BLACK AND WHITE PICTURES ON QUICK TEST\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNote\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPSYCHOLOGICAL REPORTS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1974, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m35\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m718-718\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m718\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m718\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=1)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOESTING, J\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOESTING, J\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFBF-6821-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOESTING, R\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOESTING, R\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEZQ-1244-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V129000008&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V129000008&DestLinkType=CitedReferences&DestApp=WOS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, related=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V129000008&DestLinkType=RelatedRecords&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), citations=[], identifiers=DocumentIdentifiers(doi=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10.2466/pr0.1974.35.2.718\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0033-2941\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1974U787900023\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVOCATIONAL EDUCATIONAL COUNSELING PRACTICES - SURVEY OF UNIVERSITY COUNSELING CENTERS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNote\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOURNAL OF COUNSELING PSYCHOLOGY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1974, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m21\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m579-580\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m579\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m580\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=2)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRAFF, RW\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRAFF, RW\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCRS-1663-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAQUE, D\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAQUE, D\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFRN-6600-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974U787900023&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974U787900023&DestLinkType=CitedReferences&DestApp=WOS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, related=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974U787900023&DestLinkType=RelatedRecords&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), citations=[], identifiers=DocumentIdentifiers(doi=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10.1037/h0037309\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0022-0167\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1975MA10200016\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSILENCES\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m - GOLUBOVIC,P\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilm Review\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLITERATURE-FILM QUARTERLY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1975, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m286-287\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m286\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m287\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=2)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWELSH, JM\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWELSH, JM\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEDC-8023-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1975MA10200016&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1975MA10200016&DestLinkType=CitedReferences&DestApp=WOS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, related=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1975MA10200016&DestLinkType=RelatedRecords&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), citations=[], identifiers=DocumentIdentifiers(doi=None, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0090-4260\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOS:A1975MG48600022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, title=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFOR WILLIAM CARLOS WILLIAMS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArt and Literature\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source_types=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoetry\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], source=DocumentSource(source_title=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOUTHERN HUMANITIES REVIEW\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, publish_year=1975, publish_month=None, volume=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, issue=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m90-90\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, begin=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m90\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, end=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m90\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, count=1)), names=DocumentNames(authors=[AuthorName(display_name=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTONGUE, M\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, wos_standard=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTONGUE, M\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, researcher_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGFI-5936-2022\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1975MG48600022&DestLinkType=FullRecord&DestApp=WOS_CPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, citing_articles=None, references=None, related=None), citations=[], identifiers=DocumentIdentifiers(doi=None, issn=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0038-4186\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[]))])}\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m----> 4\u001b[0m api_new_response \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_response_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m api_new_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      7\u001b[0m     title \u001b[38;5;241m=\u001b[39m doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "api_response_string = \"\"\"{DocumentsList(metadata=Metadata(page=1, limit=10, total=2569), hits=[Document(uid='WOS:A1973R217300120', title='NONLINEAR EIGENVALUES', types=['Abstract'], source_types=['Meeting Abstract'], source=DocumentSource(source_title='NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY', publish_year=1973, publish_month=None, volume='20', issue='7', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='A658-A658', begin='A658', end='A658', count=1)), names=DocumentNames(authors=[AuthorName(display_name='MAY, EL', wos_standard='MAY, EL', researcher_id='DFT-8752-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973R217300120&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references=None, related=None), citations=[], identifiers=DocumentIdentifiers(doi=None, issn=None, eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1973Q729400021', title='PROGRESS IN GROUP AND FAMILY THERAPY - SAGER,CJ AND KAPLAN,HS', types=['Review'], source_types=['Book Review'], source=DocumentSource(source_title='PERSONNEL AND GUIDANCE JOURNAL', publish_year=1973, publish_month=None, volume='52', issue='1', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='64-65', begin='64', end='65', count=2)), names=DocumentNames(authors=[AuthorName(display_name='MASUCCI, MJ', wos_standard='MASUCCI, MJ', researcher_id='FLQ-5931-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973Q729400021&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973Q729400021&DestLinkType=CitedReferences&DestApp=WOS', related='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973Q729400021&DestLinkType=RelatedRecords&DestApp=WOS_CPL'), citations=[], identifiers=DocumentIdentifiers(doi=None, issn='0031-5737', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1973P706700010', title='SOUND OF SILENTS - EARLY SHREW', types=['Article'], source_types=['Article'], source=DocumentSource(source_title='ENGLISH JOURNAL', publish_year=1973, publish_month=None, volume='62', issue='5', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='754-&', begin='754', end='&', count=0)), names=DocumentNames(authors=[AuthorName(display_name='WELSH, JM', wos_standard='WELSH, JM', researcher_id='EDC-8023-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973P706700010&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973P706700010&DestLinkType=CitedReferences&DestApp=WOS', related='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973P706700010&DestLinkType=RelatedRecords&DestApp=WOS_CPL'), citations=[], identifiers=DocumentIdentifiers(doi='10.2307/814288', issn='0013-8274', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1973P011500004', title='ALGORITHM FOR COMPUTER REGISTRATION', types=['Article'], source_types=['Article'], source=DocumentSource(source_title='COLLEGE AND UNIVERSITY', publish_year=1973, publish_month=None, volume='48', issue='2', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='87-89', begin='87', end='89', count=3)), names=DocumentNames(authors=[AuthorName(display_name='KUNDELL, FA', wos_standard='KUNDELL, FA', researcher_id='FIW-7790-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1973P011500004&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references=None, related=None), citations=[], identifiers=DocumentIdentifiers(doi=None, issn='0010-0889', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1974T132600033', title='REGIONAL DEVELOPMENT IN BRITAIN - MANNERS,G, KEEBLE,D, RODGERS,B AND WARREN,K', types=['Review'], source_types=['Book Review'], source=DocumentSource(source_title='PROFESSIONAL GEOGRAPHER', publish_year=1974, publish_month=None, volume='26', issue='2', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='232-232', begin='232', end='232', count=1)), names=DocumentNames(authors=[AuthorName(display_name='ROSING, RA', wos_standard='ROSING, RA', researcher_id='DPZ-8554-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974T132600033&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974T132600033&DestLinkType=CitedReferences&DestApp=WOS', related='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974T132600033&DestLinkType=RelatedRecords&DestApp=WOS_CPL'), citations=[], identifiers=DocumentIdentifiers(doi=None, issn='0033-0124', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1974V479900005', title='ATTITUDES ABOUT SEX-ROLES, SEX, AND MARITAL-STATUS OF ANTI-NIXON DEMONSTRATORS, COMPARISON OF 3 STUDIES', types=['Article'], source_types=['Article'], source=DocumentSource(source_title='PSYCHOLOGICAL REPORTS', publish_year=1974, publish_month=None, volume='35', issue='3', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='1049-1050', begin='1049', end='1050', count=2)), names=DocumentNames(authors=[AuthorName(display_name='JOESTING, J', wos_standard='JOESTING, J', researcher_id='FBF-6821-2022'), AuthorName(display_name='JOESTING, R', wos_standard='JOESTING, R', researcher_id='EZQ-1244-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V479900005&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V479900005&DestLinkType=CitedReferences&DestApp=WOS', related='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V479900005&DestLinkType=RelatedRecords&DestApp=WOS_CPL'), citations=[], identifiers=DocumentIdentifiers(doi='10.2466/pr0.1974.35.3.1049', issn='0033-2941', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1974V129000008', title='BLACK COLLEGE-STUDENTS RESPONSES TO BLACK AND WHITE PICTURES ON QUICK TEST', types=['Article'], source_types=['Note'], source=DocumentSource(source_title='PSYCHOLOGICAL REPORTS', publish_year=1974, publish_month=None, volume='35', issue='2', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='718-718', begin='718', end='718', count=1)), names=DocumentNames(authors=[AuthorName(display_name='JOESTING, J', wos_standard='JOESTING, J', researcher_id='FBF-6821-2022'), AuthorName(display_name='JOESTING, R', wos_standard='JOESTING, R', researcher_id='EZQ-1244-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V129000008&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V129000008&DestLinkType=CitedReferences&DestApp=WOS', related='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974V129000008&DestLinkType=RelatedRecords&DestApp=WOS_CPL'), citations=[], identifiers=DocumentIdentifiers(doi='10.2466/pr0.1974.35.2.718', issn='0033-2941', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1974U787900023', title='VOCATIONAL EDUCATIONAL COUNSELING PRACTICES - SURVEY OF UNIVERSITY COUNSELING CENTERS', types=['Article'], source_types=['Note'], source=DocumentSource(source_title='JOURNAL OF COUNSELING PSYCHOLOGY', publish_year=1974, publish_month=None, volume='21', issue='6', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='579-580', begin='579', end='580', count=2)), names=DocumentNames(authors=[AuthorName(display_name='GRAFF, RW', wos_standard='GRAFF, RW', researcher_id='CRS-1663-2022'), AuthorName(display_name='RAQUE, D', wos_standard='RAQUE, D', researcher_id='FRN-6600-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974U787900023&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974U787900023&DestLinkType=CitedReferences&DestApp=WOS', related='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1974U787900023&DestLinkType=RelatedRecords&DestApp=WOS_CPL'), citations=[], identifiers=DocumentIdentifiers(doi='10.1037/h0037309', issn='0022-0167', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1975MA10200016', title=\"'SILENCES' - GOLUBOVIC,P\", types=['Review'], source_types=['Film Review'], source=DocumentSource(source_title='LITERATURE-FILM QUARTERLY', publish_year=1975, publish_month=None, volume='3', issue='3', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='286-287', begin='286', end='287', count=2)), names=DocumentNames(authors=[AuthorName(display_name='WELSH, JM', wos_standard='WELSH, JM', researcher_id='EDC-8023-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1975MA10200016&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1975MA10200016&DestLinkType=CitedReferences&DestApp=WOS', related='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1975MA10200016&DestLinkType=RelatedRecords&DestApp=WOS_CPL'), citations=[], identifiers=DocumentIdentifiers(doi=None, issn='0090-4260', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[])), Document(uid='WOS:A1975MG48600022', title=\"'FOR WILLIAM CARLOS WILLIAMS'\", types=['Art and Literature'], source_types=['Poetry'], source=DocumentSource(source_title='SOUTHERN HUMANITIES REVIEW', publish_year=1975, publish_month=None, volume='9', issue='1', supplement=None, special_issue=None, article_number=None, pages=DocumentSourcePages(range='90-90', begin='90', end='90', count=1)), names=DocumentNames(authors=[AuthorName(display_name='TONGUE, M', wos_standard='TONGUE, M', researcher_id='GFI-5936-2022')], inventors=None, book_corp=None, book_editors=None, books=None, additional_authors=None, anonymous=None, assignees=None, corp=None, editors=None, investigators=None, sponsors=None, issuing_organizations=None), links=DocumentLinks(record='https://www.webofscience.com/api/gateway?GWVersion=2&SrcApp=salisbury_university_research_data&SrcAuth=WosAPI&KeyUT=WOS:A1975MG48600022&DestLinkType=FullRecord&DestApp=WOS_CPL', citing_articles=None, references=None, related=None), citations=[], identifiers=DocumentIdentifiers(doi=None, issn='0038-4186', eissn=None, isbn=None, eisbn=None, pmid=None), keywords=DocumentKeywords(author_keywords=[]))])}\"\"\"\n",
    "\n",
    "api_new_response = json.loads(api_response_string)\n",
    "\n",
    "for doc in api_new_response[\"hits\"]:\n",
    "    title = doc[\"title\"]\n",
    "    authors = \", \".join([author[\"display_name\"] for author in doc[\"names\"][\"authors\"]])\n",
    "    print(f\"Title: {title}\\nAuthors: {authors}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

            ```

            src/other/testing_data/__init__.py:
            ```

            ```

            src/other/testing_data/abstracts.py:
            ```
from pydantic import BaseModel
from typing import Dict


class Abstract(BaseModel):
    abstract: str
    doi: str


# Key = DOI, Value = Abstract
doi_to_abstract_dict: Dict[str, str] = {}

# this is in the data
abstract_1 = Abstract(
    abstract="Drawing on expectation states theory and expertise utilization literature, we examine the effects of team members' actual expertise and social status on the degree of influence they exert over team processes via perceived expertise. We also explore the conditions under which teams rely on perceived expertise versus social status in determining influence relationships in teams. To do so, we present a contingency model in which the salience of expertise and social status depends on the types of intragroup conflicts. Using multiwave survey data from 50 student project teams with 320 members at a large national research institute located in South Korea, we found that both actual expertise and social status had direct and indirect effects on member influence through perceived expertise. Furthermore, perceived expertise at the early stage of team projects is driven by social status, whereas perceived expertise at the later stage of a team project is mainly driven by actual expertise. Finally, we found that members who are being perceived as experts are more influential when task conflict is high or when relationship conflict is low. We discuss the implications of these findings for research and practice.",
    doi="10.1177/1059601117728145",
)
doi_to_abstract_dict[abstract_1.doi] = abstract_1.abstract

# # this is not in the data
# abstract_2 = Abstract(
#     abstract="The goal of this paper is to investigate how deregulating foreign equity ownership influences a firm's innovation investment. We attempt to answer this question using data from 530 Korean manufacturing firms between 1998 and 2003 through generalised estimating equations. Our findings suggest that foreign ownership and R&D investment exhibit an inverted U-shaped relationship because the incentives to monitor managers' decision-making processes initially play a greater role, but this role stagnates as the share owned by foreign investors becomes concentrated. In addition, we consider firm heterogeneity and observe the negative moderation effects of firm age and the positive moderation effects of growth opportunity.",
#     doi="10.1080/09537325.2021.1991572"
# )
# doi_to_abstract_dict[abstract_2.doi] = abstract_2.abstract

# # this is in the data
# abstract_3 = Abstract(
#     abstract="This study examined the role of perceived organizational commitment on managers' assessments of employees' career growth opportunities. Based on a paired sample of 161 legal secretaries and their managers, results indicated that managers used the attitudes and behaviors displayed by employees (strong extra-role performance and enhanced work engagement) as cues from which to base their perceptions of employees' affective commitment to the organization. In turn, employees perceived as highly committed to the organization experienced enhanced content and structural career growth opportunities. Moreover, the relation between managers' perceptions of employees' organizational commitment and content career growth opportunities was stronger for employees perceived as also highly committed to their careers than for employees perceived as less committed to their careers.",
#     doi="10.1177/0894845317714892"
# )
# doi_to_abstract_dict[abstract_3.doi] = abstract_3.abstract

# # this is in the data
# abstract_4 = Abstract(
#     abstract="This study examined how firms combine alliances and acquisitions in an exploration/exploitation framework. By conducting cluster analysis on a sample of 1270 acquisitions made by 836 firms, we first identified the patterns in alliance and acquisition activities undertaken by these firms. Five distinct patterns were identified: (I) low alliance-low acquisition, (II) low alliance-high acquisition, (III) high alliance-low acquisition, (IV) high alliance-high acquisition, and (V) medium alliance-very high acquisition. Next, we analyzed the different ways in which the two modes were interlinked within these five patterns for exploration/exploitation. Patterns III and IV appeared to involve both exploration/exploitation and mutually reinforce exploration/exploitation. In contrast, in the remaining patterns, the two modes appeared to be more loosely coupled with each other, with a focus on exploitation.",
#     doi="10.1177/03063070221129452"
# )
# doi_to_abstract_dict[abstract_4.doi] = abstract_4.abstract

            ```

            src/other/testing_data/load_pkls.py:
            ```
import os
import pickle

THIS_DIRECTORY = os.path.dirname(os.path.abspath(__file__))

METHOD_DIRECTORY = os.path.join(THIS_DIRECTORY, "method_extraction")
SENTENCE_DIRECTORY = os.path.join(THIS_DIRECTORY, "sentence_analysis")
SUMMARY_DIRECTORY = os.path.join(THIS_DIRECTORY, "summary")

METHOD_PKL_FILE_NAME = "method_extraction.pkl"
SENTENCE_PKL_FILE_NAME = "sentence_analysis.pkl"
SUMMARY_PKL_FILE_NAME = "summary.pkl"

METHOD_PKL_FILE_PATH = os.path.join(METHOD_DIRECTORY, METHOD_PKL_FILE_NAME)
SENTENCE_PKL_FILE_PATH = os.path.join(SENTENCE_DIRECTORY, SENTENCE_PKL_FILE_NAME)
SUMMARY_PKL_FILE_PATH = os.path.join(SUMMARY_DIRECTORY, SUMMARY_PKL_FILE_NAME)


def load_pkls():
    with open(METHOD_PKL_FILE_PATH, "rb") as f:
        method_extraction_list = pickle.load(f)
    with open(SENTENCE_PKL_FILE_PATH, "rb") as f:
        sentence_analysis_list = pickle.load(f)
    with open(SUMMARY_PKL_FILE_PATH, "rb") as f:
        summary_list = pickle.load(f)
    return method_extraction_list, sentence_analysis_list, summary_list


if __name__ == "__main__":
    method_extraction_list, sentence_analysis_list, summary_list = load_pkls()
    print(method_extraction_list)
    print(sentence_analysis_list)
    print(summary_list)

            ```

            src/other/testing_data/make_pkls.py:
            ```
import os
import json
import pickle
from typing import List
import logging


def get_directories() -> List[str]:
    logger.info("Getting directories")
    THIS_DIRECTORY = os.path.dirname(os.path.abspath(__file__))
    logger.info(f"THIS_DIRECTORY: {THIS_DIRECTORY}")
    METHOD_EXTRACTION_DIRECTORY = os.path.join(THIS_DIRECTORY, "method_extraction")
    logger.info(f"METHOD_EXTRACTION_DIRECTORY: {METHOD_EXTRACTION_DIRECTORY}")
    SENTENCE_ANALYSIS_DIRECTORY = os.path.join(THIS_DIRECTORY, "sentence_analysis")
    logger.info(f"SENTENCE_ANALYSIS_DIRECTORY: {SENTENCE_ANALYSIS_DIRECTORY}")
    SUMMARY_DIRECTORY = os.path.join(THIS_DIRECTORY, "summary")
    logger.info(f"SUMMARY_DIRECTORY: {SUMMARY_DIRECTORY}")
    logger.info("Returning directories")
    return (METHOD_EXTRACTION_DIRECTORY, SENTENCE_ANALYSIS_DIRECTORY, SUMMARY_DIRECTORY)


def get_files_from_directory(directory_path: str) -> List[str]:
    logger.info(f"Getting files from {directory_path}")
    files_list = [
        f
        for f in os.listdir(directory_path)
        if os.path.isfile(os.path.join(directory_path, f))
    ]
    logger.info(f"Got {len(files_list)} files from {directory_path}")
    return files_list


def load_json_file(file_path: str) -> dict:
    logger.info(f"Loading json file from {file_path}")
    with open(file_path, "r") as f:
        logger.info(f"Loaded json file from {file_path}")
        logger.info(f"Returning json file")
        return json.load(f)


def get_list_of_dicts(directory_path: str, files_list: List[str]) -> List[dict]:
    logger.info(f"Getting list of dicts from {files_list}")
    list_of_dicts = [
        load_json_file(os.path.join(directory_path, file_path))
        for file_path in files_list
    ]
    logger.info(f"Got {len(list_of_dicts)} list of dicts")
    return list_of_dicts


def save_list_of_dicts(
    logger: logging.Logger, list_of_dicts: List[dict], save_path: str
) -> None:
    logger.info(f"Saving list of dicts to {save_path}")
    logger.info(f"List of dicts: {list_of_dicts}")
    with open(save_path, "wb") as f:
        logger.info(f"Dumping list of dicts to {save_path}")
        pickle.dump(list_of_dicts, f)
    logger.info(f"Saved list of dicts to {save_path}")


def make_pkl(logger: logging.Logger, directory_path: str, pkl_file_name: str) -> None:
    files_list = get_files_from_directory(directory_path)
    list_of_dicts = get_list_of_dicts(directory_path, files_list)
    save_path = os.path.join(directory_path, pkl_file_name)
    save_list_of_dicts(logger, list_of_dicts, save_path)


def get_method_extraction_file_list(
    logger: logging.Logger, method_extraction_directory: str
) -> List[str]:
    logger.info(
        f"Getting method extraction file list from {method_extraction_directory}"
    )
    return get_files_from_directory(method_extraction_directory)


def get_sentence_analysis_file_list(
    logger: logging.Logger, sentence_analysis_directory: str
) -> List[str]:
    logger.info(
        f"Getting sentence analysis file list from {sentence_analysis_directory}"
    )
    return get_files_from_directory(sentence_analysis_directory)


def get_summary_file_list(logger: logging.Logger, summary_directory: str) -> List[str]:
    logger.info(f"Getting summary file list from {summary_directory}")
    return get_files_from_directory(summary_directory)


def set_up_logging() -> None:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    return logger


def set_log_to_console(log_to_console: bool) -> None:
    LOG_TO_CONSOLE = log_to_console
    logger.addHandler(logging.StreamHandler()) if LOG_TO_CONSOLE else None


if __name__ == "__main__":
    logger = set_up_logging()
    logger.info("Set up logging")
    set_log_to_console(True)
    logger.info("Set LOG_TO_CONSOLE True")
    logger.info("Getting directories")
    (
        method_extraction_directory,
        sentence_analysis_directory,
        summary_directory,
    ) = get_directories()
    logger.info(
        f"Got directories: {method_extraction_directory}, {sentence_analysis_directory}, {summary_directory}"
    )
    logger.info("Getting method extraction file list")
    method_extraction_file_list = get_method_extraction_file_list(
        logger, method_extraction_directory
    )
    logger.info(f"Got method extraction file list: {method_extraction_file_list}")
    logger.info("Getting sentence analysis file list")
    sentence_analysis_file_list = get_sentence_analysis_file_list(
        logger, sentence_analysis_directory
    )
    logger.info(f"Got sentence analysis file list: {sentence_analysis_file_list}")
    logger.info("Getting summary file list")
    summary_file_list = get_summary_file_list(logger, summary_directory)
    logger.info(f"Got summary file list: {summary_file_list}")
    logger.info("Making all pkls")
    make_pkl(logger, method_extraction_directory, "method_extraction.pkl")
    logger.info("Made method extraction pkl")
    make_pkl(logger, sentence_analysis_directory, "sentence_analysis.pkl")
    logger.info("Made sentence analysis pkl")
    make_pkl(logger, summary_directory, "summary.pkl")
    logger.info("Made summary pkl")
    logger.info("Made all pkls")

            ```

        src/other/to_csv.py:
        ```
import os
import pandas as pd
import json


def load_json(file_path):
    with open(file_path, "r") as file:
        return json.load(file)


def create_general_info_sheet(data, writer):
    general_info = {
        "Category": [],
        "URL": [],
        "Faculty Count": [],
        "Department Count": [],
        "Article Count": [],
        "Total Citations": [],
        "Citation Average": [],
    }

    for category, category_info in data.items():
        general_info["Category"].append(category)
        general_info["URL"].append(
            "https://cosc425-site.vercel.app/categories/category/"
            + category_info["url"]
        )
        general_info["Faculty Count"].append(category_info["faculty_count"])
        general_info["Department Count"].append(category_info["department_count"])
        general_info["Article Count"].append(category_info["article_count"])
        general_info["Total Citations"].append(category_info["tc_count"])
        general_info["Citation Average"].append(category_info["citation_average"])

    df = pd.DataFrame(general_info)
    print("General Info DataFrame:")
    print(df.head())
    print("Shape:", df.shape)
    df.to_excel(writer, sheet_name="General Info", index=False)


def create_list_sheet(data, writer, sheet_name, list_key):
    expanded_list = {
        "Category": [],
        "item": [],
    }

    for category, details in data.items():
        if list_key in details:
            for item in details[list_key]:
                expanded_list["Category"].append(category)
                expanded_list["item"].append(item)

    df = pd.DataFrame(expanded_list)
    print(f"{sheet_name} DataFrame:")
    print(df.head())
    print("Shape:", df.shape)
    df.to_excel(writer, sheet_name=sheet_name, index=False)


def create_faculty_stats_sheet(data, writer, sheet_name="Faculty Stats"):
    all_data = []
    for category, faculty_stats in data.items():
        if "faculty_stats" in faculty_stats:
            for faculty, details in faculty_stats["faculty_stats"].items():
                if (
                    "citation_map" in details
                    and "article_citation_map" in details["citation_map"]
                ):
                    for article, citation_count in details["citation_map"][
                        "article_citation_map"
                    ].items():
                        all_data.append(
                            {
                                "Category": category,
                                "Faculty Name": faculty,
                                "Article Title": article,
                                "Total Citations": details.get("total_citations", 0),
                                "Article Count": details.get("article_count", 0),
                                "Average Citations": details.get(
                                    "average_citations", 0
                                ),
                                "Citations per Article": citation_count,
                            }
                        )
    df = pd.DataFrame(all_data)
    print(df.head())

    if not df.empty:
        df.to_excel(writer, sheet_name=sheet_name, index=False)
    else:
        print("DataFrame is empty, nothing to write to Excel.")


def create_article_stats_sheet(data, writer, sheet_name="Article Stats"):
    all_data = []
    for category, details in data.items():
        if "article_citation_map" in details:
            for article, citations in details["article_citation_map"].items():
                all_data.append(
                    {
                        "Category": category,
                        "Article Title": article,
                        "Citations": citations,
                    }
                )
    df = pd.DataFrame(all_data)
    print(df.head())
    if not df.empty:
        df.to_excel(writer, sheet_name=sheet_name, index=False)
    else:
        print("DataFrame is empty, nothing to write to Excel.")


def main():
    file_path = "../../TextAnalysis/output_data.json"
    data = load_json(file_path)
    faculty_file_path = "processed_faculty_stats_data.json"
    article_file_path = "processed_article_stats_data.json"

    faculty_data = load_json(faculty_file_path)
    article_data = load_json(article_file_path)

    output_file = "SU_Category_Data_Themes.xlsx"

    # Context manager to handle Excel writer
    with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
        create_general_info_sheet(data, writer)
        create_list_sheet(data, writer, "Faculty Members", "faculty")
        create_list_sheet(data, writer, "Departments", "departments")
        create_list_sheet(data, writer, "Article Titles", "titles")
        create_list_sheet(data, writer, "Research Themes", "themes")
        create_faculty_stats_sheet(faculty_data, writer, sheet_name="Faculty Stats")
        create_article_stats_sheet(article_data, writer, sheet_name="Article Stats")

    # Check file size to confirm data was written
    file_size = os.path.getsize(output_file)
    print(f"Generated file size: {file_size} bytes")


if __name__ == "__main__":
    main()

        ```

        src/other/vis2.py:
        ```
import sys
import pandas as pd
import numpy as np
import networkx as nx
from PyQt5 import QtWidgets
import pyqtgraph as pg
from pyqtgraph.Qt import QtCore, QtGui
from tqdm import tqdm

# Load data
print("Loading data...")
articles_df = pd.read_json("test_processed_article_stats_data.json")
print("Data loaded.")


class CollaborationNetworkApp(QtWidgets.QMainWindow):
    def __init__(self, threshold):
        super().__init__()
        self.setWindowTitle("Collaboration Network")
        self.setGeometry(100, 100, 800, 600)

        # Create a graph
        self.graph = nx.Graph()
        self.edge_info = {}
        self.node_info = {}

        print("Building graph...")
        # Build the graph and collect edge information
        sample_articles = list(articles_df["math"]["article_citation_map"].items())[
            :20
        ]  # Take only the first 20 articles
        for article, details in sample_articles:
            faculty_members = details["faculty_members"]
            if len(faculty_members) > threshold:
                for i in range(len(faculty_members)):
                    for j in range(i + 1, len(faculty_members)):
                        edge = tuple(sorted((faculty_members[i], faculty_members[j])))
                        self.graph.add_edge(*edge)
                        if edge not in self.edge_info:
                            self.edge_info[edge] = []
                        self.edge_info[edge].append(article)
                for member in faculty_members:
                    if member not in self.node_info:
                        self.node_info[member] = []
                    self.node_info[member].append(article)
        print(
            "Graph built with",
            len(self.graph.nodes),
            "nodes and",
            len(self.graph.edges),
            "edges.",
        )

        # Create a plot widget
        self.plot_widget = pg.PlotWidget()
        self.setCentralWidget(self.plot_widget)

        # Draw the network
        self.draw_network()

        # Connect mouse press event
        self.plot_widget.scene().sigMouseClicked.connect(self.on_mouse_click)

    def draw_network(self):
        print("Calculating layout...")
        pos = nx.spring_layout(self.graph, k=0.1, iterations=10)
        self.node_positions = pos
        print("Layout calculated.")

        # Draw edges with progress bar
        print("Drawing edges...")
        total_edges = len(self.graph.edges())
        with tqdm(total=total_edges, desc="Drawing edges", unit="edge") as pbar:
            for idx, edge in enumerate(self.graph.edges(), start=1):
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                line = pg.PlotDataItem(
                    [x0, x1], [y0, y1], pen=pg.mkPen("gray", width=1)
                )
                self.plot_widget.addItem(line)
                pbar.update(1)
        print("Edges drawn.")

        # Draw nodes
        print("Drawing nodes...")
        self.node_items = {}
        node_positions = np.array(list(pos.values()))
        node_item = pg.ScatterPlotItem(
            node_positions[:, 0],
            node_positions[:, 1],
            size=10,
            brush=pg.mkBrush(255, 0, 0, 120),
        )
        self.plot_widget.addItem(node_item)
        for node, (x, y) in pos.items():
            self.node_items[node] = (x, y)
        print("Nodes drawn.")

    def on_mouse_click(self, event):
        pos = event.scenePos()
        mouse_point = self.plot_widget.plotItem.vb.mapSceneToView(pos)
        x, y = mouse_point.x(), mouse_point.y()

        # Check if a node is clicked
        for node, (nx, ny) in self.node_items.items():
            if np.hypot(x - nx, y - ny) < 0.1:
                self.show_node_info(node)
                return

        # Check if an edge is clicked
        for edge in self.graph.edges():
            x0, y0 = self.node_positions[edge[0]]
            x1, y1 = self.node_positions[edge[1]]
            if self.is_point_near_line(x, y, x0, y0, x1, y1):
                self.show_edge_info(edge)
                return

    def is_point_near_line(self, px, py, x0, y0, x1, y1, tol=0.1):
        line_vec = np.array([x1 - x0, y1 - y0])
        point_vec = np.array([px - x0, py - y0])
        line_len = np.linalg.norm(line_vec)
        if line_len < 1e-5:
            return False
        line_unitvec = line_vec / line_len
        proj_length = np.dot(point_vec, line_unitvec)
        if proj_length < 0 or proj_length > line_len:
            return False
        nearest_point = np.array([x0, y0]) + proj_length * line_unitvec
        dist = np.linalg.norm(np.array([px, py]) - nearest_point)
        return dist < tol

    def show_node_info(self, node):
        articles = ", ".join(self.node_info[node])
        QtWidgets.QMessageBox.information(
            self,
            "Node Info",
            f"Node: {node}\nCollaborations: {self.graph.degree[node]}\nArticles: {articles}",
        )

    def show_edge_info(self, edge):
        articles = ", ".join(self.edge_info[edge])
        QtWidgets.QMessageBox.information(
            self,
            "Edge Info",
            f"Collaboration between {edge[0]} and {edge[1]}\nArticles: {articles}",
        )


def main():
    threshold = 10  # Adjust this based on your analysis
    app = QtWidgets.QApplication(sys.argv)
    main_window = CollaborationNetworkApp(threshold)
    main_window.show()
    sys.exit(app.exec_())


if __name__ == "__main__":
    main()

        ```

        src/tests/__init__.py:
        ```
from .other import *

        ```
