# GitHub - SpencerPresley/COSC425-DATA: AI-powered toolkit for analyzing and classifying academic research publications using LLMs and automated data collection. Output options: Mongodb database via providing your databse url. Json. Excel spreadsheet. See README for the quick setup, see documentation for implementation details:

## AI-powered toolkit for analyzing and classifying academic research publications using LLMs and automated data collection. Output options: Mongodb database via providing your databse url. Json. Excel spreadsheet. See README for the quick setup, see documentation for implementation details: - SpencerPresley/COSC425-DATA

[Skip to content](https://github.com/spencerpresley/COSC425-DATA#start-of-content)

You signed in with another tab or window. [Reload](https://github.com/spencerpresley/COSC425-DATA) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/spencerpresley/COSC425-DATA) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/spencerpresley/COSC425-DATA) to refresh your session.Dismiss alert

[SpencerPresley](https://github.com/SpencerPresley)/ **[COSC425-DATA](https://github.com/SpencerPresley/COSC425-DATA)** Public

- [Notifications](https://github.com/login?return_to=%2FSpencerPresley%2FCOSC425-DATA) You must be signed in to change notification settings
- [Fork\\
3](https://github.com/login?return_to=%2FSpencerPresley%2FCOSC425-DATA)
- [Star\\
2](https://github.com/login?return_to=%2FSpencerPresley%2FCOSC425-DATA)


AI-powered toolkit for analyzing and classifying academic research publications using LLMs and automated data collection. Output options: Mongodb database via providing your databse url. Json. Excel spreadsheet. See README for the quick setup, see documentation for implementation details:


[cosc425-data.readthedocs.io/](https://cosc425-data.readthedocs.io/ "https://cosc425-data.readthedocs.io/")

[2\\
stars](https://github.com/SpencerPresley/COSC425-DATA/stargazers) [3\\
forks](https://github.com/SpencerPresley/COSC425-DATA/forks) [Branches](https://github.com/SpencerPresley/COSC425-DATA/branches) [Tags](https://github.com/SpencerPresley/COSC425-DATA/tags) [Activity](https://github.com/SpencerPresley/COSC425-DATA/activity)

[Star](https://github.com/login?return_to=%2FSpencerPresley%2FCOSC425-DATA)

[Notifications](https://github.com/login?return_to=%2FSpencerPresley%2FCOSC425-DATA) You must be signed in to change notification settings

# SpencerPresley/COSC425-DATA

main

[**7** Branches](https://github.com/SpencerPresley/COSC425-DATA/branches) [**25** Tags](https://github.com/SpencerPresley/COSC425-DATA/tags)

[Go to Branches page](https://github.com/SpencerPresley/COSC425-DATA/branches)[Go to Tags page](https://github.com/SpencerPresley/COSC425-DATA/tags)

Go to file

Code

## Folders and files

| Name | Name | Last commit message | Last commit date |
| --- | --- | --- | --- |
| ## Latest commit<br>[![SpencerPresley](https://avatars.githubusercontent.com/u/153392395?v=4&size=40)](https://github.com/SpencerPresley)[SpencerPresley](https://github.com/SpencerPresley/COSC425-DATA/commits?author=SpencerPresley)<br>[more added to mongodb guide](https://github.com/SpencerPresley/COSC425-DATA/commit/a7c464cffc37857e7a2cd41bfdccecb8ec10a5b4)<br>Jan 2, 2025<br>[a7c464c](https://github.com/SpencerPresley/COSC425-DATA/commit/a7c464cffc37857e7a2cd41bfdccecb8ec10a5b4)Â Â·Â Jan 2, 2025<br>## History<br>[248 Commits](https://github.com/SpencerPresley/COSC425-DATA/commits/main/) |
| [.github/workflows](https://github.com/SpencerPresley/COSC425-DATA/tree/main/.github/workflows "This path skips through empty directories") | [.github/workflows](https://github.com/SpencerPresley/COSC425-DATA/tree/main/.github/workflows "This path skips through empty directories") | [setting up readthedocs](https://github.com/SpencerPresley/COSC425-DATA/commit/379ffb592737c59b034da25a976a22476951d4b6 "setting up readthedocs") | Nov 24, 2024 |
| [additional\_information](https://github.com/SpencerPresley/COSC425-DATA/tree/main/additional_information "additional_information") | [additional\_information](https://github.com/SpencerPresley/COSC425-DATA/tree/main/additional_information "additional_information") | [more added to mongodb guide](https://github.com/SpencerPresley/COSC425-DATA/commit/a7c464cffc37857e7a2cd41bfdccecb8ec10a5b4 "more added to mongodb guide") | Jan 2, 2025 |
| [docs](https://github.com/SpencerPresley/COSC425-DATA/tree/main/docs "docs") | [docs](https://github.com/SpencerPresley/COSC425-DATA/tree/main/docs "docs") | [updates to pipeline to handle creating the excel via CLI args, mega uâ€¦](https://github.com/SpencerPresley/COSC425-DATA/commit/991396ba2a53eca5b948400c3d3866254ee518b0 "updates to pipeline to handle creating the excel via CLI args, mega update to the readme, pandas added as a depdency in pyproject.toml") | Dec 18, 2024 |
| [src](https://github.com/SpencerPresley/COSC425-DATA/tree/main/src "src") | [src](https://github.com/SpencerPresley/COSC425-DATA/tree/main/src "src") | [README.md overhaul, additional markdown files added for future purposâ€¦](https://github.com/SpencerPresley/COSC425-DATA/commit/eb7baafe2ceb2467e403c56ffd56438031c42fce "README.md overhaul, additional markdown files added for future purposes. All instances of mongodb_url or mongo_url changed to mongodb_uri / mongo_uri.") | Jan 1, 2025 |
| [.dockerignore](https://github.com/SpencerPresley/COSC425-DATA/blob/main/.dockerignore ".dockerignore") | [.dockerignore](https://github.com/SpencerPresley/COSC425-DATA/blob/main/.dockerignore ".dockerignore") | [docker set up. requirements.txt added. scripts for digital measures vâ€¦](https://github.com/SpencerPresley/COSC425-DATA/commit/b2a1a35c7139b24972c73690a9eb5c2dfd19be0a "docker set up. requirements.txt added. scripts for digital measures verification. script to find dependencies and add them to install_packages.sh script to install into a conda environment. convert script to convert pip requirements to conda if want to use conda. couple other files,") | Sep 4, 2024 |
| [.gitignore](https://github.com/SpencerPresley/COSC425-DATA/blob/main/.gitignore ".gitignore") | [.gitignore](https://github.com/SpencerPresley/COSC425-DATA/blob/main/.gitignore ".gitignore") | [crossref wrapper (pulling data from crossref api then fetching abstraâ€¦](https://github.com/SpencerPresley/COSC425-DATA/commit/48788a2e52b95cff6d30f10358f8a91aed5682a6 "crossref wrapper (pulling data from crossref api then fetching abstracts via selenium lookup of the doi link) integrated into full pipeline. Logging added throghout to go to their own files in each submodule. Some things moved into class structure. Pull out extra context and abstract from the fetching seperately so we can use them if we want. Save raw results (abstract + extra context) to a list and then dump it to json in crossref wrapper after fetching all abstracts missing from original crossref fetch") | Nov 12, 2024 |
| [.readthedocs.yaml](https://github.com/SpencerPresley/COSC425-DATA/blob/main/.readthedocs.yaml ".readthedocs.yaml") | [.readthedocs.yaml](https://github.com/SpencerPresley/COSC425-DATA/blob/main/.readthedocs.yaml ".readthedocs.yaml") | [readthedocs config](https://github.com/SpencerPresley/COSC425-DATA/commit/84caa504c20f2bb77644a4e2aef99e66b89d2e46 "readthedocs config") | Nov 24, 2024 |
| [CrossRefClassificationOrder.md](https://github.com/SpencerPresley/COSC425-DATA/blob/main/CrossRefClassificationOrder.md "CrossRefClassificationOrder.md") | [CrossRefClassificationOrder.md](https://github.com/SpencerPresley/COSC425-DATA/blob/main/CrossRefClassificationOrder.md "CrossRefClassificationOrder.md") | [warning and docs for strategy\_factory.py.](https://github.com/SpencerPresley/COSC425-DATA/commit/7a622ee1c1d90e153c01ea0608af9cabc81121ab "warning and docs for strategy_factory.py.") | Oct 4, 2024 |
| [README.md](https://github.com/SpencerPresley/COSC425-DATA/blob/main/README.md "README.md") | [README.md](https://github.com/SpencerPresley/COSC425-DATA/blob/main/README.md "README.md") | [formatting fixes for the various markdown file, switched to using heaâ€¦](https://github.com/SpencerPresley/COSC425-DATA/commit/c1a3dc37259899e154e40bceb71ca58aa7d22e34 "formatting fixes for the various markdown file, switched to using headers instead of numbered list in some places to standardize markdown indenting and make the blocks (tips, warnings, notes, important) line up better") | Jan 2, 2025 |
| [\_\_init\_\_.py](https://github.com/SpencerPresley/COSC425-DATA/blob/main/__init__.py "__init__.py") | [\_\_init\_\_.py](https://github.com/SpencerPresley/COSC425-DATA/blob/main/__init__.py "__init__.py") | [Adding the backend files from the main repo](https://github.com/SpencerPresley/COSC425-DATA/commit/6b4bc97c325d6856c5e93831a7385c743e308a25 "Adding the backend files from the main repo") | Apr 9, 2024 |
| [am\_deps.txt](https://github.com/SpencerPresley/COSC425-DATA/blob/main/am_deps.txt "am_deps.txt") | [am\_deps.txt](https://github.com/SpencerPresley/COSC425-DATA/blob/main/am_deps.txt "am_deps.txt") | [updated requirements.txt and pyproject dependencies](https://github.com/SpencerPresley/COSC425-DATA/commit/c58310b926d91881b57cf71dce8d3ff3f8ff48e5 "updated requirements.txt and pyproject dependencies") | Nov 20, 2024 |
| [check\_deps.py](https://github.com/SpencerPresley/COSC425-DATA/blob/main/check_deps.py "check_deps.py") | [check\_deps.py](https://github.com/SpencerPresley/COSC425-DATA/blob/main/check_deps.py "check_deps.py") | [Finishing touches complete.](https://github.com/SpencerPresley/COSC425-DATA/commit/aced798c6c61d37e386f3047e411cda20866f37e "Finishing touches complete.  AbstractClassifier extra_context optional param added, so that extra_context can be injected into prompt.  classification_prompts.py: added instructions to use extra context  human_prompt.py: added extra_context placeholder to prompt string  CrossrefWrapper.py added additional error handling and altered logic  scraper.py: Moved the check for if driver then driver.quite() to a finally block  enums.py: added extra_context enum  abstract_classifier_factory.py: added param to take in extra_context  classification_orchestrator: added handling for if any error occurs during the processing of an item we pop that item from the list  pipeline.py: fixed issue of how doi was being extracted  AttributeExtractionStrategies.py: added extra_context extraction strategy  New output files") | Nov 21, 2024 |
| [compare\_deps.py](https://github.com/SpencerPresley/COSC425-DATA/blob/main/compare_deps.py "compare_deps.py") | [compare\_deps.py](https://github.com/SpencerPresley/COSC425-DATA/blob/main/compare_deps.py "compare_deps.py") | [Finishing touches complete.](https://github.com/SpencerPresley/COSC425-DATA/commit/aced798c6c61d37e386f3047e411cda20866f37e "Finishing touches complete.  AbstractClassifier extra_context optional param added, so that extra_context can be injected into prompt.  classification_prompts.py: added instructions to use extra context  human_prompt.py: added extra_context placeholder to prompt string  CrossrefWrapper.py added additional error handling and altered logic  scraper.py: Moved the check for if driver then driver.quite() to a finally block  enums.py: added extra_context enum  abstract_classifier_factory.py: added param to take in extra_context  classification_orchestrator: added handling for if any error occurs during the processing of an item we pop that item from the list  pipeline.py: fixed issue of how doi was being extracted  AttributeExtractionStrategies.py: added extra_context extraction strategy  New output files") | Nov 21, 2024 |
| [deps.txt](https://github.com/SpencerPresley/COSC425-DATA/blob/main/deps.txt "deps.txt") | [deps.txt](https://github.com/SpencerPresley/COSC425-DATA/blob/main/deps.txt "deps.txt") | [updated requirements.txt and pyproject dependencies](https://github.com/SpencerPresley/COSC425-DATA/commit/c58310b926d91881b57cf71dce8d3ff3f8ff48e5 "updated requirements.txt and pyproject dependencies") | Nov 20, 2024 |
| [dockerfile](https://github.com/SpencerPresley/COSC425-DATA/blob/main/dockerfile "dockerfile") | [dockerfile](https://github.com/SpencerPresley/COSC425-DATA/blob/main/dockerfile "dockerfile") | [updated requirments.txt](https://github.com/SpencerPresley/COSC425-DATA/commit/6443cd777d02ca8ba409d92c01dae9b53dd451ea "updated requirments.txt") | Sep 23, 2024 |
| [pyproject.toml](https://github.com/SpencerPresley/COSC425-DATA/blob/main/pyproject.toml "pyproject.toml") | [pyproject.toml](https://github.com/SpencerPresley/COSC425-DATA/blob/main/pyproject.toml "pyproject.toml") | [very minor change to README. Version updated to 1.0.98](https://github.com/SpencerPresley/COSC425-DATA/commit/92513b9d217909e8046f084ba477577258deb192 "very minor change to README. Version updated to 1.0.98") | Jan 1, 2025 |
| [requirements.txt](https://github.com/SpencerPresley/COSC425-DATA/blob/main/requirements.txt "requirements.txt") | [requirements.txt](https://github.com/SpencerPresley/COSC425-DATA/blob/main/requirements.txt "requirements.txt") | [documentation](https://github.com/SpencerPresley/COSC425-DATA/commit/4912ccee98083b428a9c5e615f6114606c8147c8 "documentation") | Nov 24, 2024 |
| [setup\_environment.py](https://github.com/SpencerPresley/COSC425-DATA/blob/main/setup_environment.py "setup_environment.py") | [setup\_environment.py](https://github.com/SpencerPresley/COSC425-DATA/blob/main/setup_environment.py "setup_environment.py") | [black format test](https://github.com/SpencerPresley/COSC425-DATA/commit/1c83ddfd0d3286c4a51cc820ddcf867e75c381f0 "black format test") | Nov 9, 2024 |
| View all files |

## Repository files navigation

# Academic Metrics

[Permalink: Academic Metrics](https://github.com/spencerpresley/COSC425-DATA#academic-metrics)

Important

ðŸŽ‰ **Now Available on PyPI!**

Install with: `pip install academic-metrics`

This is the recommended installation method for most users.

See [**Installation and Setup Steps**](https://github.com/spencerpresley/COSC425-DATA#installation-and-setup-steps)

**What is it?**

This repository (COSC425-DATA) hosts the source code for the **Academic Metrics** package.

**Academic Metrics** is an AI-powered toolkit for collecting and classifying academic research publications.

The system:

- Collects publication data from Crossref API based on institutional affiliation
- Uses LLMs to classify research into NSF PhD research focus areas
- Extracts and analyzes themes and methodologies from abstracts
- Generates comprehensive analytics at article, author, and category levels
- Stores results in MongoDB (local or live via atlas), local JSON files, and optionally Excel files

## Table of Contents

[Permalink: Table of Contents](https://github.com/spencerpresley/COSC425-DATA#table-of-contents)

- [Academic Metrics](https://github.com/spencerpresley/COSC425-DATA#academic-metrics)
  - [Table of Contents](https://github.com/spencerpresley/COSC425-DATA#table-of-contents)
  - [Features](https://github.com/spencerpresley/COSC425-DATA#features)
  - [Documentation](https://github.com/spencerpresley/COSC425-DATA#documentation)
  - [Example Site and Demo](https://github.com/spencerpresley/COSC425-DATA#example-site-and-demo)
  - [Installation and Setup Steps](https://github.com/spencerpresley/COSC425-DATA#installation-and-setup-steps)
    - [0\. External Setup](https://github.com/spencerpresley/COSC425-DATA#0-external-setup)
    - [1\. Installation](https://github.com/spencerpresley/COSC425-DATA#1-installation)
    - [2\. Creating the directory and necessary files](https://github.com/spencerpresley/COSC425-DATA#2-creating-the-directory-and-necessary-files)
    - [3\. Virtual Environment (Optional but Recommended)](https://github.com/spencerpresley/COSC425-DATA#3-virtual-environment-optional-but-recommended)
    - [4\. Environment Variables](https://github.com/spencerpresley/COSC425-DATA#4-environment-variables)
    - [5\. Setting required environment variables](https://github.com/spencerpresley/COSC425-DATA#5-setting-required-environment-variables)
      - [1\. Open the `.env` file you just created, and add the following variables](https://github.com/spencerpresley/COSC425-DATA#1-open-the-env-file-you-just-created-and-add-the-following-variables)
      - [2\. Retrieve and set your MongoDB URI](https://github.com/spencerpresley/COSC425-DATA#2-retrieve-and-set-your-mongodb-uri)
      - [3\. Set your database name](https://github.com/spencerpresley/COSC425-DATA#3-set-your-database-name)
      - [4\. Set your OpenAI API Key](https://github.com/spencerpresley/COSC425-DATA#4-set-your-openai-api-key)
    - [6\. Using the package](https://github.com/spencerpresley/COSC425-DATA#6-using-the-package)
      - [Option 1 (Short Script)](https://github.com/spencerpresley/COSC425-DATA#option-1-short-script)
        - [1\. Create the python file](https://github.com/spencerpresley/COSC425-DATA#1-create-the-python-file)
        - [2\. Copy paste the following code into the file you just created](https://github.com/spencerpresley/COSC425-DATA#2-copy-paste-the-following-code-into-the-file-you-just-created)
        - [3\. Run the script](https://github.com/spencerpresley/COSC425-DATA#3-run-the-script)
      - [Option 2 (Command Line Interface)](https://github.com/spencerpresley/COSC425-DATA#option-2-command-line-interface)
        - [1\. Create the python file](https://github.com/spencerpresley/COSC425-DATA#1-create-the-python-file-1)
        - [2\. Copy and paste the following code into the file you just created](https://github.com/spencerpresley/COSC425-DATA#2-copy-and-paste-the-following-code-into-the-file-you-just-created)
        - [3\. Run the script](https://github.com/spencerpresley/COSC425-DATA#3-run-the-script-1)
        - [Examples](https://github.com/spencerpresley/COSC425-DATA#examples)
  - [Wrapping Up](https://github.com/spencerpresley/COSC425-DATA#wrapping-up)

## Features

[Permalink: Features](https://github.com/spencerpresley/COSC425-DATA#features)

- **Data Collection**: Automated fetching of publications via Crossref API
- **AI Classification**: LLM-powered analysis of research abstracts
- **Multi-level Analytics**:

  - Article-level metrics and classifications
  - Author/faculty publication statistics
  - Category-level aggregated data
- **Flexible Storage**: MongoDB integration, local JSON output, and optionally Excel files
- **Configurable Pipeline**: Customizable date ranges, models, and processing options
- **And more!**: There are many useful tools within the academic metrics package that can be used for much more than just classification of academic research data, and they're all quite intuitive to use. See [Other Uses](https://github.com/SpencerPresley/COSC425-DATA/blob/main/additional_information/OtherUses.md) for more information.

## Documentation

[Permalink: Documentation](https://github.com/spencerpresley/COSC425-DATA#documentation)

To be able to see any and all implementation details regarding code logic, structure, prompts, and more you can check out our documentation. The documentation is built with [_Sphinx_](https://github.com/sphinx-doc/sphinx), allowing for easy use and a sense of famliarity.

[**Academic Metrics Documentation**](https://cosc425-data.readthedocs.io/en/latest/)

## Example Site and Demo

[Permalink: Example Site and Demo](https://github.com/spencerpresley/COSC425-DATA#example-site-and-demo)

We also built an example site with the data we collected so that you can get a small idea of the potential uses for the data. This is by no means the only use case, but it does serve as a nice introduction to decide if this package would be useful for you.

Note

The source code for the example site is available [here](https://github.com/cbarbes1/AITaxonomy-Front)

[**Example Site**](https://ai-taxonomy-front.vercel.app/)

Tip

You can use our site source code for your own site!
To easily launch your own website using the data you collect and classify via _Academic Metrics_ see [**Site Creation Guide**](https://github.com/SpencerPresley/COSC425-DATA/blob/main/additional_information/SiteCreationGuide.md)

To see a demo of the site, you can watch the below video:

[![Demo Video](https://camo.githubusercontent.com/7a5046da3f9b1e79a18ed6ef65bd76d72f5a23236347eb4624f770d4b5e4cdbb/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f4c6f6a497745764667726b2f6d617872657364656661756c742e6a7067)](https://youtu.be/LojIwEvFgrk)

* * *

## Installation and Setup Steps

[Permalink: Installation and Setup Steps](https://github.com/spencerpresley/COSC425-DATA#installation-and-setup-steps)

Hey all, Spencer here, we are pleased to announce as of January 1st, 2025, you can now install the _Academic Metrics_ package via _pip_ and easily run the entire system via a short script or command line interface. Below are instructions outlining step by step how to do it. The steps walkthrough each piece of the process starting with installing python and setting up your environment, if you do not need help with those type of steps or want to jump straight to the code, first see [1\. Installation](https://github.com/spencerpresley/COSC425-DATA#1-installation), then you can skip to [6\. Using the package](https://github.com/spencerpresley/COSC425-DATA#6-using-the-package).

### 0\. External Setup

[Permalink: 0. External Setup](https://github.com/spencerpresley/COSC425-DATA#0-external-setup)

1. **Installing and setting up Python 3.12:**

While you should be able to use any version of Python >= 3.7, we recommend using Python 3.12 as that is the version we used to develop the system, and the one it's been tested on.

For a detailed Python installation guide, see our [Python Installation Guide](https://github.com/SpencerPresley/COSC425-DATA/blob/main/additional_information/_guides/_python_install.md).

2. **Installing and setting up MongoDB:**

For a detailed MongoDB installation and setup guide, see our [MongoDB Installation Guide](https://github.com/SpencerPresley/COSC425-DATA/blob/main/additional_information/_guides/_mongodb_install.md).

Once you have MongoDB installed and running, you can create a database to store your data in, if you haven't already.

To create a new database, you can run:



```
use <db_name>
```







If you need more help, the MongoDB Installation Guide goes into more detail on how to create a database and verify it exists.

Collection creation is handled by the system, you do not need to create them.




* * *


### 1\. Installation

[Permalink: 1. Installation](https://github.com/spencerpresley/COSC425-DATA#1-installation)

Install `academic_metrics>=1.0.98` via pip.

To install the latest version of the package, you can run the following command:

```
pip install academic-metrics
```

### 2\. Creating the directory and necessary files

[Permalink: 2. Creating the directory and necessary files](https://github.com/spencerpresley/COSC425-DATA#2-creating-the-directory-and-necessary-files)

1. **Create the directory and navigate into it:**

For this example we will be using `am_data_collection` as the name of the directory, but you can name it whatever you want.

**All systems (seperate commands):**



```
mkdir am_data_collection
cd am_data_collection
```







Or as a single line:

**Linux / Mac / Windows Command Prompt**:



```
mkdir am_data_collection && cd am_data_collection
```







**Windows Powershell**:



```
mkdir am_data_collection; cd am_data_collection
```


### 3\. Virtual Environment (Optional but Recommended)

[Permalink: 3. Virtual Environment (Optional but Recommended)](https://github.com/spencerpresley/COSC425-DATA#3-virtual-environment-optional-but-recommended)

Now that you've created and entered your project directory, you can set up a virtual environment.

For detailed instructions on setting up and using virtual environments, see our [Python Installation Guide - Virtual Environments Section](https://github.com/SpencerPresley/COSC425-DATA/blob/main/additional_information/_guides/_python_install.md#setting-up-virtual-environments).

After setting up your virtual environment, return here to continue with the next steps.

### 4\. Environment Variables

[Permalink: 4. Environment Variables](https://github.com/spencerpresley/COSC425-DATA#4-environment-variables)

**Create a `.env` file inside the directory you just created.**

**Linux/Mac**:

```
touch .env
```

**Windows** (Command Prompt):

```
type nul > .env
```

**Windows** (PowerShell):

```
New-Item -Path .env -Type File
```

You should now have a `.env` file in your directory.

### 5\. Setting required environment variables

[Permalink: 5. Setting required environment variables](https://github.com/spencerpresley/COSC425-DATA#5-setting-required-environment-variables)

#### 1\. Open the `.env` file you just created, and add the following variables

[Permalink: 1. Open the .env file you just created, and add the following variables](https://github.com/spencerpresley/COSC425-DATA#1-open-the-env-file-you-just-created-and-add-the-following-variables)

- a variable to store your MongoDB URI, I recommend `MONGODB_URI`
- a variable to store your database name, I recommend `DB_NAME`
- a variable to store your OpenAI API Key, I recommend `OPENAI_API_KEY`

After each variable you should add `=""` to the end of the variable.

Once you've done this, your `.env` file should look something like this:

```
MONGODB_URI=""
DB_NAME=""
OPENAI_API_KEY=""
```

#### 2\. Retrieve and set your MongoDB URI

[Permalink: 2. Retrieve and set your MongoDB URI](https://github.com/spencerpresley/COSC425-DATA#2-retrieve-and-set-your-mongodb-uri)

For local MongoDB it's typically:

```
MONGODB_URI="mongodb://localhost:27017"
```

For live MongoDB:

For a live version you should use the MongoDB Atlas URI. It should look something like this:

```
mongodb+srv://<username>:<password>@<cluster-name>.<unique-id>.mongodb.net/?retryWrites=true&w=majority&appName=<YourAppNameOnAtlas>
```

So in the `.env` file you should have something that looks like this:

Local:

```
MONGODB_URI="mongodb://localhost:27017"
```

Live:

```
MONGODB_URI="mongodb+srv://<username>:<password>@<cluster-name>.<unique-id>.mongodb.net/?retryWrites=true&w=majority&appName=<YourAppNameOnAtlas>"
```

Warning

I recommend starting locally unless you need to use a live MongoDB instance.
This will avoid the need to deal with setting up MongoDB Atlas, which while not difficult, it is an added step.

#### 3\. Set your database name

[Permalink: 3. Set your database name](https://github.com/spencerpresley/COSC425-DATA#3-set-your-database-name)

You can pick any name you want for `DB_NAME`, but it needs to be a name of a valid database on your mongodb server. To make one on the command line you can run:

```
mongosh
use <db_name>
```

For this demonstration we will be using `academic_metrics_data` as the `DB_NAME`.

First we'll create the database on the command line:

```
mongosh
use academic_metrics_data
```

This is to ensure the database actually exists so that the system can access it.

Now that the database exists, we'll set the `DB_NAME` in the `.env` file.

```
DB_NAME="academic_metrics_data"
```

#### 4\. Set your OpenAI API Key

[Permalink: 4. Set your OpenAI API Key](https://github.com/spencerpresley/COSC425-DATA#4-set-your-openai-api-key)

If you do not have an OpenAI API key you will need to create one, but do not worry, it's easy.

Go to the following link and click on "+ Create new secret key":

[https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)

Give the key a name, and then copy the key.

Then in the `.env` file paste the key in the `OPENAI_API_KEY` variable.

It should look similar to this, but with the full key instead of `sk-proj...`:

```
OPENAI_API_KEY="sk-proj..."
```

Important

You will need to add funds to your OpenAI account to use the API.

When using the default model for the system (gpt-4o-mini), it cost us about $3-4 dollars to process all of the data from Salisbury University from 2009-2024.

For larger models such as gpt-4o, the cost will be much higher.

We saw good results using gpt-4o-mini, and it's also the most cost effective. So I recommend starting with that.

Additionally, whether you opt to use our command line interface or your own script, the data is processed one month at a time and saved to the database, so if you run out of funds on your OpenAI account you will not lose data for the entire run, only the current month being processed. Simply add funds to your account and continue.

You do not have to change anything in the code once you run it again, the system checks for existing data and only processes data that has not yet been processed.

All together your `.env` file should look like this:

```
MONGODB_URI="mongodb://localhost:27017"
DB_NAME="academic_metrics_data"
OPENAI_API_KEY="sk-proj..."
```

### 6\. Using the package

[Permalink: 6. Using the package](https://github.com/spencerpresley/COSC425-DATA#6-using-the-package)

To use the system, you have 2 options:

1. Writing a short script (code provided) to loop over a range of dates you'd like to collect.

2. Using a provided function to run a command line interface version.


For most users, I recommend the second option, it's only a few lines of code which you can copy and paste, the rest of the usage is handled by the command line interface and doesn't require any additional coding, you can find the second option in the [Option 2 (Command Line Interface)](https://github.com/spencerpresley/COSC425-DATA#option-2-command-line-interface) section.

On the other hand, if you plan on using the main system, or other tools within the package within your own scripts, or just don't enjoy using command line interfaces, I recommend the first option.

While I recommend the second option unless you're planning on using the package's offerings in a more complex manner, the basic code to run the system for the first option is provided in full in [Option 1 (Short Script)](https://github.com/spencerpresley/COSC425-DATA#option-1-short-script) section.

To see some examples of more complex use cases with examples, you can check out the [Other Uses](https://github.com/SpencerPresley/COSC425-DATA/blob/main/additional_information/OtherUses.md) section.

#### Option 1 (Short Script)

[Permalink: Option 1 (Short Script)](https://github.com/spencerpresley/COSC425-DATA#option-1-short-script)

For this option you need to do the following:

##### 1\. Create the python file

[Permalink: 1. Create the python file](https://github.com/spencerpresley/COSC425-DATA#1-create-the-python-file)

Within your directory, create a new python file, for this example we will be using `run_am.py`, but you can name it whatever you want.

**Linux/Mac**:

```
touch run_am.py
```

**Windows (Command Prompt):**

```
type nul > run_am.py
```

**Windows (PowerShell):**

```
New-Item -Path run_am.py -Type File
```

You should now have a python file in your directory whose name matches the one you created.

##### 2\. Copy paste the following code into the file you just created

[Permalink: 2. Copy paste the following code into the file you just created](https://github.com/spencerpresley/COSC425-DATA#2-copy-paste-the-following-code-into-the-file-you-just-created)

```
# dotenv is the python package responsible for handling env files
from dotenv import load_dotenv

# os is used to get the environment variables from the .env file
import os

# PipelineRunner is the main class used to run the pipeline
from academic_metrics.runners import PipelineRunner

# load_dotenv is used to load the environment variables from the .env file
load_dotenv()

# Get the environment variables from the .env file
ai_api_key = os.getenv("OPENAI_API_KEY")
mongodb_uri = os.getenv("MONGODB_URI")
db_name = os.getenv("DB_NAME")

# Set the date range you want to process
# Years is a list of years as strings you want to process
# Months is a list of strings representing the months you want processed for each year
# For example if you want to process data from 2009-2024 for all months out of the year, you would do:
# Note: the process runs left to right, so from beginning of list to the end of the list,
# so this will process 2024, then 2023, then 2022, etc.
# Data will be saved after each month is processed.
years = [\
    "2024",\
    "2023",\
    "2022",\
    "2021",\
    "2020",\
    "2019",\
    "2018",\
    "2017",\
    "2016",\
    "2015",\
    "2014",\
    "2013",\
    "2012",\
    "2011",\
    "2010",\
    "2009",\
]
months = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"]

# Loop over the years and months and run the pipeline for each month
# New objects are created for each month to avoid memory issues as well as to avoid overwriting data
for year in years:
    for month in months:

        # Create a new PipelineRunner object for each month
        # parameters:
        # ai_api_key: the OpenAI API key
        # crossref_affiliation: the affiliation to use for the Crossref API
        # data_from_month: the month to start collecting data from
        # data_to_month: the month to end collecting data on
        # data_from_year: the year to start collecting data from
        # data_to_year: the year to end collecting data on
        # mongodb_uri: the URL of the MongoDB server
        # db_name: the name of the database to use
        pipeline_runner = PipelineRunner(
            ai_api_key=ai_api_key,
            crossref_affiliation="Salisbury University",
            data_from_month=int(month),
            data_to_month=int(month),
            data_from_year=int(year),
            data_to_year=int(year),
            mongodb_uri=mongodb_uri,
            db_name=db_name,
        )

        # Run the pipeline for the current month
        pipeline_runner.run_pipeline()
```

If you'd like to save the data to excel files in addition to the other data formats, you can do so via importing the function `get_excel_report` from `academic_metrics.runners` and calling it at the end of the script.

Full code for convenience:

```
# dotenv is the python package responsible for handling env files
from dotenv import load_dotenv

# os is used to get the environment variables from the .env file
import os

# PipelineRunner is the main class used to run the pipeline
# get_excel_report is the function used to save the data to excel files
# it takes in a DatabaseWrapper object as a parameter, which connects to the database
# and retrives the data before writing it to 3 seperate excel files. One for each data type.
from academic_metrics.runners import PipelineRunner, get_excel_report

# DatabaseWrapper is the class used to connect to the database and retrieve the data
from academic_metrics.DB import DatabaseWrapper

# load_dotenv is used to load the environment variables from the .env file
load_dotenv()

# Get the environment variables from the .env file
# If you used the same names as the ones in the examples, you can just copy paste these
# if you used different names, you will need to change them to match the ones in your .env file
ai_api_key = os.getenv("OPENAI_API_KEY")
mongodb_uri = os.getenv("MONGODB_URI")
db_name = os.getenv("DB_NAME")

# Set the date range you want to process
# Years is a list of years as strings you want to process
# Months is a list of strings representing the months you want processed for each year
# For example if you want to process data from 2009-2024 for all months out of the year, you would do:
# Note: the process runs left to right, so from beginning of list to the end of the list,
# so this will process 2024, then 2023, then 2022, etc.
# Data will be saved after each month is processed.
years = [\
    "2024",\
    "2023",\
    "2022",\
    "2021",\
    "2020",\
    "2019",\
    "2018",\
    "2017",\
    "2016",\
    "2015",\
    "2014",\
    "2013",\
    "2012",\
    "2011",\
    "2010",\
    "2009",\
]
months = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"]

# Loop over the years and months and run the pipeline for each month
#
# New objects are created for each month
# to avoid memory issues as well as to avoid overwriting data
for year in years:
    for month in months:

        # Create a new PipelineRunner object for each month
        # parameters:
        # ai_api_key: the OpenAI API key
        # crossref_affiliation: the affiliation to use for the Crossref API
        # data_from_month: the month to start collecting data from
        # data_to_month: the month to end collecting data on
        # data_from_year: the year to start collecting data from
        # data_to_year: the year to end collecting data on
        # mongodb_uri: the URL of the MongoDB server
        # db_name: the name of the database to use
        pipeline_runner = PipelineRunner(
            ai_api_key=ai_api_key,
            crossref_affiliation="Salisbury University",
            data_from_month=int(month),
            data_to_month=int(month),
            data_from_year=int(year),
            data_to_year=int(year),
            mongodb_uri=mongodb_uri,
            db_name=db_name,
        )

        # Run the pipeline for the current month
        pipeline_runner.run_pipeline()

# Create a new DatabaseWrapper object so it can be given to get_excel_report
db = DatabaseWrapper(db_name=db_name, mongo_uri=mongodb_uri)

# Call the get_excel_report function, passing in the db object, to save the data to excel files
#
# Once this finishes running, you should have 3 excel files in your directory:
# article_data.xlsx, faculty_data.xlsx, and category_data.xlsx
get_excel_report(db)
```

##### 3\. Run the script

[Permalink: 3. Run the script](https://github.com/spencerpresley/COSC425-DATA#3-run-the-script)

```
python run_am.py
```

#### Option 2 (Command Line Interface)

[Permalink: Option 2 (Command Line Interface)](https://github.com/spencerpresley/COSC425-DATA#option-2-command-line-interface)

For this options you will still need to create a python file, but the code will only be a couple lines long as you'll be passing in your arguments via the command line.

##### 1\. Create the python file

[Permalink: 1. Create the python file](https://github.com/spencerpresley/COSC425-DATA#1-create-the-python-file-1)

Within your directory, create a new python file, for this example we will be using `run_am.py`, but you can name it whatever you want.

**Linux/Mac**:

```
touch run_am.py
```

**Windows (Command Prompt):**

```
type nul > run_am.py
```

**Windows (PowerShell):**

```
New-Item -Path run_am.py -Type File
```

You should now have a python file in your directory whose name matches the one you created.

##### 2\. Copy and paste the following code into the file you just created

[Permalink: 2. Copy and paste the following code into the file you just created](https://github.com/spencerpresley/COSC425-DATA#2-copy-and-paste-the-following-code-into-the-file-you-just-created)

```
from dotenv import load_dotenv
from academic_metrics.runners import command_line_runner

load_dotenv()

command_line_runner()
```

Warning

If you did not use `MONGODB_URI` and `OPENAI_API_KEY` as the variable names in the .env file, you will need to make a couple changes to the above code.

**How to use with different variable names:**

The `command_line_runner` function takes in 2 optional arguments:

- `openai_api_key_env_var_name`
- `mongodb_uri_env_var_name`

Which correspond to the names of the environment variables you used in your .env file.

To use the different names, do the following:

```
from dotenv import load_dotenv
from academic_metrics.runners import command_line_runner

load_dotenv()

# The strings should be changes to match the names you used in your .env file
command_line_runner(
    openai_api_key_env_var_name="YOUR_OPENAI_API_KEY_ENV_VAR_NAME",
    mongodb_uri_env_var_name="YOUR_MONGODB_URI_ENV_VAR_NAME",
)
```

##### 3\. Run the script

[Permalink: 3. Run the script](https://github.com/spencerpresley/COSC425-DATA#3-run-the-script-1)

For this option you will still run the script from command line, but you will also be passing in arguments, details laid out below.

There are various command line arguments you can pass in, almost all are detailed here, but to see a complete list you can run:

```
python run_am.py --help
```

When running the script, you can configure the pipeline by passing in the following arguments:

- `--from-month` \- The month to start collecting data from, defaults to 1
- `--to-month` \- The month to end collecting data on, defaults to 12
- `--from-year` \- The year to start collecting data from, defaults to 2024
- `--to-year` \- The year to end collecting data on, defaults to 2024
- `--db-name` \- The name of the database to use (required)
- `--crossref-affiliation` \- The affiliation to use for the Crossref API, defaults to Salisbury University (required)

If you want to save the data to excel files you can pass in the `--as-excel` argument.

Note

The `--as-excel` argument is an additional action, it doesn't remove the the saving to other data formats, but merely adds the excel saving functionality.

##### Examples

[Permalink: Examples](https://github.com/spencerpresley/COSC425-DATA#examples)

Say you want to collect data for every month from 2019 to 2024 for Salisbury University and save it to excel files. You would run the following command:

```
python run_am.py --from-month=1 \
--to-month=12 \
--from-year=2019 \
--to-year=2024 \
--crossref-affiliation="Salisbury University" \
--as-excel \
--db-name="Your_Database_Name"
```

To make this simpler, we can actually take advantage of the default values for some of the arguments.

Recall from before:

- `--from-month` defaults to `1`
- `--to-month` defaults to `12`
- `--from-year` defaults to `2024`
- `--to-year` defaults to `2024`
- `--crossref-affiliation` defaults to `Salisbury University`

Using the defaults, we can make that command much more concise:

```
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name"
```

**On AI Models**:

The default AI (LLM) model used for all phases is `gpt-4o-mini`. You can specify a different model for each phase independently by passing in the following arguments:

- `--pre-classification-model` \- The model to use for the pre-classification step
- `--classification-model` \- The model to use for the classification step
- `--theme-model` \- The model to use for the theme extraction step

Here's how you would run the pipeline using the larger `gpt-4o` model:

```
python run_am.py --from-month=1 \
--to-month=12 \
--from-year=2019 \
--to-year=2024 \
--crossref-affiliation="Salisbury University" \
--as-excel \
--db-name="Your_Database_Name" \
--pre-classification-model="gpt-4o" \
--classification-model="gpt-4o" \
--theme-model="gpt-4o"
```

and taking advantage of the defaults:

```
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name" \
--pre-classification-model="gpt-4o" \
--classification-model="gpt-4o" \
--theme-model="gpt-4o"
```

Warning

This process consumes a lot of tokens, and OpenAI API service usage is based off the number of input/output tokens used, with each model having different cost per input/output token.

You can check the cost of each model at [https://openai.com/api/pricing/](https://openai.com/api/pricing/).

During testing we found that using `gpt-4o-mini` was the most cost effective.

In addition we spent a lot of time testing prompts and models, our prompts have been tuned to a point where they elicit good results from `gpt-4o-mini`, thus a larger model may not be necessary to get the results you want.

If you want to use a larger model like `gpt-4o` you can do so, but be warned it could end up running through your funds very quickly, depending on the size of the date range you're processing, and how many articles Crossref covers for the institution being processed.

If you are interested in using a larger model, I recommend you first start with a smaller model on a limited date range to see if you're satisfied with the results.

If you then decide to use a larger model such as `gpt-4o`, whether it be out of curiosity or you want to see if it provides better results, I still recommend you start with a smaller date range to get an idea of what it will cost. If you find the cost to be acceptable, then you can start expanding the date range.

**Other institutions**:

Our system uses the Crossref API to collect available data, then it scrapes the DOI link to get any missing data as well as any additional data that may be available.

We found that the Crossref API sometimes misses some Abstracts for example, our scraping process will fill in nearly all, if not all, of the missing abstracts.

Due to this, and the wealth of institutions Crossref covers, you can use the system for any institution that has a DOI link.

Here's how you'd run the same query on the system but for **University of Maryland** data:

```
python run_am.py \
--from-year=2019 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

You can even go back as far as you want, for example say you want to collect all data from the beginning of the 21st century:

```
python run_am.py \
--from-year=2000 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

Or maybe you want to collect all data as far back as possible, so you can see longterm trends and history of the institution:

```
python run_am.py \
--from-year=1900 \
--as-excel \
--db-name="Your_Database_Name" \
--crossref-affiliation="University of Maryland"
```

The from year does not require that there be data that far back, it simply means that is the cutoff point for the data you want to collect.

So say you're not entirely sure what year your University started, or aren't sure how far back Crossref covers, you can simply enter a very far back year, like 1900, and the system will collect all data from that year and onwards.

* * *

## Wrapping Up

[Permalink: Wrapping Up](https://github.com/spencerpresley/COSC425-DATA#wrapping-up)

That's it! You've now successfully installed and run the system.

If you have any questions, need help, or have interest in collaborating on this project or others, feel free to reach out to me, contact information is provided below.

If you are a potential employer, please reach out to me by email or linkedin, contact information is provided below.

Contact information:

- Email: [spencerpresley96@gmail.com](mailto:spencerpresley96@gmail.com)
- LinkedIn: [https://www.linkedin.com/in/spencerpresley96/](https://www.linkedin.com/in/spencerpresley96/)

Happy coding!

## About

AI-powered toolkit for analyzing and classifying academic research publications using LLMs and automated data collection. Output options: Mongodb database via providing your databse url. Json. Excel spreadsheet. See README for the quick setup, see documentation for implementation details:


[cosc425-data.readthedocs.io/](https://cosc425-data.readthedocs.io/ "https://cosc425-data.readthedocs.io/")

### Topics

[data-science](https://github.com/topics/data-science "Topic: data-science") [academic-publications](https://github.com/topics/academic-publications "Topic: academic-publications") [research-analytics](https://github.com/topics/research-analytics "Topic: research-analytics") [llm](https://github.com/topics/llm "Topic: llm") [llms](https://github.com/topics/llms "Topic: llms")

### Resources

[Readme](https://github.com/spencerpresley/COSC425-DATA#readme-ov-file)

[Activity](https://github.com/SpencerPresley/COSC425-DATA/activity)

### Stars

[**2**\\
stars](https://github.com/SpencerPresley/COSC425-DATA/stargazers)

### Watchers

[**1**\\
watching](https://github.com/SpencerPresley/COSC425-DATA/watchers)

### Forks

[**3**\\
forks](https://github.com/SpencerPresley/COSC425-DATA/forks)

[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FSpencerPresley%2FCOSC425-DATA&report=SpencerPresley+%28user%29)

## [Releases\  25](https://github.com/SpencerPresley/COSC425-DATA/releases)

[v1.0.98\\
Latest\\
\\
Jan 2, 2025](https://github.com/SpencerPresley/COSC425-DATA/releases/tag/v1.0.98)

[\+ 24 releases](https://github.com/SpencerPresley/COSC425-DATA/releases)

## [Packages\  0](https://github.com/users/SpencerPresley/packages?repo_name=COSC425-DATA)

No packages published

## [Contributors\  3](https://github.com/SpencerPresley/COSC425-DATA/graphs/contributors)

- [![@SpencerPresley](https://avatars.githubusercontent.com/u/153392395?s=64&v=4)](https://github.com/SpencerPresley)[**SpencerPresley** Spencer Presley](https://github.com/SpencerPresley)
- [![@cbarbes1](https://avatars.githubusercontent.com/u/115114714?s=64&v=4)](https://github.com/cbarbes1)[**cbarbes1** Cole Barbes](https://github.com/cbarbes1)
- [![@judemood](https://avatars.githubusercontent.com/u/145386689?s=64&v=4)](https://github.com/judemood)[**judemood**](https://github.com/judemood)

## Languages

- [Rich Text Format51.3%](https://github.com/SpencerPresley/COSC425-DATA/search?l=rich-text-format)
- [Python42.0%](https://github.com/SpencerPresley/COSC425-DATA/search?l=python)
- [Jupyter Notebook6.7%](https://github.com/SpencerPresley/COSC425-DATA/search?l=jupyter-notebook)

You canâ€™t perform that action at this time.

